{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "[![Open on GitHub](https://img.shields.io/badge/GitHub-View%20Source-181717?style=for-the-badge&logo=github)](https://github.com/SeenaKhosravi/NASS/blob/main/NASS_Analysis.ipynb)\n",
        "[![Open In Colab](https://img.shields.io/badge/Colab-Open%20Notebook-F9AB00?style=for-the-badge&logo=google-colab)](https://colab.research.google.com/github/SeenaKhosravi/NASS/blob/main/NASS_Analysis.ipynb)\n",
        "[![Open in Vertex AI](https://img.shields.io/badge/Vertex%20AI-Open%20Workbench-4285F4?style=for-the-badge&logo=google-cloud)](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/SeenaKhosravi/NASS/main/NASS_Analysis.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbe6c3e6"
      },
      "source": [
        "# Socioeconomic and Demographic Drivers of Ambulatory Surgery Usage\n",
        "### HCUP NASS 2020 – Reproducible Pipeline (Python + R)\n",
        "\n",
        "**Author:** Seena Khosravi, MD  \n",
        "**Last Updated:** September 2, 2025  \n",
        "**Data Source:** Healthcare Cost and Utilization Project (HCUP) NASS 2020\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "This notebook provides a reproducible analysis pipeline for examining socioeconomic and demographic factors influencing ambulatory surgery usage patterns. The analysis combines Python for data processing and R for statistical modeling.\n",
        "\n",
        "### Data Usage Agreement\n",
        "**DUA Compliant** — This notebook uses a simulated dataset unless you have purchased the full dataset from HCUP. The full dataset can be loaded from your local storage or cloud storage.\n",
        "\n",
        "### Key Features\n",
        "- **Platform Agnostic:** Works in Jupyter, Colab, Vertex AI, or local environments\n",
        "- **Flexible Data Sources:** GitHub (simulated), Google Cloud Storage, or local files\n",
        "- **Reproducible:** All dependencies and environment setup included\n",
        "- **Scalable:** Handles both simulated (1GB, 700k rows) and full dataset (12 GB, 7.8M rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "019b6fe9"
      },
      "source": [
        "---\n",
        "\n",
        "## Design Notes\n",
        "\n",
        "### Architecture\n",
        "- **Python cells** handle \"plumbing\" (file I/O, environment setup, rpy2 configuration, data previews)\n",
        "- **R cells** (prefixed by `%%R`) perform statistical analysis: survey weights, Census lookups, multilevel models, plots\n",
        "\n",
        "### Data Sources\n",
        "- **Default:** Simulated dataset (1GB) from GitHub releases\n",
        "- **Local:** Switch to locally stored files via configuration\n",
        "- **Cloud:** Google Cloud Storage support for large datasets\n",
        "- **Full Dataset:** 7.8M row HCUP release (requires DUA compliance)\n",
        "\n",
        "### Environment Support\n",
        "- Jupyter Notebook\n",
        "- Google Colab\n",
        "- Vertex AI Workbench\n",
        "- Local Python/R environments\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Configuration\n",
        "\n",
        "Configure all settings in one place - data sources, debugging options, and file paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== CONFIGURATION ====================\n",
        "# Data Source Options\n",
        "DATA_SOURCE = \"github\"      # Options: \"github\", \"local\", \"gcs\", \"drive\"\n",
        "VERBOSE_PRINTS = True       # False → suppress debug output\n",
        "\n",
        "# GitHub source (default - simulated data)\n",
        "GITHUB_URL = \"https://github.com/SeenaKhosravi/NASS/releases/download/v1.0.0/nass_2020_simulated.csv\"\n",
        "\n",
        "# Local file options\n",
        "LOCAL_FILENAME = \"nass_2020_local.csv\"\n",
        "\n",
        "# Google Cloud Storage options\n",
        "GCS_BUCKET = \"nass_2020\"\n",
        "GCS_BLOB = \"nass_2020_all.csv\"\n",
        "GCS_SERVICE_ACCOUNT_KEY = \"/path/to/service-account-key.json\"  # Optional\n",
        "\n",
        "# Google Drive options (for Colab)\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/NASS/nass_2020_full.csv\"\n",
        "# ======================================================\n",
        "\n",
        "print(\"✓ Configuration loaded\")\n",
        "print(f\"  Data source: {DATA_SOURCE}\")\n",
        "print(f\"  Verbose mode: {VERBOSE_PRINTS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Environment Setup & Package Installation\n",
        "\n",
        "Smart package installation and environment detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "class EnvironmentManager:\n",
        "    def __init__(self):\n",
        "        self.detect_environment()\n",
        "        self.setup_packages()\n",
        "    \n",
        "    def detect_environment(self):\n",
        "        \"\"\"Detect runtime environment\"\"\"\n",
        "        self.is_colab = 'COLAB_GPU' in os.environ or 'google.colab' in sys.modules\n",
        "        self.is_vertex = 'DL_ANACONDA_HOME' in os.environ\n",
        "        \n",
        "        if self.is_colab:\n",
        "            self.env_type = \"Google Colab\"\n",
        "        elif self.is_vertex:\n",
        "            self.env_type = \"Vertex AI\"\n",
        "        else:\n",
        "            self.env_type = \"Local/Jupyter\"\n",
        "        \n",
        "        print(f\"Environment: {self.env_type}\")\n",
        "    \n",
        "    def install_package(self, package, conda_name=None):\n",
        "        \"\"\"Smart package installation with fallback\"\"\"\n",
        "        try:\n",
        "            __import__(package)\n",
        "            return True\n",
        "        except ImportError:\n",
        "            try:\n",
        "                if conda_name and not self.is_colab:\n",
        "                    subprocess.check_call(['conda', 'install', '-y', conda_name], \n",
        "                                        stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "                else:\n",
        "                    subprocess.check_call([sys.executable, '-m', 'pip', 'install', package], \n",
        "                                        stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "                return True\n",
        "            except:\n",
        "                return False\n",
        "    \n",
        "    def setup_packages(self):\n",
        "        \"\"\"Install required packages efficiently\"\"\"\n",
        "        packages = {\n",
        "            'pandas': None,\n",
        "            'requests': None,\n",
        "            'pathlib': None,  # Built-in, but check anyway\n",
        "            'rpy2': 'rpy2',\n",
        "            'google.cloud.storage': 'google-cloud-storage'\n",
        "        }\n",
        "        \n",
        "        print(\"Installing/checking packages...\")\n",
        "        failed = []\n",
        "        \n",
        "        for pkg, install_name in packages.items():\n",
        "            install_name = install_name or pkg\n",
        "            if not self.install_package(pkg, install_name):\n",
        "                failed.append(pkg)\n",
        "        \n",
        "        if failed:\n",
        "            print(f\"⚠️  Failed to install: {', '.join(failed)}\")\n",
        "            print(\"Some features may not work\")\n",
        "        else:\n",
        "            print(\"✓ All packages ready\")\n",
        "        \n",
        "        # Mount Google Drive if needed\n",
        "        if DATA_SOURCE == \"drive\" and self.is_colab:\n",
        "            self.mount_drive()\n",
        "    \n",
        "    def mount_drive(self):\n",
        "        \"\"\"Mount Google Drive in Colab\"\"\"\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"✓ Google Drive mounted\")\n",
        "        except:\n",
        "            print(\"❌ Failed to mount Google Drive\")\n",
        "\n",
        "# Initialize environment\n",
        "env_manager = EnvironmentManager()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Data Loading\n",
        "\n",
        "Streamlined data loader with robust error handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "class DataLoader:\n",
        "    def __init__(self):\n",
        "        self.base_path = Path('/content') if env_manager.is_colab else Path.home()\n",
        "        self.data_dir = self.base_path / 'data'\n",
        "        self.data_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    def load_data(self):\n",
        "        \"\"\"Load data based on configuration\"\"\"\n",
        "        loaders = {\n",
        "            'github': self._load_github,\n",
        "            'local': self._load_local,\n",
        "            'gcs': self._load_gcs,\n",
        "            'drive': self._load_drive\n",
        "        }\n",
        "        \n",
        "        if DATA_SOURCE not in loaders:\n",
        "            raise ValueError(f\"Invalid DATA_SOURCE: {DATA_SOURCE}\")\n",
        "        \n",
        "        print(f\"Loading data from: {DATA_SOURCE.upper()}\")\n",
        "        return loaders[DATA_SOURCE]()\n",
        "    \n",
        "    def _load_github(self):\n",
        "        \"\"\"Load from GitHub releases\"\"\"\n",
        "        response = requests.get(GITHUB_URL, timeout=60)\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        data_path = self.data_dir / \"nass_data.csv\"\n",
        "        data_path.write_bytes(response.content)\n",
        "        \n",
        "        print(f\"✓ Downloaded from GitHub ({response.headers.get('content-length', 'unknown')} bytes)\")\n",
        "        return pd.read_csv(data_path)\n",
        "    \n",
        "    def _load_local(self):\n",
        "        \"\"\"Load from local file\"\"\"\n",
        "        search_paths = [\n",
        "            self.base_path / LOCAL_FILENAME,\n",
        "            self.data_dir / LOCAL_FILENAME,\n",
        "            Path.cwd() / LOCAL_FILENAME\n",
        "        ]\n",
        "        \n",
        "        for path in search_paths:\n",
        "            if path.exists():\n",
        "                print(f\"✓ Found local file: {path}\")\n",
        "                return pd.read_csv(path)\n",
        "        \n",
        "        raise FileNotFoundError(f\"File not found in: {[str(p) for p in search_paths]}\")\n",
        "    \n",
        "    def _load_gcs(self):\n",
        "        \"\"\"Load from Google Cloud Storage\"\"\"\n",
        "        from google.cloud import storage\n",
        "        \n",
        "        # Smart authentication\n",
        "        if Path(GCS_SERVICE_ACCOUNT_KEY).exists():\n",
        "            client = storage.Client.from_service_account_json(GCS_SERVICE_ACCOUNT_KEY)\n",
        "        else:\n",
        "            client = storage.Client()  # Use default credentials\n",
        "        \n",
        "        bucket = client.bucket(GCS_BUCKET)\n",
        "        blob = bucket.blob(GCS_BLOB)\n",
        "        \n",
        "        data_path = self.data_dir / \"nass_data.csv\"\n",
        "        blob.download_to_filename(data_path)\n",
        "        \n",
        "        print(f\"✓ Downloaded from GCS: {GCS_BUCKET}/{GCS_BLOB}\")\n",
        "        return pd.read_csv(data_path)\n",
        "    \n",
        "    def _load_drive(self):\n",
        "        \"\"\"Load from Google Drive (Colab only)\"\"\"\n",
        "        if not env_manager.is_colab:\n",
        "            raise RuntimeError(\"Drive loading only available in Google Colab\")\n",
        "        \n",
        "        drive_path = Path(DRIVE_PATH)\n",
        "        if not drive_path.exists():\n",
        "            raise FileNotFoundError(f\"Drive file not found: {DRIVE_PATH}\")\n",
        "        \n",
        "        print(f\"✓ Loading from Google Drive: {DRIVE_PATH}\")\n",
        "        return pd.read_csv(drive_path)\n",
        "\n",
        "# Load data\n",
        "try:\n",
        "    loader = DataLoader()\n",
        "    df = loader.load_data()\n",
        "    \n",
        "    print(f\"✅ Data loaded successfully!\")\n",
        "    print(f\"   Shape: {df.shape}\")\n",
        "    print(f\"   Memory: {df.memory_usage(deep=True).sum() / 1e6:.1f} MB\")\n",
        "    \n",
        "    if VERBOSE_PRINTS:\n",
        "        print(f\"\\nColumns: {list(df.columns)}\")\n",
        "        print(f\"\\nFirst 3 rows:\")\n",
        "        print(df.head(3))\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"❌ Data loading failed: {e}\")\n",
        "    print(f\"💡 Try changing DATA_SOURCE or check file paths\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. R Environment Setup\n",
        "\n",
        "Load R integration and install R packages efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load rpy2 extension for R integration\n",
        "try:\n",
        "    %load_ext rpy2.ipython\n",
        "    print(\"✓ R integration loaded\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Failed to load R integration: {e}\")\n",
        "    print(\"Install rpy2: pip install rpy2\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%R -i VERBOSE_PRINTS\n",
        "\n",
        "# Smart R package installation\n",
        "required_packages <- c(\"data.table\", \"survey\", \"dplyr\", \"tidyverse\", \n",
        "                      \"tidycensus\", \"ggplot2\", \"gridExtra\", \"pROC\", \n",
        "                      \"broom\", \"lme4\")\n",
        "\n",
        "# Check what's already installed\n",
        "installed_packages <- rownames(installed.packages())\n",
        "missing_packages <- required_packages[!required_packages %in% installed_packages]\n",
        "\n",
        "# Install missing packages\n",
        "if(length(missing_packages) > 0) {\n",
        "  cat(\"Installing R packages:\", paste(missing_packages, collapse=\", \"), \"\\n\")\n",
        "  install.packages(missing_packages, repos=\"https://cloud.r-project.org\", \n",
        "                   quiet=!VERBOSE_PRINTS, dependencies=TRUE)\n",
        "}\n",
        "\n",
        "# Load all packages\n",
        "success <- sapply(required_packages, function(pkg) {\n",
        "  suppressMessages(suppressWarnings(library(pkg, character.only=TRUE, quietly=TRUE)))\n",
        "})\n",
        "\n",
        "cat(\"✓ R packages loaded:\", sum(success), \"/\", length(required_packages), \"\\n\")\n",
        "if(any(!success)) {\n",
        "  cat(\"⚠️  Failed to load:\", paste(names(success)[!success], collapse=\", \"), \"\\n\")\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10dad708"
      },
      "outputs": [],
      "source": [
        "%%R -i df -i VERBOSE_PRINTS\n",
        "options(datatable.print.nrows = 10)\n",
        "\n",
        "NASS <- as.data.table(df)\n",
        "if (VERBOSE_PRINTS) print(NASS[1:10])\n",
        "\n",
        "# Light type coercion\n",
        "num_cols  <- c(\"AGE\",\"DISCWT\",\"TOTCHG\",\"TOTAL_AS_ENCOUNTERS\")\n",
        "NASS[, (num_cols) := lapply(.SD, as.numeric), .SDcols = num_cols]\n",
        "\n",
        "# Boolean helper\n",
        "NASS[, WHITE := fifelse(RACE == 1, 1, 0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7341f8ab"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "cat(\"Rows:\", nrow(NASS), \"  Cols:\", ncol(NASS), \"\\n\")\n",
        "top10 <- NASS[, .N, by = CPTCCS1][order(-N)][1:10]\n",
        "knitr::kable(top10, caption = \"Top 10 CPTCCS1 counts (simulated)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43230586"
      },
      "source": [
        "---\n",
        "## 9. R Analysis - Income Quartile vs Procedure\n",
        "\n",
        "Visualize income distribution within the most common procedures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee3556ca"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "top_codes <- top10$CPTCCS1\n",
        "plt_income <- NASS[CPTCCS1 %in% top_codes] |>\n",
        "  ggplot(aes(x = fct_infreq(CPTCCS1), fill = ZIPINC_QRTL)) +\n",
        "  geom_bar(position = \"fill\") +\n",
        "  scale_y_continuous(labels = scales::percent) +\n",
        "  coord_flip() +\n",
        "  labs(y = \"Share within CPT\", x = \"CPTCCS1\", fill = \"ZIP Quartile\",\n",
        "       title = \"Income distribution within 10 most-common procedures\")\n",
        "print(plt_income)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20a6a8a8"
      },
      "source": [
        "---\n",
        "## 10. Census API Setup\n",
        "\n",
        "Set up environment variable for Census API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d0c69b9"
      },
      "outputs": [],
      "source": [
        "import getpass, os, json, textwrap\n",
        "os.environ[\"CENSUS_API_KEY\"] = getpass.getpass(\"Enter your Census API key (will not echo):\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c39ad38"
      },
      "source": [
        "---\n",
        "## 11. R | Set Census Key & Pull 2020 DHC Totals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "240fd569"
      },
      "outputs": [],
      "source": [
        "%%R -i states_in_nass=character() -i VERBOSE_PRINTS\n",
        "# If you've already installed the key once, this is a no-op\n",
        "tidycensus::census_api_key(Sys.getenv(\"CENSUS_API_KEY\"), overwrite = FALSE, install = FALSE)\n",
        "\n",
        "get_vars <- function(base) sprintf(\"%s_%03dN\", base, 1:49)\n",
        "\n",
        "vars_total <- get_vars(\"P12\")\n",
        "vars_white <- get_vars(\"P12I\")\n",
        "\n",
        "pull_state_totals <- function(vars){\n",
        "  get_decennial(geography = \"state\",\n",
        "                variables = vars,\n",
        "                year = 2020, sumfile = \"dhc\") |>\n",
        "  group_by(NAME) |> summarise(total = sum(value))\n",
        "}\n",
        "\n",
        "total_pop  <- pull_state_totals(vars_total)\n",
        "white_pop  <- pull_state_totals(vars_white)\n",
        "\n",
        "census_prop <- merge(total_pop, white_pop, by = \"NAME\",\n",
        "                     suffixes = c(\"_all\",\"_white\"))\n",
        "census_prop[, prop_white := total_white / total_all]\n",
        "\n",
        "if (VERBOSE_PRINTS) head(census_prop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6239ac5"
      },
      "source": [
        "---\n",
        "## 12. R | Weighted vs Unweighted Proportion Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9a539f9"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "library(survey)\n",
        "\n",
        "# Survey design using provided discharge weight\n",
        "des <- svydesign(ids = ~1, weights = ~DISCWT, data = NASS)\n",
        "\n",
        "unweighted_hat <- mean(NASS$WHITE)\n",
        "weighted_hat   <- svymean(~WHITE, des)[1]\n",
        "\n",
        "us_prop <- weighted.mean(census_prop$prop_white,\n",
        "                         w = census_prop$total_all)\n",
        "\n",
        "cat(sprintf(\"Unweighted NASS white %%: %.3f\\n\", unweighted_hat))\n",
        "cat(sprintf(\"Weighted   NASS white %%: %.3f\\n\", weighted_hat))\n",
        "cat(sprintf(\"2020 Census (all NASS states) white %%: %.3f\\n\", us_prop))\n",
        "\n",
        "svytest <- svyciprop(~WHITE, des,\n",
        "                     method = \"likelihood\", level = 0.95)\n",
        "print(svytest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d30e6ec7"
      },
      "source": [
        "---\n",
        "## 13. R | Age-by-sex plot vs Census (adapted from `agesociodiv.r`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfb4756f"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "age_breaks <- c(-Inf,4,9,14,17,19,20,21,24,29,34,39,44,49,54,59,61,64,\n",
        "                66,69,74,79,84,Inf)\n",
        "age_labels <- c(\"U5\",\"5-9\",\"10-14\",\"15-17\",\"18-19\",\"20\",\"21\",\n",
        "                \"22-24\",\"25-29\",\"30-34\",\"35-39\",\"40-44\",\"45-49\",\n",
        "                \"50-54\",\"55-59\",\"60-61\",\"62-64\",\"65-66\",\"67-69\",\n",
        "                \"70-74\",\"75-79\",\"80-84\",\"85+\")\n",
        "\n",
        "NASS[, AGE_GROUP := cut(AGE, breaks = age_breaks,\n",
        "                        labels = age_labels, right = TRUE)]\n",
        "\n",
        "plot_df <- NASS[, .(white = sum(WHITE),\n",
        "                    n     = .N),\n",
        "                by = .(SEX = factor(FEMALE, labels=c(\"Male\",\"Female\")),\n",
        "                       AGE_GROUP)]\n",
        "plot_df[, prop := white/n]\n",
        "\n",
        "gg_gender <- ggplot(plot_df, aes(x = AGE_GROUP, y = prop,\n",
        "                                 group = SEX, color = SEX)) +\n",
        "  geom_line(linewidth=1) +\n",
        "  geom_point() +\n",
        "  scale_y_continuous(labels = scales::percent) +\n",
        "  labs(y = \"% White (NASS, simulated)\", x = \"Age-group\",\n",
        "       title = \"Crude white proportion by age & sex\") +\n",
        "  theme_minimal() +\n",
        "  theme(axis.text.x = element_text(angle=45, hjust=1))\n",
        "print(gg_gender)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "add395e2"
      },
      "source": [
        "---\n",
        "## 14. R | Multilevel logistic models (hospital nested, 3 tiers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8574bf3a"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "features <- NASS[, .(WHITE,\n",
        "                     FEMALE,\n",
        "                     ZIPINC_QRTL,\n",
        "                     PAY1,\n",
        "                     CPTCCS1,\n",
        "                     HOSP_LOCATION,\n",
        "                     HOSP_TEACH,\n",
        "                     HOSP_NASS)]\n",
        "\n",
        "features[, c(names(features)) := lapply(.SD, as.factor)]\n",
        "\n",
        "formulas <- list(\n",
        "  m1 = WHITE ~ FEMALE + (1|HOSP_NASS),\n",
        "  m2 = WHITE ~ FEMALE + ZIPINC_QRTL + (1|HOSP_NASS),\n",
        "  m3 = WHITE ~ FEMALE + ZIPINC_QRTL + PAY1 + CPTCCS1 +\n",
        "                    HOSP_LOCATION + HOSP_TEACH + (1|HOSP_NASS)\n",
        ")\n",
        "\n",
        "fit <- lapply(formulas, glmer, family = binomial, data = features,\n",
        "              control = glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e4)))\n",
        "\n",
        "sapply(fit, function(m) broom::tidy(m, effects = \"fixed\")[1:5,])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa999863"
      },
      "source": [
        "---\n",
        "## 15. R | Compare AUC across the three models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9781417"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "library(pROC)\n",
        "auc_vals <- sapply(fit, function(m){\n",
        "  preds <- predict(m, type=\"response\")\n",
        "  roc(features$WHITE, preds)$auc\n",
        "})\n",
        "knitr::kable(data.frame(model = names(auc_vals), AUC = auc_vals),\n",
        "             caption = \"AUC (in-sample, simulated data)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a2a8180"
      },
      "source": [
        "---\n",
        "## 16. Python | Teardown Helper (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38a0324d"
      },
      "outputs": [],
      "source": [
        "if not USE_DRIVE:\n",
        "    print(\"Done ✅ — runtime will auto-delete downloaded CSV when session ends.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMqcgUl+g+r3Kzj0zmsaBpz",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
