{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "[![Open on GitHub](https://img.shields.io/badge/GitHub-View%20Source-181717?style=for-the-badge&logo=github)](https://github.com/SeenaKhosravi/NASS/blob/main/NASS_Analysis.ipynb)\n",
        "[![Open In Colab](https://img.shields.io/badge/Colab-Open%20Notebook-F9AB00?style=for-the-badge&logo=google-colab)](https://colab.research.google.com/github/SeenaKhosravi/NASS/blob/main/NASS_Analysis.ipynb)\n",
        "[![Open in Vertex AI](https://img.shields.io/badge/Vertex%20AI-Open%20Workbench-4285F4?style=for-the-badge&logo=google-cloud)](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/SeenaKhosravi/NASS/main/NASS_Analysis.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbe6c3e6"
      },
      "source": [
        "# Socioeconomic and Demographic Drivers of Ambulatory Surgery Usage\n",
        "### HCUP NASS 2020 ‚Äì Reproducible Pipeline (Python + R)\n",
        "\n",
        "**Author:** Seena Khosravi, MD  \n",
        "**Last Updated:** September 2, 2025  \n",
        "**Data Source:** Healthcare Cost and Utilization Project (HCUP) NASS 2020\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "This notebook provides a reproducible analysis pipeline for examining socioeconomic and demographic factors influencing ambulatory surgery usage patterns. The analysis combines Python for data processing and R for statistical modeling.\n",
        "\n",
        "### Data Usage Agreement\n",
        "**DUA Compliant** ‚Äî This notebook uses a simulated dataset unless you have purchased the full dataset from HCUP. The full dataset can be loaded from your local storage or cloud storage.\n",
        "\n",
        "### Key Features\n",
        "- **Platform Agnostic:** Works in Jupyter, Colab, Vertex AI, or local environments\n",
        "- **Flexible Data Sources:** GitHub (simulated), Google Cloud Storage, or local files\n",
        "- **Reproducible:** All dependencies and environment setup included\n",
        "- **Scalable:** Handles both simulated (1GB, 700k rows) and full dataset (12 GB, 7.8M rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "019b6fe9"
      },
      "source": [
        "---\n",
        "\n",
        "## Design Notes\n",
        "\n",
        "### Architecture\n",
        "- **Python cells** handle \"plumbing\" (file I/O, environment setup, rpy2 configuration, data previews)\n",
        "- **R cells** (prefixed by `%%R`) perform statistical analysis: survey weights, Census lookups, multilevel models, plots\n",
        "\n",
        "### Data Sources\n",
        "- **Default:** Simulated dataset (1GB) from GitHub releases\n",
        "- **Local:** Switch to locally stored files via configuration\n",
        "- **Cloud:** Google Cloud Storage support for large datasets\n",
        "- **Full Dataset:** 7.8M row HCUP release (requires DUA compliance)\n",
        "\n",
        "### Environment Support\n",
        "- Jupyter Notebook\n",
        "- Google Colab\n",
        "- Vertex AI Workbench\n",
        "- Local Python/R environments\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f14bf61b"
      },
      "source": [
        "---\n",
        "## 1. Configuration & Setup\n",
        "\n",
        "### Data Source Configuration\n",
        "Choose your data source and toggle verbose output for debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9b279b8c"
      },
      "outputs": [],
      "source": [
        "# Configuration Settings\n",
        "# Choose your data source and toggle verbose mode for debugging\n",
        "\n",
        "USE_DRIVE = False       # True ‚Üí mount Google Drive and read full HCUP files\n",
        "VERBOSE_PRINTS = True   # False ‚Üí suppress head()/str() previews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1f7b978"
      },
      "source": [
        "---\n",
        "## 2. Optional Drive Mount\n",
        "\n",
        "For Google Colab users who want to access files from Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f60c4b73"
      },
      "outputs": [],
      "source": [
        "if USE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adc327a0"
      },
      "source": [
        "---\n",
        "## 3. Data Loading\n",
        "\n",
        "### Multiple Data Source Support\n",
        "This section provides a flexible data loader that supports GitHub, local files, and Google Cloud Storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ba8c1f1",
        "outputId": "2f658cb3-8c9e-4c3c-ce1a-098e7c530b06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úîÔ∏è Data ready ‚Üí /content/nass_2020_simulated.csv\n"
          ]
        }
      ],
      "source": [
        "# ==================== USER CONFIGURATION ====================\n",
        "DATA_SOURCE = \"github\"  # Options: \"local\", \"github\", \"gcs\"\n",
        "\n",
        "# GitHub source (default - simulated data)\n",
        "GITHUB_URL = \"https://github.com/SeenaKhosravi/NASS/releases/download/v1.0.0/nass_2020_simulated.csv\"\n",
        "\n",
        "# Local source (relative to environment's home directory)\n",
        "LOCAL_FILENAME = \"nass_2020_local.csv\"\n",
        "\n",
        "# Google Cloud Storage source\n",
        "GCS_BUCKET = \"nass_2020\"\n",
        "GCS_BLOB = \"nass_2020_all.csv\"\n",
        "USE_EXPLICIT_AUTH = False\n",
        "SERVICE_ACCOUNT_KEY_PATH = \"/path/to/service-account-key.json\"\n",
        "# ===========================================================\n",
        "\n",
        "import pathlib\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "class DataLoader:\n",
        "    \"\"\"Flexible data loader supporting multiple sources and environments\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.detect_environment()\n",
        "        self.setup_paths()\n",
        "        \n",
        "    def detect_environment(self):\n",
        "        \"\"\"Detect if we're in Colab, Vertex AI, or local environment\"\"\"\n",
        "        self.is_colab = 'COLAB_GPU' in os.environ\n",
        "        self.environment = \"colab\" if self.is_colab else \"vertex/local\"\n",
        "        print(f\"Environment detected: {self.environment}\")\n",
        "    \n",
        "    def setup_paths(self):\n",
        "        \"\"\"Set appropriate data directory based on environment\"\"\"\n",
        "        if self.is_colab:\n",
        "            self.base_path = pathlib.Path('/content')\n",
        "        else:\n",
        "            self.base_path = pathlib.Path.home()\n",
        "        \n",
        "        self.data_dir = self.base_path / 'data'\n",
        "        self.data_dir.mkdir(exist_ok=True)\n",
        "        print(f\"Data directory: {self.data_dir}\")\n",
        "    \n",
        "    def load_from_github(self):\n",
        "        \"\"\"Load simulated data from GitHub releases\"\"\"\n",
        "        try:\n",
        "            print(f\"Downloading from: {GITHUB_URL}\")\n",
        "            response = requests.get(GITHUB_URL, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            \n",
        "            data_path = self.data_dir / \"nass_data.csv\"\n",
        "            with open(data_path, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            \n",
        "            print(f\"‚úì Successfully downloaded from GitHub\")\n",
        "            return pd.read_csv(data_path)\n",
        "            \n",
        "        except Exception as e:\n",
        "            raise Exception(f\"GitHub download failed: {str(e)}\")\n",
        "    \n",
        "    def load_from_local(self):\n",
        "        \"\"\"Load data from local file\"\"\"\n",
        "        # Check multiple possible locations\n",
        "        possible_paths = [\n",
        "            self.base_path / LOCAL_FILENAME,\n",
        "            self.data_dir / LOCAL_FILENAME,\n",
        "            pathlib.Path(LOCAL_FILENAME)  # Current directory\n",
        "        ]\n",
        "        \n",
        "        for local_path in possible_paths:\n",
        "            if local_path.exists():\n",
        "                print(f\"‚úì Found local file: {local_path}\")\n",
        "                return pd.read_csv(local_path)\n",
        "        \n",
        "        raise FileNotFoundError(f\"Local file not found. Checked: {[str(p) for p in possible_paths]}\")\n",
        "    \n",
        "    def load_from_gcs(self):\n",
        "        \"\"\"Load data from Google Cloud Storage\"\"\"\n",
        "        try:\n",
        "            from google.cloud import storage\n",
        "            \n",
        "            # Setup GCS client\n",
        "            if USE_EXPLICIT_AUTH and os.path.exists(SERVICE_ACCOUNT_KEY_PATH):\n",
        "                client = storage.Client.from_service_account_json(SERVICE_ACCOUNT_KEY_PATH)\n",
        "            else:\n",
        "                client = storage.Client()  # Use default credentials\n",
        "            \n",
        "            bucket = client.bucket(GCS_BUCKET)\n",
        "            blob = bucket.blob(GCS_BLOB)\n",
        "            \n",
        "            if not blob.exists():\n",
        "                raise FileNotFoundError(f\"GCS blob not found: {GCS_BUCKET}/{GCS_BLOB}\")\n",
        "            \n",
        "            data_path = self.data_dir / \"nass_data.csv\"\n",
        "            blob.download_to_filename(data_path)\n",
        "            \n",
        "            print(f\"‚úì Successfully downloaded from GCS: {GCS_BUCKET}/{GCS_BLOB}\")\n",
        "            return pd.read_csv(data_path)\n",
        "            \n",
        "        except Exception as e:\n",
        "            raise Exception(f\"GCS loading failed: {str(e)}\")\n",
        "    \n",
        "    def load_data(self):\n",
        "        \"\"\"Main method to load data based on user selection\"\"\"\n",
        "        print(f\"Loading data from: {DATA_SOURCE.upper()}\")\n",
        "        \n",
        "        try:\n",
        "            if DATA_SOURCE.lower() == \"github\":\n",
        "                return self.load_from_github()\n",
        "            elif DATA_SOURCE.lower() == \"local\":\n",
        "                return self.load_from_local()\n",
        "            elif DATA_SOURCE.lower() == \"gcs\":\n",
        "                return self.load_from_gcs()\n",
        "            else:\n",
        "                raise ValueError(f\"Invalid DATA_SOURCE: {DATA_SOURCE}. Use 'github', 'local', or 'gcs'\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {str(e)}\")\n",
        "            self._print_troubleshooting()\n",
        "            raise\n",
        "    \n",
        "    def _print_troubleshooting(self):\n",
        "        \"\"\"Print helpful troubleshooting messages\"\"\"\n",
        "        if DATA_SOURCE == \"github\":\n",
        "            print(\"\\nüìã GitHub Troubleshooting:\")\n",
        "            print(\"1. Check internet connection\")\n",
        "            print(\"2. Verify URL accessibility\")\n",
        "            \n",
        "        elif DATA_SOURCE == \"local\":\n",
        "            print(\"\\nüìã Local File Troubleshooting:\")\n",
        "            print(f\"1. Check if file exists: {self.base_path / LOCAL_FILENAME}\")\n",
        "            print(f\"2. Or in data directory: {self.data_dir / LOCAL_FILENAME}\")\n",
        "            \n",
        "        elif DATA_SOURCE == \"gcs\":\n",
        "            print(\"\\nüìã GCS Troubleshooting:\")\n",
        "            print(\"1. Verify GCP authentication\")\n",
        "            print(\"2. Check bucket and file permissions\")\n",
        "            print(\"3. Ensure google-cloud-storage is installed\")\n",
        "\n",
        "# Execute data loading\n",
        "try:\n",
        "    loader = DataLoader()\n",
        "    df = loader.load_data()\n",
        "    \n",
        "    print(f\"\\n‚úÖ Data loaded successfully!\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    if VERBOSE_PRINTS:\n",
        "        print(\"\\nFirst 5 rows:\")\n",
        "        print(df.head())\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Failed to load data. Please check your configuration.\")\n",
        "    print(\"Consider switching DATA_SOURCE or checking your file paths.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4969c3bc"
      },
      "source": [
        "---\n",
        "## 4. Data Verification\n",
        "\n",
        "Verify the loaded data structure and preview first few rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic data verification\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(f\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "if VERBOSE_PRINTS:\n",
        "    print(f\"\\nMissing values:\")\n",
        "    print(df.isnull().sum())\n",
        "    print(f\"\\nBasic statistics:\")\n",
        "    print(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6be40ca3"
      },
      "source": [
        "---\n",
        "## 5. R Environment Setup\n",
        "\n",
        "### Load rpy2 Extension\n",
        "Enable R integration within Python notebook environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7f465618"
      },
      "outputs": [],
      "source": [
        "%load_ext rpy2.ipython"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75dcee11"
      },
      "source": [
        "---\n",
        "## 6. R Package Installation and Loading\n",
        "\n",
        "Install required R packages and load them for the analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82d4ec18"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "req_pkgs <- c(\"data.table\",\"survey\",\"dplyr\",\"tidyverse\",\"tidycensus\",\n",
        "              \"ggplot2\",\"gridExtra\",\"pROC\",\"broom\",\"lme4\")\n",
        "new <- req_pkgs[!req_pkgs %in% installed.packages()[,\"Package\"]]\n",
        "if(length(new)) install.packages(new, repos = \"https://cloud.r-project.org\")\n",
        "\n",
        "# Check which packages were successfully installed and load them\n",
        "installed <- installed.packages()[,\"Package\"]\n",
        "loaded_pkgs <- c()\n",
        "failed_pkgs <- c()\n",
        "for (pkg in req_pkgs) {\n",
        "  if (pkg %in% installed) {\n",
        "    library(pkg, character.only = TRUE)\n",
        "    loaded_pkgs <- c(loaded_pkgs, pkg)\n",
        "  } else {\n",
        "    failed_pkgs <- c(failed_pkgs, pkg)\n",
        "  }\n",
        "}\n",
        "\n",
        "if (length(failed_pkgs) > 0) {\n",
        "  cat(\"Warning: The following packages failed to install and were not loaded:\", paste(failed_pkgs, collapse = \", \"), \"\\n\")\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6f7737d"
      },
      "source": [
        "---\n",
        "## 7. R Data Processing\n",
        "\n",
        "Read the CSV with data.table (fast) and perform initial type coercion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10dad708"
      },
      "outputs": [],
      "source": [
        "%%R -i df -i VERBOSE_PRINTS\n",
        "options(datatable.print.nrows = 10)\n",
        "\n",
        "NASS <- as.data.table(df)\n",
        "if (VERBOSE_PRINTS) print(NASS[1:10])\n",
        "\n",
        "# Light type coercion\n",
        "num_cols  <- c(\"AGE\",\"DISCWT\",\"TOTCHG\",\"TOTAL_AS_ENCOUNTERS\")\n",
        "NASS[, (num_cols) := lapply(.SD, as.numeric), .SDcols = num_cols]\n",
        "\n",
        "# Boolean helper\n",
        "NASS[, WHITE := fifelse(RACE == 1, 1, 0)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e94d69c"
      },
      "source": [
        "---\n",
        "## 8. R Analysis - Headline Counts\n",
        "\n",
        "Replicate headline counts and verify data structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7341f8ab"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "cat(\"Rows:\", nrow(NASS), \"  Cols:\", ncol(NASS), \"\\n\")\n",
        "top10 <- NASS[, .N, by = CPTCCS1][order(-N)][1:10]\n",
        "knitr::kable(top10, caption = \"Top 10 CPTCCS1 counts (simulated)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43230586"
      },
      "source": [
        "---\n",
        "## 9. R Analysis - Income Quartile vs Procedure\n",
        "\n",
        "Visualize income distribution within the most common procedures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee3556ca"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "top_codes <- top10$CPTCCS1\n",
        "plt_income <- NASS[CPTCCS1 %in% top_codes] |>\n",
        "  ggplot(aes(x = fct_infreq(CPTCCS1), fill = ZIPINC_QRTL)) +\n",
        "  geom_bar(position = \"fill\") +\n",
        "  scale_y_continuous(labels = scales::percent) +\n",
        "  coord_flip() +\n",
        "  labs(y = \"Share within CPT\", x = \"CPTCCS1\", fill = \"ZIP Quartile\",\n",
        "       title = \"Income distribution within 10 most-common procedures\")\n",
        "print(plt_income)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20a6a8a8"
      },
      "source": [
        "---\n",
        "## 10. Census API Setup\n",
        "\n",
        "Set up environment variable for Census API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d0c69b9"
      },
      "outputs": [],
      "source": [
        "import getpass, os, json, textwrap\n",
        "os.environ[\"CENSUS_API_KEY\"] = getpass.getpass(\"Enter your Census API key (will not echo):\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c39ad38"
      },
      "source": [
        "---\n",
        "## 11. R | Set Census Key & Pull 2020 DHC Totals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "240fd569"
      },
      "outputs": [],
      "source": [
        "%%R -i states_in_nass=character() -i VERBOSE_PRINTS\n",
        "# If you've already installed the key once, this is a no-op\n",
        "tidycensus::census_api_key(Sys.getenv(\"CENSUS_API_KEY\"), overwrite = FALSE, install = FALSE)\n",
        "\n",
        "get_vars <- function(base) sprintf(\"%s_%03dN\", base, 1:49)\n",
        "\n",
        "vars_total <- get_vars(\"P12\")\n",
        "vars_white <- get_vars(\"P12I\")\n",
        "\n",
        "pull_state_totals <- function(vars){\n",
        "  get_decennial(geography = \"state\",\n",
        "                variables = vars,\n",
        "                year = 2020, sumfile = \"dhc\") |>\n",
        "  group_by(NAME) |> summarise(total = sum(value))\n",
        "}\n",
        "\n",
        "total_pop  <- pull_state_totals(vars_total)\n",
        "white_pop  <- pull_state_totals(vars_white)\n",
        "\n",
        "census_prop <- merge(total_pop, white_pop, by = \"NAME\",\n",
        "                     suffixes = c(\"_all\",\"_white\"))\n",
        "census_prop[, prop_white := total_white / total_all]\n",
        "\n",
        "if (VERBOSE_PRINTS) head(census_prop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6239ac5"
      },
      "source": [
        "---\n",
        "## 12. R | Weighted vs Unweighted Proportion Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9a539f9"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "library(survey)\n",
        "\n",
        "# Survey design using provided discharge weight\n",
        "des <- svydesign(ids = ~1, weights = ~DISCWT, data = NASS)\n",
        "\n",
        "unweighted_hat <- mean(NASS$WHITE)\n",
        "weighted_hat   <- svymean(~WHITE, des)[1]\n",
        "\n",
        "us_prop <- weighted.mean(census_prop$prop_white,\n",
        "                         w = census_prop$total_all)\n",
        "\n",
        "cat(sprintf(\"Unweighted NASS white %%: %.3f\\n\", unweighted_hat))\n",
        "cat(sprintf(\"Weighted   NASS white %%: %.3f\\n\", weighted_hat))\n",
        "cat(sprintf(\"2020 Census (all NASS states) white %%: %.3f\\n\", us_prop))\n",
        "\n",
        "svytest <- svyciprop(~WHITE, des,\n",
        "                     method = \"likelihood\", level = 0.95)\n",
        "print(svytest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d30e6ec7"
      },
      "source": [
        "---\n",
        "## 13. R | Age-by-sex plot vs Census (adapted from `agesociodiv.r`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfb4756f"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "age_breaks <- c(-Inf,4,9,14,17,19,20,21,24,29,34,39,44,49,54,59,61,64,\n",
        "                66,69,74,79,84,Inf)\n",
        "age_labels <- c(\"U5\",\"5-9\",\"10-14\",\"15-17\",\"18-19\",\"20\",\"21\",\n",
        "                \"22-24\",\"25-29\",\"30-34\",\"35-39\",\"40-44\",\"45-49\",\n",
        "                \"50-54\",\"55-59\",\"60-61\",\"62-64\",\"65-66\",\"67-69\",\n",
        "                \"70-74\",\"75-79\",\"80-84\",\"85+\")\n",
        "\n",
        "NASS[, AGE_GROUP := cut(AGE, breaks = age_breaks,\n",
        "                        labels = age_labels, right = TRUE)]\n",
        "\n",
        "plot_df <- NASS[, .(white = sum(WHITE),\n",
        "                    n     = .N),\n",
        "                by = .(SEX = factor(FEMALE, labels=c(\"Male\",\"Female\")),\n",
        "                       AGE_GROUP)]\n",
        "plot_df[, prop := white/n]\n",
        "\n",
        "gg_gender <- ggplot(plot_df, aes(x = AGE_GROUP, y = prop,\n",
        "                                 group = SEX, color = SEX)) +\n",
        "  geom_line(linewidth=1) +\n",
        "  geom_point() +\n",
        "  scale_y_continuous(labels = scales::percent) +\n",
        "  labs(y = \"% White (NASS, simulated)\", x = \"Age-group\",\n",
        "       title = \"Crude white proportion by age & sex\") +\n",
        "  theme_minimal() +\n",
        "  theme(axis.text.x = element_text(angle=45, hjust=1))\n",
        "print(gg_gender)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "add395e2"
      },
      "source": [
        "---\n",
        "## 14. R | Multilevel logistic models (hospital nested, 3 tiers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8574bf3a"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "features <- NASS[, .(WHITE,\n",
        "                     FEMALE,\n",
        "                     ZIPINC_QRTL,\n",
        "                     PAY1,\n",
        "                     CPTCCS1,\n",
        "                     HOSP_LOCATION,\n",
        "                     HOSP_TEACH,\n",
        "                     HOSP_NASS)]\n",
        "\n",
        "features[, c(names(features)) := lapply(.SD, as.factor)]\n",
        "\n",
        "formulas <- list(\n",
        "  m1 = WHITE ~ FEMALE + (1|HOSP_NASS),\n",
        "  m2 = WHITE ~ FEMALE + ZIPINC_QRTL + (1|HOSP_NASS),\n",
        "  m3 = WHITE ~ FEMALE + ZIPINC_QRTL + PAY1 + CPTCCS1 +\n",
        "                    HOSP_LOCATION + HOSP_TEACH + (1|HOSP_NASS)\n",
        ")\n",
        "\n",
        "fit <- lapply(formulas, glmer, family = binomial, data = features,\n",
        "              control = glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e4)))\n",
        "\n",
        "sapply(fit, function(m) broom::tidy(m, effects = \"fixed\")[1:5,])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa999863"
      },
      "source": [
        "---\n",
        "## 15. R | Compare AUC across the three models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9781417"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "library(pROC)\n",
        "auc_vals <- sapply(fit, function(m){\n",
        "  preds <- predict(m, type=\"response\")\n",
        "  roc(features$WHITE, preds)$auc\n",
        "})\n",
        "knitr::kable(data.frame(model = names(auc_vals), AUC = auc_vals),\n",
        "             caption = \"AUC (in-sample, simulated data)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a2a8180"
      },
      "source": [
        "---\n",
        "## 16. Python | Teardown Helper (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38a0324d"
      },
      "outputs": [],
      "source": [
        "if not USE_DRIVE:\n",
        "    print(\"Done ‚úÖ ‚Äî runtime will auto-delete downloaded CSV when session ends.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMqcgUl+g+r3Kzj0zmsaBpz",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
