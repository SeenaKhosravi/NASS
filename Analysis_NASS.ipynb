{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open on GitHub](https://img.shields.io/badge/GitHub-View%20Source-181717?style=for-the-badge&logo=github)](https://github.com/SeenaKhosravi/NASS/blob/main/Analysis_NASS.ipynb)\n",
    "[![Open In Colab](https://img.shields.io/badge/Colab-Open%20Notebook-F9AB00?style=for-the-badge&logo=google-colab)](https://colab.research.google.com/github/SeenaKhosravi/NASS/blob/main/Analysis_NASS.ipynb)\n",
    "[![Launch on GCE](https://img.shields.io/badge/Google%20Cloud-Launch%20Instance-4285F4?style=for-the-badge&logo=google-cloud)](https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https://github.com/SeenaKhosravi/NASS&cloudshell_tutorial=deploy/gce-tutorial.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbe6c3e6"
   },
   "source": [
    "# Socioeconomic and Demographic Drivers of Ambulatory Surgery Usage\n",
    "### HCUP NASS 2020 – Reproducible Pipeline (Python + R)\n",
    "\n",
    "**Author:** Seena Khosravi, MD  \n",
    "**LLMs Utilized:** Claude Sonnet 4, Opus 4; ChatGPT 4o, o4; Deepseek 3.1; Gemini 2.5 Pro  \n",
    "**Last Updated:** September 14, 2025  \n",
    "\n",
    "\n",
    "**Data Source:**  \n",
    "Department of Health & Human Services (HHS)  \n",
    "Agency for Healthcare Research and Quality (AHRQ)  \n",
    "Healthcare Cost and Utilization Project (HCUP)  \n",
    "National Ambulatory Surgical Sample (NASS) 2020\n",
    "\n",
    " **[HCUP NASS 2020 Introduction](https://hcup-us.ahrq.gov/db/nation/nass/NASS_Introduction_2020.jsp)** - Official AHRQ documentation\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOmuZCsusrTj"
   },
   "source": [
    "\n",
    "## Overview\n",
    "This notebook provides a reproducible analysis pipeline for examining socioeconomic and demographic factors influencing ambulatory surgery usage patterns. The analysis combines Python for data processing and R for statistical modeling.\n",
    "\n",
    "### Data Usage Agreement\n",
    "**DUA Compliant Online Implementation** — This notebook uses a simulated, artificial, smaller dataset with identical structure to the file created by [Raw_NASS_Processing.R](https://github.com/SeenaKhosravi/NASS/blob/a7764ce80be8a82fc449831821c27d957176c410/Raw%20NASS%20%20Processing.R). The simulated dataset production methodology is found in [Generate_Simulated_NASS.R](https://github.com/SeenaKhosravi/NASS/blob/161bf2b5c149da9654c0e887655b361fa2176db0/Generate_Simulated_NASS.R). If DUA signed and data purchased from HCUP, this notebook can run on full dataset loaded from your local or cloud storage.\n",
    "\n",
    "[Please see the DUA Agreement here.](https://hcup-us.ahrq.gov/team/NationwideDUA.jsp)\n",
    "\n",
    "### Key Features\n",
    "- **Multi-platform:** Works on Jupyter implementations via local environments, server, cloud VM instance, or platform as a service\n",
    "- **Flexible Data Storage:** GitHub (simulated, static, open access), Google Drive, Google Cloud Storage, or local file\n",
    "- **Reproducible:** All dependencies and environment setup included; Automated, Click-through VM set-up\n",
    "- **Scalable:** Handles both simulated (0.2GB, 139k rows) and full dataset (12GB, 7.8M rows) with scalable cloud options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "019b6fe9"
   },
   "source": [
    "---\n",
    "\n",
    "## Design Notes\n",
    "\n",
    "### Architecture\n",
    "- **Python primary, w/ R run via rpy2 python extension**\n",
    "- **Python cells** handle \"plumbing\" (file I/O, environment setup, rpy2 configuration, data previews)\n",
    "- **R cells** (prefixed by `%%R`) perform statistical analysis: survey weights, Census lookups, multilevel models, plots, classifiers, etc.\n",
    "\n",
    "### Data Sources\n",
    "- **Default:** Simulated dataset (0.2GB) from GitHub releases\n",
    "- **Local:** Switch to locally stored files via configuration\n",
    "- **Drive:** Google Drive (Only available in Colab)\n",
    "- **Cloud:** Google Cloud Storage support for large datasets\n",
    "\n",
    "### Environment Support\n",
    "- **Local:** JupyterLab w/ Python 3.8+ kernel (requires R 4.0+, 4.4+ preferred)\n",
    "- **Jupyter Server:** May require configuration depending on implementation (conda recommended)\n",
    "- **Google Colab:** Pro recommended for full dataset, high-RAM runtime suggested\n",
    "- **Virtual Machine Instance:** Automated GCE deployment via scripts, pre-configured R setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvQPKX8GrfZr"
   },
   "source": [
    "---\n",
    "\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUEIOQlXrPrd"
   },
   "source": [
    "\n",
    "## 1. Configuration\n",
    "\n",
    "Configure all settings here prior to run - data sources, debugging options, and file paths. Defaults to simulated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUU1wwMKrPre"
   },
   "outputs": [],
   "source": [
    "# ==================== CONFIGURATION ====================\n",
    "# Data Source Options\n",
    "DATA_SOURCE = \"github\"      # Options: \"github\", \"local\", \"gcs\", \"drive\"\n",
    "VERBOSE_PRINTS = True    # False → suppress debug output\n",
    "\n",
    "# GitHub source (default - simulated data)\n",
    "GITHUB_URL = \"https://github.com/SeenaKhosravi/NASS/releases/download/v1.0.0/nass_2020_simulated.csv\"\n",
    "\n",
    "# Local file options\n",
    "LOCAL_FILENAME = \"nass_2020_local.csv\"\n",
    "\n",
    "# Google Cloud Storage options (auto-detects user's buckets)\n",
    "DATA_SOURCE = \"gcs\"\n",
    "GCS_BUCKET = \"nass_2020\"                    # Your bucket\n",
    "GCS_BLOB = \"nass_2020_all.csv\"              # Your file  \n",
    "AUTO_DETECT_USER_GCS = False                # No project needed for public data\n",
    "\n",
    "# Google Drive options (for Colab)\n",
    "DRIVE_PATH = \"/content/drive/MyDrive/NASS/nass_2020_full.csv\"\n",
    "# ======================================================\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  Data source: {DATA_SOURCE}\")\n",
    "print(f\"  Auto-detect user GCS: {AUTO_DETECT_USER_GCS}\")\n",
    "print(f\"  Verbose mode: {VERBOSE_PRINTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WJ8_5sRrPre"
   },
   "source": [
    "---\n",
    "## 2. Python Environment Setup \n",
    "\n",
    "Detect environment and install Python packages via Conda if available, with fallbacks. If in Google Colab, mount Google Drive.\n",
    "\n",
    "**Note:** If running in Vertex AI Workbench for the first time, you need additional R setup prior to loading rpy2. Please skip the following cell and return after completing the subsequent cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9RcJjrWrPrf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "class EnvironmentManager:\n",
    "    def __init__(self):\n",
    "        self.detect_environment()\n",
    "        self.setup_packages()\n",
    "\n",
    "    def detect_environment(self):\n",
    "        \"\"\"Detect runtime environment\"\"\"\n",
    "        self.is_colab = 'COLAB_GPU' in os.environ or 'google.colab' in sys.modules\n",
    "        self.is_vertex = 'DL_ANACONDA_HOME' in os.environ\n",
    "\n",
    "        if self.is_colab:\n",
    "            self.env_type = \"Google Colab\"\n",
    "        elif self.is_vertex:\n",
    "            self.env_type = \"Vertex AI\"\n",
    "        else:\n",
    "            self.env_type = \"Local/Jupyter\"\n",
    "\n",
    "        print(f\"Environment detected: {self.env_type}\")\n",
    "\n",
    "    def check_conda_available(self):\n",
    "        \"\"\"Check if conda is available\"\"\"\n",
    "        try:\n",
    "            subprocess.check_call(['conda', '--version'],\n",
    "                                stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "            return True\n",
    "        except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "            return False\n",
    "\n",
    "    def install_package(self, package, conda_name=None):\n",
    "        \"\"\"Smart package installation with fallback\"\"\"\n",
    "        try:\n",
    "            __import__(package)\n",
    "            return True\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "\n",
    "            # Try conda first if available and not in Colab\n",
    "            if conda_name and not self.is_colab and self.check_conda_available():\n",
    "                try:\n",
    "                    subprocess.check_call(['conda', 'install', '-y', conda_name],\n",
    "                                        stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "                    return True\n",
    "                except subprocess.CalledProcessError:\n",
    "                    print(f\"  Conda install failed for {conda_name}, trying pip...\")\n",
    "\n",
    "            # Fallback to pip\n",
    "            try:\n",
    "                subprocess.check_call([sys.executable, '-m', 'pip', 'install', package],\n",
    "                                    stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "                return True\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"  Pip install failed for {package}: {e}\")\n",
    "                return False\n",
    "\n",
    "    def setup_packages(self):\n",
    "        \"\"\"Install required packages efficiently\"\"\"\n",
    "        packages = {\n",
    "            'pandas': 'pandas',\n",
    "            'requests': 'requests',\n",
    "            'rpy2': 'rpy2',\n",
    "            'google.cloud.storage': 'google-cloud-storage'\n",
    "        }\n",
    "\n",
    "        print(\"Installing and checking packages...\")\n",
    "        failed = []\n",
    "\n",
    "        for pkg, install_name in packages.items():\n",
    "            if not self.install_package(pkg, install_name):\n",
    "                failed.append(pkg)\n",
    "\n",
    "        # Store failed packages globally for recovery\n",
    "        globals()['failed_packages'] = failed\n",
    "\n",
    "        if failed:\n",
    "            print(f\"Warning: Failed to install: {', '.join(failed)}\")\n",
    "            print(\"Some features may not work\")\n",
    "\n",
    "            # Provide specific guidance for rpy2\n",
    "            if 'rpy2' in failed:\n",
    "                print(\"\\nFor rpy2 installation issues:\")\n",
    "                if self.is_vertex:\n",
    "                    print(\"   - Vertex AI: R may not be installed by default\")\n",
    "                    print(\"   - Run the next cell for automated R setup\")\n",
    "                else:\n",
    "                    print(\"   - On Windows: May need Visual Studio Build Tools\")\n",
    "                    print(\"   - Try: conda install -c conda-forge rpy2\")\n",
    "                    print(\"   - Or: pip install rpy2 (requires R to be installed)\")\n",
    "        else:\n",
    "            print(\"All packages ready\")\n",
    "\n",
    "        # Mount Google Drive if needed (check if DATA_SOURCE exists)\n",
    "        try:\n",
    "            if globals().get('DATA_SOURCE') == \"drive\" and self.is_colab:\n",
    "                self.mount_drive()\n",
    "        except NameError:\n",
    "            pass  # DATA_SOURCE not defined yet\n",
    "\n",
    "    def mount_drive(self):\n",
    "        \"\"\"Mount Google Drive in Colab\"\"\"\n",
    "        try:\n",
    "            from google.colab import drive\n",
    "            drive.mount('/content/drive')\n",
    "            print(\"Google Drive mounted successfully\")\n",
    "        except:\n",
    "            print(\"Error: Failed to mount Google Drive\")\n",
    "\n",
    "# Initialize environment\n",
    "env_manager = EnvironmentManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mm4dTQb1rPrh"
   },
   "source": [
    "---\n",
    "## 3. R Environment Setup\n",
    "\n",
    "Load R integration and install R packages efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wpUbizDrPrh"
   },
   "outputs": [],
   "source": [
    "# Load rpy2 extension for R integration\n",
    "try:\n",
    "    %load_ext rpy2.ipython\n",
    "    print(\"R integration loaded successfully\")\n",
    "    globals()['R_AVAILABLE'] = True\n",
    "except Exception as e:\n",
    "    print(f\"Error: Failed to load R integration: {e}\")\n",
    "\n",
    "    # Windows-specific troubleshooting\n",
    "    if \"R.dll\" in str(e) or \"error 0x7e\" in str(e):\n",
    "        print(\"\\nWindows R.dll loading issue detected:\")\n",
    "        print(\"   This is a common Windows + rpy2 compatibility issue\")\n",
    "        print(\"   Solutions:\")\n",
    "        print(\"   1. Restart Python kernel and try again\")\n",
    "        print(\"   2. Check R version compatibility with rpy2\")\n",
    "        print(\"   3. Try reinstalling R and rpy2\")\n",
    "        print(\"   4. Use Python-only analysis (fallback available)\")\n",
    "        globals()['R_AVAILABLE'] = False\n",
    "    else:\n",
    "        print(\"Install rpy2: pip install rpy2\")\n",
    "        globals()['R_AVAILABLE'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5iP0V_1rPri"
   },
   "source": [
    "Install essential R packages and attempt installation of optional packages.\n",
    "\n",
    "**Note:** Other packages needed for specific analysis (advanced modeling packages) will be installed and called as needed later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGFfQQC5rPri"
   },
   "outputs": [],
   "source": [
    "%%R -i VERBOSE_PRINTS\n",
    "\n",
    "# Environment-aware R package setup for Local, Colab, and GCE/Linux VMs\n",
    "\n",
    "# Detect environment\n",
    "is_colab <- Sys.getenv(\"COLAB_GPU\") != \"\"\n",
    "is_gce <- file.exists(\"/opt/nass\") || file.exists(\"/etc/apt\")  # GCE/Linux VM indicators\n",
    "\n",
    "if(is_colab) {\n",
    "  cat(\"Google Colab detected\\n\")\n",
    "} else if(is_gce) {\n",
    "  cat(\"GCE/Linux VM detected\\n\")\n",
    "} else {\n",
    "  cat(\"Local environment detected\\n\")\n",
    "}\n",
    "\n",
    "# Essential packages for all environments\n",
    "essential_packages <- c(\n",
    "  \"data.table\",    # Fast data manipulation\n",
    "  \"ggplot2\",       # Plotting\n",
    "  \"scales\"         # For ggplot2 percentage scales\n",
    ")\n",
    "\n",
    "optional_packages <- c(\n",
    "  \"survey\"        # Survey statistics\n",
    ")\n",
    "\n",
    "# Fast installation settings\n",
    "repos <- \"https://cloud.r-project.org\"\n",
    "options(repos = repos)\n",
    "Sys.setenv(MAKEFLAGS = paste0(\"-j\", parallel::detectCores()))\n",
    "\n",
    "# Package check and load functions\n",
    "pkg_available <- function(pkg) {\n",
    "  tryCatch({\n",
    "    find.package(pkg, quiet = TRUE)\n",
    "    TRUE\n",
    "  }, error = function(e) FALSE)\n",
    "}\n",
    "\n",
    "load_pkg <- function(pkg) {\n",
    "  tryCatch({\n",
    "    suppressMessages(library(pkg, character.only = TRUE, quietly = TRUE))\n",
    "    TRUE\n",
    "  }, error = function(e) FALSE)\n",
    "}\n",
    "\n",
    "# Install missing essential packages\n",
    "missing_essential <- essential_packages[!sapply(essential_packages, pkg_available)]\n",
    "\n",
    "if(length(missing_essential) > 0) {\n",
    "  cat(\"Installing essential packages:\", paste(missing_essential, collapse = \", \"), \"\\n\")\n",
    "\n",
    "  tryCatch({\n",
    "    install.packages(missing_essential,\n",
    "                    repos = repos,\n",
    "                    type = getOption(\"pkgType\"),\n",
    "                    dependencies = FALSE,\n",
    "                    quiet = !VERBOSE_PRINTS,\n",
    "                    Ncpus = parallel::detectCores())\n",
    "  }, error = function(e) {\n",
    "    cat(\"Binary install failed, trying source...\\n\")\n",
    "    install.packages(missing_essential,\n",
    "                    repos = repos,\n",
    "                    type = \"source\",\n",
    "                    dependencies = FALSE,\n",
    "                    quiet = !VERBOSE_PRINTS)\n",
    "  })\n",
    "}\n",
    "\n",
    "# Load essential packages\n",
    "essential_loaded <- sapply(essential_packages, load_pkg)\n",
    "essential_success <- sum(essential_loaded)\n",
    "\n",
    "cat(\"Essential packages loaded:\", essential_success, \"/\", length(essential_packages), \"\\n\")\n",
    "\n",
    "# Quick install optional packages (30s timeout)\n",
    "missing_optional <- optional_packages[!sapply(optional_packages, pkg_available)]\n",
    "\n",
    "if(length(missing_optional) > 0) {\n",
    "  cat(\"Installing optional packages...\\n\")\n",
    "\n",
    "  for(pkg in missing_optional) {\n",
    "    tryCatch({\n",
    "      setTimeLimit(cpu = 30, elapsed = 30, transient = TRUE)\n",
    "      install.packages(pkg, repos = repos,\n",
    "                      type = getOption(\"pkgType\"),\n",
    "                      dependencies = FALSE,\n",
    "                      quiet = TRUE)\n",
    "      cat(\"Installed:\", pkg, \"\\n\")\n",
    "    }, error = function(e) {\n",
    "      cat(\"Skipped (timeout):\", pkg, \"\\n\")\n",
    "    })\n",
    "\n",
    "    setTimeLimit(cpu = Inf, elapsed = Inf, transient = FALSE)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Load optional packages\n",
    "optional_loaded <- sapply(optional_packages, load_pkg)\n",
    "optional_success <- sum(optional_loaded)\n",
    "\n",
    "cat(\"Optional packages loaded:\", optional_success, \"/\", length(optional_packages), \"\\n\")\n",
    "\n",
    "# Check core functionality\n",
    "has_datatable <- require(\"data.table\", quietly = TRUE)\n",
    "has_ggplot <- require(\"ggplot2\", quietly = TRUE)\n",
    "\n",
    "# Enhanced GCE/Linux setup if packages are missing\n",
    "if(is_gce && (!has_datatable || !has_ggplot)) {\n",
    "  cat(\"\\nGCE/Linux detected - attempting enhanced installation...\\n\")\n",
    "\n",
    "  # System dependencies for Linux VMs\n",
    "  if(Sys.which(\"apt-get\") != \"\") {\n",
    "    cat(\"Installing system dependencies...\\n\")\n",
    "    system_deps <- c(\n",
    "      \"apt-get update -qq\",\n",
    "      \"apt-get install -y libfontconfig1-dev libcairo2-dev\",\n",
    "      \"apt-get install -y libxml2-dev libcurl4-openssl-dev libssl-dev\",\n",
    "      \"apt-get install -y libharfbuzz-dev libfribidi-dev\",\n",
    "      \"apt-get install -y libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev\"\n",
    "    )\n",
    "    \n",
    "    for(cmd in system_deps) {\n",
    "      system(paste(\"sudo\", cmd), ignore.stdout = TRUE, ignore.stderr = TRUE)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  # Retry failed packages with multiple repositories\n",
    "  failed_packages <- c()\n",
    "  if(!has_datatable) failed_packages <- c(failed_packages, \"data.table\")\n",
    "  if(!has_ggplot) failed_packages <- c(failed_packages, \"ggplot2\", \"scales\")\n",
    "\n",
    "  repos_gce <- c(\"https://cran.rstudio.com/\", \"https://cloud.r-project.org\")\n",
    "\n",
    "  for(pkg in failed_packages) {\n",
    "    cat(\"Installing\", pkg, \"...\")\n",
    "    installed <- FALSE\n",
    "\n",
    "    for(repo in repos_gce) {\n",
    "      tryCatch({\n",
    "        install.packages(pkg, repos = repo, dependencies = TRUE, quiet = TRUE)\n",
    "        if(require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "          cat(\" Success\\n\")\n",
    "          installed <- TRUE\n",
    "          break\n",
    "        }\n",
    "      }, error = function(e) NULL)\n",
    "    }\n",
    "\n",
    "    if(!installed) cat(\" FAILED\\n\")\n",
    "  }\n",
    "\n",
    "  # Re-check after enhanced installation\n",
    "  has_datatable <- require(\"data.table\", quietly = TRUE)\n",
    "  has_ggplot <- require(\"ggplot2\", quietly = TRUE)\n",
    "\n",
    "  cat(\"After enhanced installation: data.table =\", has_datatable, \"| ggplot2 =\", has_ggplot, \"\\n\")\n",
    "}\n",
    "\n",
    "# Final status check\n",
    "if(has_datatable && has_ggplot) {\n",
    "  cat(\"Core environment ready! (data.table + ggplot2)\\n\")\n",
    "  setDTthreads(0)  # Use all cores\n",
    "\n",
    "} else if(has_datatable) {\n",
    "  cat(\"Warning: Partial setup - data.table ready, plotting may be limited\\n\")\n",
    "  setDTthreads(0)\n",
    "\n",
    "} else {\n",
    "  cat(\"Error: Critical failure - data.table not available\\n\")\n",
    "  stop(\"Cannot proceed without data.table\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAtjPad_rPri"
   },
   "source": [
    "Verify R setup is complete and ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMh-C5gvrPrj"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Quick verification and setup\n",
    "cat(\"Verifying R environment...\\n\")\n",
    "\n",
    "# Test core functionality\n",
    "tryCatch({\n",
    "  # Test data.table (essential)\n",
    "  dt_test <- data.table(x = 1:3, y = letters[1:3])\n",
    "  cat(\"data.table ready\\n\")\n",
    "\n",
    "  # Test ggplot2 (optional)\n",
    "  if(require(\"ggplot2\", quietly = TRUE)) {\n",
    "    cat(\"ggplot2 ready\\n\")\n",
    "  } else {\n",
    "    cat(\"Warning: ggplot2 not available (plots disabled)\\n\")\n",
    "  }\n",
    "\n",
    "  # Set up data.table options for performance\n",
    "  setDTthreads(0)  # Use all cores\n",
    "\n",
    "  cat(\"R environment optimized and ready!\\n\")\n",
    "\n",
    "}, error = function(e) {\n",
    "  cat(\"Error: R environment verification failed:\", e$message, \"\\n\")\n",
    "  stop(\"R setup incomplete\")\n",
    "})\n",
    "\n",
    "# Clean up test objects\n",
    "rm(list = ls()[!ls() %in% c(\"VERBOSE_PRINTS\")])\n",
    "invisible(gc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8pUXVbsrPrg"
   },
   "source": [
    "---\n",
    "## 4. Data Loading\n",
    "\n",
    "Config based data loader with error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-KUtlK-rPrh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Optional, Dict, Any\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "class NASSDataLoader:\n",
    "    \"\"\"Data loader for NASS dataset with proper error handling and progress reporting.\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose: bool = True):\n",
    "        self.verbose = verbose\n",
    "        self.data_dir = Path.home() / 'data'\n",
    "        self.data_dir.mkdir(exist_ok=True)\n",
    "        self._setup_environment()\n",
    "    \n",
    "    def _setup_environment(self):\n",
    "        \"\"\"Detect environment and set appropriate settings.\"\"\"\n",
    "        self.is_colab = 'google.colab' in sys.modules\n",
    "        self.is_local = not self.is_colab\n",
    "        \n",
    "        if self.verbose:\n",
    "            env_name = \"Google Colab\" if self.is_colab else \"Local/Jupyter\"\n",
    "            print(f\"Environment: {env_name}\")\n",
    "    \n",
    "    def load_data(self, source: str, **config) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Load data based on source configuration.\n",
    "        \n",
    "        Args:\n",
    "            source: One of 'github', 'local', 'drive', 'cloud'\n",
    "            **config: Source-specific configuration parameters\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame if successful, None if failed\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"Loading data from source: {source}\")\n",
    "        \n",
    "        try:\n",
    "            if source == \"github\":\n",
    "                return self._load_from_github(config.get('url'))\n",
    "            elif source == \"local\":\n",
    "                return self._load_from_local(config.get('filename'))\n",
    "            elif source == \"drive\" and self.is_colab:\n",
    "                return self._load_from_drive(config.get('path'))\n",
    "            elif source == \"cloud\":\n",
    "                return self._load_from_cloud(config)\n",
    "            else:\n",
    "                self._report_error(f\"Unsupported source '{source}' for current environment\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            self._report_error(f\"Failed to load from {source}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _load_from_github(self, url: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Load data from GitHub public URL.\"\"\"\n",
    "        if not url:\n",
    "            self._report_error(\"GitHub URL not provided in configuration\")\n",
    "            return None\n",
    "        \n",
    "        cache_file = self.data_dir / \"nass_github_cache.csv\"\n",
    "        \n",
    "        # Check cache first\n",
    "        if cache_file.exists():\n",
    "            cache_age = time.time() - cache_file.stat().st_mtime\n",
    "            if cache_age < 3600:  # 1 hour cache\n",
    "                if self.verbose:\n",
    "                    print(\"✓ Using cached GitHub data\")\n",
    "                return pd.read_csv(cache_file)\n",
    "        \n",
    "        try:\n",
    "            if self.verbose:\n",
    "                print(\"Downloading from GitHub...\")\n",
    "            \n",
    "            # Stream download with progress\n",
    "            response = requests.get(url, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            with open(cache_file, 'wb') as f:\n",
    "                downloaded = 0\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        downloaded += len(chunk)\n",
    "                        if self.verbose and total_size > 0:\n",
    "                            progress = (downloaded / total_size) * 100\n",
    "                            print(f\"\\rProgress: {progress:.1f}%\", end='', flush=True)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"\\n✓ Downloaded {downloaded / (1024*1024):.1f} MB\")\n",
    "            \n",
    "            return pd.read_csv(cache_file)\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            self._report_error(f\"GitHub download failed: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _load_from_local(self, filename: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Load data from local file.\"\"\"\n",
    "        if not filename:\n",
    "            self._report_error(\"Local filename not provided in configuration\")\n",
    "            return None\n",
    "        \n",
    "        # Try multiple possible locations\n",
    "        possible_paths = [\n",
    "            Path(filename),\n",
    "            self.data_dir / filename,\n",
    "            Path.cwd() / filename,\n",
    "            Path.cwd() / 'data' / filename\n",
    "        ]\n",
    "        \n",
    "        for file_path in possible_paths:\n",
    "            if file_path.exists():\n",
    "                try:\n",
    "                    if self.verbose:\n",
    "                        size_mb = file_path.stat().st_size / (1024*1024)\n",
    "                        print(f\"Loading local file: {file_path} ({size_mb:.1f} MB)\")\n",
    "                    \n",
    "                    # Use chunked reading for large files\n",
    "                    if file_path.stat().st_size > 100 * 1024 * 1024:  # 100MB\n",
    "                        return self._load_large_csv(file_path)\n",
    "                    else:\n",
    "                        return pd.read_csv(file_path)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    self._report_error(f\"Failed to read {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        self._report_error(f\"Local file '{filename}' not found in any expected location\")\n",
    "        return None\n",
    "    \n",
    "    def _load_from_drive(self, drive_path: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Load data from Google Drive (Colab only).\"\"\"\n",
    "        if not self.is_colab:\n",
    "            self._report_error(\"Google Drive access only available in Colab\")\n",
    "            return None\n",
    "        \n",
    "        if not drive_path:\n",
    "            self._report_error(\"Google Drive path not provided in configuration\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Mount Drive if not already mounted\n",
    "            if not Path('/content/drive').exists():\n",
    "                if self.verbose:\n",
    "                    print(\"Mounting Google Drive...\")\n",
    "                from google.colab import drive\n",
    "                drive.mount('/content/drive')\n",
    "            \n",
    "            file_path = Path(drive_path)\n",
    "            if not file_path.exists():\n",
    "                self._report_error(f\"File not found on Google Drive: {drive_path}\")\n",
    "                return None\n",
    "            \n",
    "            if self.verbose:\n",
    "                size_mb = file_path.stat().st_size / (1024*1024)\n",
    "                print(f\"Loading from Google Drive: {size_mb:.1f} MB\")\n",
    "            \n",
    "            return self._load_large_csv(file_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self._report_error(f\"Google Drive access failed: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _load_from_cloud(self, config: Dict[str, Any]) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Load data from cloud storage with optimized transfer.\"\"\"\n",
    "        cloud_type = config.get('type', 'gcs')\n",
    "        \n",
    "        if cloud_type == 'gcs':\n",
    "            return self._load_from_gcs(config)\n",
    "        else:\n",
    "            self._report_error(f\"Unsupported cloud storage type: {cloud_type}\")\n",
    "            return None\n",
    "    \n",
    "    def _load_from_gcs(self, config: Dict[str, Any]) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Load data from Google Cloud Storage.\"\"\"\n",
    "        bucket_name = config.get('bucket')\n",
    "        blob_name = config.get('blob')\n",
    "        \n",
    "        if not bucket_name or not blob_name:\n",
    "            self._report_error(\"GCS bucket and blob names required in configuration\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            from google.cloud import storage\n",
    "            \n",
    "            # Try different authentication methods\n",
    "            client = None\n",
    "            auth_methods = [\n",
    "                self._try_default_credentials,\n",
    "                self._try_service_account,\n",
    "                self._try_anonymous_access\n",
    "            ]\n",
    "            \n",
    "            for auth_method in auth_methods:\n",
    "                client = auth_method()\n",
    "                if client:\n",
    "                    break\n",
    "            \n",
    "            if not client:\n",
    "                self._report_error(\"Failed to authenticate with Google Cloud Storage\")\n",
    "                return None\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Accessing gs://{bucket_name}/{blob_name}\")\n",
    "            \n",
    "            bucket = client.bucket(bucket_name)\n",
    "            blob = bucket.blob(blob_name)\n",
    "            \n",
    "            if not blob.exists():\n",
    "                self._report_error(f\"File not found: gs://{bucket_name}/{blob_name}\")\n",
    "                return None\n",
    "            \n",
    "            # Get file info\n",
    "            blob.reload()\n",
    "            size_gb = blob.size / (1024**3)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"File size: {size_gb:.2f} GB\")\n",
    "            \n",
    "            # Download with progress tracking\n",
    "            cache_file = self.data_dir / f\"nass_gcs_{bucket_name}_{blob_name.replace('/', '_')}\"\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"Downloading from GCS...\")\n",
    "            \n",
    "            # Use resumable download for large files\n",
    "            if blob.size > 100 * 1024 * 1024:  # 100MB\n",
    "                self._download_large_blob(blob, cache_file)\n",
    "            else:\n",
    "                blob.download_to_filename(cache_file)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"✓ Download complete, loading into memory...\")\n",
    "            \n",
    "            return self._load_large_csv(cache_file)\n",
    "            \n",
    "        except ImportError:\n",
    "            self._report_error(\"google-cloud-storage package not installed\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self._report_error(f\"GCS access failed: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _try_default_credentials(self):\n",
    "        \"\"\"Try default application credentials.\"\"\"\n",
    "        try:\n",
    "            from google.cloud import storage\n",
    "            return storage.Client()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def _try_service_account(self):\n",
    "        \"\"\"Try service account key file.\"\"\"\n",
    "        try:\n",
    "            from google.cloud import storage\n",
    "            key_file = self.data_dir / 'gcs_service_account.json'\n",
    "            if key_file.exists():\n",
    "                return storage.Client.from_service_account_json(str(key_file))\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    def _try_anonymous_access(self):\n",
    "        \"\"\"Try anonymous access for public buckets.\"\"\"\n",
    "        try:\n",
    "            from google.cloud import storage\n",
    "            return storage.Client.create_anonymous_client()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def _download_large_blob(self, blob, cache_file):\n",
    "        \"\"\"Download large blob with progress tracking.\"\"\"\n",
    "        chunk_size = 1024 * 1024  # 1MB chunks\n",
    "        \n",
    "        with open(cache_file, 'wb') as f:\n",
    "            downloaded = 0\n",
    "            \n",
    "            # Download in chunks\n",
    "            for chunk_start in range(0, blob.size, chunk_size):\n",
    "                chunk_end = min(chunk_start + chunk_size - 1, blob.size - 1)\n",
    "                \n",
    "                chunk_data = blob.download_as_bytes(\n",
    "                    start=chunk_start,\n",
    "                    end=chunk_end\n",
    "                )\n",
    "                \n",
    "                f.write(chunk_data)\n",
    "                downloaded += len(chunk_data)\n",
    "                \n",
    "                if self.verbose:\n",
    "                    progress = (downloaded / blob.size) * 100\n",
    "                    print(f\"\\rProgress: {progress:.1f}%\", end='', flush=True)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print()  # New line after progress\n",
    "    \n",
    "    def _load_large_csv(self, file_path: Path) -> pd.DataFrame:\n",
    "        \"\"\"Load large CSV files efficiently with progress tracking.\"\"\"\n",
    "        file_size = file_path.stat().st_size\n",
    "        \n",
    "        if file_size < 50 * 1024 * 1024:  # Less than 50MB, load normally\n",
    "            return pd.read_csv(file_path)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Loading large file in chunks...\")\n",
    "        \n",
    "        # Read in chunks and combine\n",
    "        chunk_size = 50000  # 50k rows per chunk\n",
    "        chunks = []\n",
    "        \n",
    "        try:\n",
    "            total_chunks = sum(1 for _ in pd.read_csv(file_path, chunksize=chunk_size))\n",
    "            \n",
    "            for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):\n",
    "                chunks.append(chunk)\n",
    "                if self.verbose:\n",
    "                    progress = ((i + 1) / total_chunks) * 100\n",
    "                    print(f\"\\rLoading: {progress:.1f}%\", end='', flush=True)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print()  # New line after progress\n",
    "            \n",
    "            result = pd.concat(chunks, ignore_index=True)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"✓ Loaded {len(result):,} rows\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self._report_error(f\"Failed to load large CSV: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _report_error(self, message: str):\n",
    "        \"\"\"Report error in a consistent format.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"✗ Error: {message}\")\n",
    "    \n",
    "    def get_cache_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get information about cached files.\"\"\"\n",
    "        cache_files = list(self.data_dir.glob(\"nass_*\"))\n",
    "        \n",
    "        info = {\n",
    "            'cache_directory': str(self.data_dir),\n",
    "            'cached_files': []\n",
    "        }\n",
    "        \n",
    "        for file_path in cache_files:\n",
    "            stat = file_path.stat()\n",
    "            info['cached_files'].append({\n",
    "                'name': file_path.name,\n",
    "                'size_mb': stat.st_size / (1024*1024),\n",
    "                'modified': time.ctime(stat.st_mtime)\n",
    "            })\n",
    "        \n",
    "        return info\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear all cached files.\"\"\"\n",
    "        cache_files = list(self.data_dir.glob(\"nass_*\"))\n",
    "        \n",
    "        for file_path in cache_files:\n",
    "            try:\n",
    "                file_path.unlink()\n",
    "                if self.verbose:\n",
    "                    print(f\"✓ Deleted: {file_path.name}\")\n",
    "            except Exception as e:\n",
    "                if self.verbose:\n",
    "                    print(f\"✗ Failed to delete {file_path.name}: {str(e)}\")\n",
    "\n",
    "# Initialize the data loader\n",
    "data_loader = NASSDataLoader(verbose=VERBOSE_PRINTS)\n",
    "\n",
    "# Load data based on configuration\n",
    "config_map = {\n",
    "    \"github\": {\"url\": GITHUB_URL},\n",
    "    \"local\": {\"filename\": LOCAL_FILENAME},\n",
    "    \"drive\": {\"path\": DRIVE_PATH},\n",
    "    \"gcs\": {\n",
    "        \"type\": \"gcs\",\n",
    "        \"bucket\": GCS_BUCKET,\n",
    "        \"blob\": GCS_BLOB\n",
    "    }\n",
    "}\n",
    "\n",
    "if DATA_SOURCE in config_map:\n",
    "    print(f\"Loading NASS data from {DATA_SOURCE}...\")\n",
    "    df = data_loader.load_data(DATA_SOURCE, **config_map[DATA_SOURCE])\n",
    "    \n",
    "    if df is not None:\n",
    "        print(f\"✓ Successfully loaded: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "        print(f\"Memory usage: {df.memory_usage(deep=True).sum() / (1024*1024):.1f} MB\")\n",
    "    else:\n",
    "        print(\"✗ Failed to load data\")\n",
    "        print(\"\\nTroubleshooting:\")\n",
    "        if DATA_SOURCE == \"local\":\n",
    "            print(f\"  - Ensure file '{LOCAL_FILENAME}' exists in one of these locations:\")\n",
    "            print(f\"    • {Path.cwd()}\")\n",
    "            print(f\"    • {data_loader.data_dir}\")\n",
    "        elif DATA_SOURCE == \"gcs\":\n",
    "            print(f\"  - Verify bucket '{GCS_BUCKET}' and file '{GCS_BLOB}' exist\")\n",
    "            print(f\"  - Check authentication (service account key, default credentials)\")\n",
    "        elif DATA_SOURCE == \"drive\":\n",
    "            print(f\"  - Ensure file exists at: {DRIVE_PATH}\")\n",
    "            print(f\"  - Google Drive must be mounted in Colab\")\n",
    "        elif DATA_SOURCE == \"github\":\n",
    "            print(f\"  - Check URL is accessible: {GITHUB_URL}\")\n",
    "            print(f\"  - Verify internet connection\")\n",
    "else:\n",
    "    print(f\"✗ Invalid data source: {DATA_SOURCE}\")\n",
    "    print(f\"Valid sources: {list(config_map.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete stored Google credentials\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Find and delete the stored credentials file\n",
    "try:\n",
    "    # Check common credential storage locations\n",
    "    credential_files = [\n",
    "        Path.home() / 'data' / 'gcs_token.json',  # From your data loader\n",
    "        Path.cwd() / 'data' / 'gcs_token.json',\n",
    "        Path('/content/data/gcs_token.json'),  # Colab\n",
    "    ]\n",
    "    \n",
    "    deleted_files = []\n",
    "    for cred_file in credential_files:\n",
    "        if cred_file.exists():\n",
    "            cred_file.unlink()  # Delete the file\n",
    "            deleted_files.append(str(cred_file))\n",
    "            print(f\"✅ Deleted: {cred_file}\")\n",
    "    \n",
    "    if not deleted_files:\n",
    "        print(\"ℹ️  No credential files found - they may already be cleared\")\n",
    "    else:\n",
    "        print(f\"\\n🧹 Cleared {len(deleted_files)} credential file(s)\")\n",
    "    \n",
    "    # Also clear any environment variables\n",
    "    google_env_vars = [k for k in os.environ.keys() if 'GOOGLE' in k or 'GCP' in k]\n",
    "    for var in google_env_vars:\n",
    "        if var != 'COLAB_GPU':  # Don't delete Colab system vars\n",
    "            del os.environ[var]\n",
    "            print(f\"🗑️  Cleared environment variable: {var}\")\n",
    "    \n",
    "    print(\"\\n✅ Google credentials cleared successfully!\")\n",
    "    print(\"💡 Next time you run the GCS data loader, it will prompt for new authentication\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error clearing credentials: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxQ5fG5grPrj"
   },
   "source": [
    "Check if data has been loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqUzCuZOrPrj"
   },
   "outputs": [],
   "source": [
    "# Verify data is available before R processing\n",
    "try:\n",
    "    if 'df' not in globals():\n",
    "        print(\"❌ Data not loaded!\")\n",
    "        print(\"💡 Please run the 'Data Loading' section first (cell 12)\")\n",
    "        print(\"   This will create the 'df' variable needed for R analysis\")\n",
    "        raise NameError(\"df variable not found - run data loading first\")\n",
    "\n",
    "    print(f\"✅ Data verified: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "    print(\"✅ Ready for R analysis\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"❌ {e}\")\n",
    "    print(\"\\n🔄 Quick fix: Run these cells in order:\")\n",
    "    print(\"   1. Configuration (cell 5)\")\n",
    "    print(\"   2. Environment Setup (cell 7)\")\n",
    "    print(\"   3. Data Loading (cell 12)\")\n",
    "    print(\"   4. Then continue with R analysis\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3j3Ms-OArPrj"
   },
   "source": [
    "## 5. Complete Data Preprocessing\n",
    "\n",
    "Streamlined preprocessing: remove variables, clean data types, and create new variables - in Python prior to passing to R for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQzhWzohrPrj"
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing in Python (before R transfer)\n",
    "print(\"Complete preprocessing: removing variables + cleaning data types...\")\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "\n",
    "# ===== 1. REMOVE UNNECESSARY VARIABLES =====\n",
    "print(\"\\n1 Removing unnecessary variables...\")\n",
    "\n",
    "# Smart pattern-based removal in pandas (much faster than R)\n",
    "drop_patterns = [\n",
    "    r'^CPTCCS[2-9]$',      # CPTCCS2-CPTCCS9\n",
    "    r'^CPTCCS[1-3][0-9]$', # CPTCCS10-30\n",
    "    r'^CPT[2-9]$',         # CPT2-CPT9\n",
    "    r'^CPT[1-3][0-9]$',    # CPT10-30\n",
    "    r'^DXCCSR_',           # All DXCCSR columns (500+)\n",
    "]\n",
    "\n",
    "# Find columns to drop using vectorized operations\n",
    "drop_cols = []\n",
    "for pattern in drop_patterns:\n",
    "    matches = df.columns[df.columns.str.match(pattern)].tolist()\n",
    "    drop_cols.extend(matches)\n",
    "\n",
    "# Remove duplicates\n",
    "drop_cols = list(set(drop_cols))\n",
    "\n",
    "print(f\"   📊 Found {len(drop_cols)} columns to drop\")\n",
    "print(f\"   🗑️  Patterns: CPTCCS2-30, CPT2-30, all DXCCSR_*\")\n",
    "\n",
    "# Drop the columns\n",
    "df = df.drop(columns=drop_cols)\n",
    "print(f\"   ✅ Reduced from {df.shape[1] + len(drop_cols)} to {df.shape[1]} columns\")\n",
    "\n",
    "# ===== 2. CLEAN DATA TYPES FOR rpy2 =====\n",
    "print(\"\\n2️⃣ Cleaning data types for rpy2 compatibility...\")\n",
    "\n",
    "# Convert all object columns to strings (prevents mixed-type issues)\n",
    "object_columns = df.select_dtypes(include=['object']).columns\n",
    "if len(object_columns) > 0:\n",
    "    for col in object_columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "    print(f\"   ✅ Converted {len(object_columns)} object columns to strings\")\n",
    "\n",
    "# Handle NaN/inf values consistently - FIXED FOR CATEGORICAL COLUMNS\n",
    "# First handle categorical columns separately\n",
    "categorical_columns = df.select_dtypes(include=['category']).columns\n",
    "if len(categorical_columns) > 0:\n",
    "    for col in categorical_columns:\n",
    "        # Convert categorical to string first, then handle NaN\n",
    "        df[col] = df[col].astype(str)\n",
    "    print(f\"   ✅ Converted {len(categorical_columns)} categorical columns to strings\")\n",
    "\n",
    "# Now handle all non-categorical columns\n",
    "non_categorical_columns = df.select_dtypes(exclude=['category']).columns\n",
    "if len(non_categorical_columns) > 0:\n",
    "    # Replace NaN with empty strings for non-categorical columns\n",
    "    df[non_categorical_columns] = df[non_categorical_columns].fillna('')\n",
    "\n",
    "# Handle inf values in float columns\n",
    "float_cols = df.select_dtypes(include=['float64', 'float32']).columns\n",
    "if len(float_cols) > 0:\n",
    "    for col in float_cols:\n",
    "        df[col] = df[col].replace([float('inf'), float('-inf')], '')\n",
    "    print(f\"   ✅ Cleaned inf values in {len(float_cols)} float columns\")\n",
    "\n",
    "print(f\"   ✅ Cleaned NaN values in all columns\")\n",
    "\n",
    "# ===== 3. CREATE KEY ANALYTICAL VARIABLES IN PANDAS =====\n",
    "print(\"\\n3️⃣ Creating analytical variables...\")\n",
    "\n",
    "# Create WHITE indicator (1=White, 0=Non-White)\n",
    "if 'RACE' in df.columns:\n",
    "    df['WHITE'] = (df['RACE'].astype(str) == '1').astype(int)\n",
    "    print(\"   ✅ Created a race indicator boolean\")\n",
    "\n",
    "# Create age groups\n",
    "if 'AGE' in df.columns:\n",
    "    df['AGE'] = pd.to_numeric(df['AGE'], errors='coerce')  # Ensure numeric\n",
    "    df['AGE_GROUP'] = pd.cut(df['AGE'],\n",
    "                            bins=[0, 18, 30, 45, 65, float('inf')],\n",
    "                            labels=['0-17', '18-29', '30-44', '45-64', '65+'],\n",
    "                            right=False)\n",
    "    df['AGE_GROUP'] = df['AGE_GROUP'].astype(str)  # Convert to string for R\n",
    "    print(\"   ✅ Created AGE_GROUP categories\")\n",
    "\n",
    "# Create income level labels\n",
    "if 'ZIPINC_QRTL' in df.columns:\n",
    "    income_map = {1: 'Q1-Lowest', 2: 'Q2', 3: 'Q3', 4: 'Q4-Highest'}\n",
    "    df['INCOME_LEVEL'] = df['ZIPINC_QRTL'].astype(str).map(lambda x: income_map.get(int(x) if x.isdigit() else 0, 'Unknown'))\n",
    "    print(\"   ✅ Created INCOME_LEVEL labels\")\n",
    "\n",
    "# Ensure key numeric variables are properly typed\n",
    "numeric_vars = ['AGE', 'DISCWT', 'TOTCHG']\n",
    "for var in numeric_vars:\n",
    "    if var in df.columns:\n",
    "        df[var] = pd.to_numeric(df[var], errors='coerce')\n",
    "\n",
    "# save copy of cleaned data for R transfer\n",
    "cleaned_path = ultra_loader.data_dir / \"nass_data_cleaned.csv\"\n",
    "df.to_csv(cleaned_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ PREPROCESSING COMPLETE!\")\n",
    "print(f\"📊 Final shape: {df.shape}\")\n",
    "print(f\"📁 Cleaned data saved to: {cleaned_path}\")\n",
    "print(f\"💾 Ready for R transfer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1smsFldXrPrj"
   },
   "source": [
    "## 6. Final R Transfer & Processing\n",
    "\n",
    "Transfer the clean data to R and apply any final R-specific formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10dad708"
   },
   "outputs": [],
   "source": [
    "%%R -i df -i VERBOSE_PRINTS\n",
    "\n",
    "# Convert to data.table and apply R types\n",
    "NASS <- as.data.table(df)\n",
    "\n",
    "# Factor variables\n",
    "factor_vars <- c(\"ZIPINC_QRTL\", \"PAY1\", \"CPTCCS1\", \"HOSP_LOCATION\",\n",
    "                 \"HOSP_TEACH\", \"HOSP_NASS\", \"RACE\", \"AGE_GROUP\", \"INCOME_LEVEL\")\n",
    "existing_factors <- factor_vars[factor_vars %in% names(NASS)]\n",
    "NASS[, (existing_factors) := lapply(.SD, as.factor), .SDcols = existing_factors]\n",
    "\n",
    "# Boolean variables\n",
    "if(\"FEMALE\" %in% names(NASS)) NASS[, FEMALE := as.logical(as.numeric(FEMALE))]\n",
    "if(\"WHITE\" %in% names(NASS)) NASS[, WHITE := as.logical(as.numeric(WHITE))]\n",
    "\n",
    "# Compact output\n",
    "cat(\"✅ R Complete:\", nrow(NASS), \"rows,\", ncol(NASS), \"cols,\",\n",
    "    round(object.size(NASS)/1024^2, 1), \"MB\\n\")\n",
    "cat(\"Converted\", length(existing_factors), \"factors + 2 booleans\\n\")\n",
    "\n",
    "if(VERBOSE_PRINTS) {\n",
    "  cat(\"\\nColumns:\\n\")\n",
    "  print(colnames(NASS))\n",
    "}\n",
    "\n",
    "cat(\"Data in R!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Status Check/Recovery\n",
    "\n",
    "Please use this cell to reset analysis environment in case of cell crash/hang. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PRACTICAL HEALTH CHECK & RECOVERY ===\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "def quick_health_check():\n",
    "    \"\"\"Quick, accurate health check focused on actual functionality\"\"\"\n",
    "    print(\"🔍 ENVIRONMENT STATUS CHECK\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # 1. Check core data\n",
    "    if 'df' in globals():\n",
    "        df_shape = globals()['df'].shape\n",
    "        print(f\"✅ Data: {df_shape[0]:,} rows × {df_shape[1]} columns\")\n",
    "    else:\n",
    "        print(\"❌ Data: Not loaded\")\n",
    "        issues.append(\"data_missing\")\n",
    "    \n",
    "    # 2. Check R integration properly\n",
    "    r_working = False\n",
    "    try:\n",
    "        # Actually test R functionality\n",
    "        get_ipython().run_cell_magic('R', '', 'cat(\"R test successful\\\\n\")')\n",
    "        r_working = True\n",
    "        print(\"✅ R integration: Working\")\n",
    "    except Exception as e:\n",
    "        print(\"❌ R integration: Failed\")\n",
    "        issues.append(\"r_failed\")\n",
    "    \n",
    "    # 3. Check R data if R works\n",
    "    if r_working and 'df' in globals():\n",
    "        try:\n",
    "            get_ipython().run_cell_magic('R', '', '''\n",
    "            if(exists(\"NASS\")) {\n",
    "                cat(\"✅ R data: Available (\", nrow(NASS), \"rows)\\\\n\")\n",
    "            } else {\n",
    "                cat(\"❌ R data: Missing\\\\n\")\n",
    "            }\n",
    "            ''')\n",
    "        except:\n",
    "            print(\"❌ R data: Transfer failed\")\n",
    "            issues.append(\"r_data_missing\")\n",
    "    \n",
    "    # 4. Check configuration\n",
    "    config_ok = all(var in globals() for var in ['DATA_SOURCE', 'VERBOSE_PRINTS'])\n",
    "    print(f\"{'✅' if config_ok else '❌'} Configuration: {'Set' if config_ok else 'Missing'}\")\n",
    "    if not config_ok:\n",
    "        issues.append(\"config_missing\")\n",
    "    \n",
    "    # 5. Check memory usage\n",
    "    try:\n",
    "        import psutil\n",
    "        memory_pct = psutil.virtual_memory().percent\n",
    "        memory_ok = memory_pct < 90\n",
    "        print(f\"{'✅' if memory_ok else '⚠️ '} Memory: {memory_pct:.1f}% used\")\n",
    "        if not memory_ok:\n",
    "            issues.append(\"high_memory\")\n",
    "    except:\n",
    "        print(\"? Memory: Cannot check\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if not issues:\n",
    "        print(\"✅ All systems operational - Ready for analysis!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"⚠️  {len(issues)} issues detected\")\n",
    "        return issues\n",
    "\n",
    "def quick_recovery():\n",
    "    \"\"\"Simple recovery for common issues\"\"\"\n",
    "    print(\"🔧 ATTEMPTING RECOVERY...\")\n",
    "    \n",
    "    # Restore config if missing\n",
    "    if 'DATA_SOURCE' not in globals():\n",
    "        globals()['DATA_SOURCE'] = \"github\"\n",
    "        globals()['VERBOSE_PRINTS'] = True\n",
    "        print(\"   ✅ Configuration restored\")\n",
    "    \n",
    "    # Reload data if missing\n",
    "    if 'df' not in globals():\n",
    "        try:\n",
    "            # Try to reload from cache\n",
    "            from pathlib import Path\n",
    "            cache_file = Path.home() / 'data' / 'nass_data_github.csv'\n",
    "            if cache_file.exists():\n",
    "                import pandas as pd\n",
    "                globals()['df'] = pd.read_csv(cache_file)\n",
    "                print(f\"   ✅ Data reloaded: {globals()['df'].shape[0]:,} rows\")\n",
    "            else:\n",
    "                print(\"   ❌ No cached data found - run data loading section\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Data reload failed: {e}\")\n",
    "    \n",
    "    # Restore R data if possible\n",
    "    if globals().get('R_AVAILABLE', True) and 'df' in globals():\n",
    "        try:\n",
    "            get_ipython().run_cell_magic('R', '-i df', '''\n",
    "            if(!exists(\"NASS\") && exists(\"df\")) {\n",
    "                library(data.table)\n",
    "                NASS <- as.data.table(df)\n",
    "                cat(\"   ✅ R data restored\\\\n\")\n",
    "            }\n",
    "            ''')\n",
    "        except:\n",
    "            print(\"   ❌ R data restore failed\")\n",
    "    \n",
    "    # Clear memory\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(\"   ✅ Memory cleaned\")\n",
    "\n",
    "def mini_reset():\n",
    "    \"\"\"Clear variables but keep essential functions\"\"\"\n",
    "    keep = ['quick_health_check', 'quick_recovery', 'mini_reset', \n",
    "            'DATA_SOURCE', 'VERBOSE_PRINTS', 'GITHUB_URL']\n",
    "    \n",
    "    cleared = 0\n",
    "    for var in list(globals().keys()):\n",
    "        if not var.startswith('_') and var not in keep:\n",
    "            try:\n",
    "                del globals()[var]\n",
    "                cleared += 1\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(f\"🧹 Cleared {cleared} variables\")\n",
    "    print(\"💡 Re-run setup cells to restore environment\")\n",
    "\n",
    "# Run check\n",
    "issues = quick_health_check()\n",
    "\n",
    "if issues and issues != True:\n",
    "    response = input(\"\\nAttempt recovery? (y/n): \").lower()\n",
    "    if response == 'y':\n",
    "        quick_recovery()\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(\"Re-checking after recovery...\")\n",
    "        quick_health_check()\n",
    "\n",
    "print(f\"\\n🎮 Quick actions available:\")\n",
    "print(f\"   quick_recovery() - Fix common issues\")\n",
    "print(f\"   mini_reset() - Clear environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43230586"
   },
   "source": [
    "---\n",
    "\n",
    "## 1. Dataset Overview and Summary\n",
    "\n",
    "Generate comprehensive summary statistics and overview of the NASS dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Simple, reliable summary table\n",
    "cat(\"=== NASS 2020 DATASET SUMMARY ===\\n\")\n",
    "cat(\"Total observations:\", nrow(NASS), \"\\n\")\n",
    "cat(\"Total variables:\", ncol(NASS), \"\\n\\n\")\n",
    "\n",
    "# Key variables for summary\n",
    "summary_vars <- c(\"AGE\", \"FEMALE\", \"RACE\", \"ZIPINC_QRTL\", \"PAY1\", \n",
    "                 \"HOSP_LOCATION\", \"HOSP_TEACH\", \"HOSP_REGION\")\n",
    "\n",
    "available_vars <- summary_vars[summary_vars %in% names(NASS)]\n",
    "\n",
    "for(var in available_vars) {\n",
    "  cat(\"-----------------------------------------------\\n\")\n",
    "  cat(\"Variable:\", var, \"\\n\")\n",
    "  \n",
    "  if(is.numeric(NASS[[var]])) {\n",
    "    # Numeric variable summary\n",
    "    var_summary <- summary(NASS[[var]])\n",
    "    cat(\"  Type: Continuous\\n\")\n",
    "    cat(\"  Mean (SD):\", round(mean(NASS[[var]], na.rm = TRUE), 1), \n",
    "        \"(\", round(sd(NASS[[var]], na.rm = TRUE), 1), \")\\n\")\n",
    "    cat(\"  Median [IQR]:\", round(median(NASS[[var]], na.rm = TRUE), 1),\n",
    "        \"[\", round(quantile(NASS[[var]], 0.25, na.rm = TRUE), 1), \"-\",\n",
    "        round(quantile(NASS[[var]], 0.75, na.rm = TRUE), 1), \"]\\n\")\n",
    "    cat(\"  Range:\", round(min(NASS[[var]], na.rm = TRUE), 1), \"to\", \n",
    "        round(max(NASS[[var]], na.rm = TRUE), 1), \"\\n\")\n",
    "    cat(\"  Missing:\", sum(is.na(NASS[[var]])), \"observations\\n\")\n",
    "    \n",
    "  } else {\n",
    "    # Categorical variable summary\n",
    "    freq_table <- table(NASS[[var]], useNA = \"ifany\")\n",
    "    total_n <- sum(freq_table)\n",
    "    \n",
    "    cat(\"  Type: Categorical\\n\")\n",
    "    cat(\"  Levels:\", length(freq_table), \"\\n\")\n",
    "    \n",
    "    # Show top categories (up to 10)\n",
    "    sorted_freq <- sort(freq_table, decreasing = TRUE)\n",
    "    max_show <- min(10, length(sorted_freq))\n",
    "    \n",
    "    for(i in 1:max_show) {\n",
    "      cat(\"    \", names(sorted_freq)[i], \":\", sorted_freq[i], \n",
    "          \"(\", round(100 * sorted_freq[i] / total_n, 1), \"%)\\n\")\n",
    "    }\n",
    "    \n",
    "    if(length(sorted_freq) > max_show) {\n",
    "      cat(\"    ... and\", length(sorted_freq) - max_show, \"more categories\\n\")\n",
    "    }\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "cat(\"=== SUMMARY COMPLETE ===\\n\")\n",
    "\n",
    "cat(\"\\nSummary complete - ready for detailed analysis\\n\")\n",
    "flush.console()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Install and load required packages for comprehensive analysis\n",
    "required_packages <- c(\"ggplot2\", \"data.table\", \"scales\", \"RColorBrewer\")\n",
    "\n",
    "for(pkg in required_packages) {\n",
    "  if(!require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "    cat(\"Installing\", pkg, \"...\\n\")\n",
    "    install.packages(pkg, quiet = TRUE)\n",
    "    library(pkg, character.only = TRUE, quietly = TRUE)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Set consistent theme for all visualizations\n",
    "theme_nass <- theme_minimal() +\n",
    "  theme(\n",
    "    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n",
    "    plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray40\"),\n",
    "    axis.title = element_text(size = 11, face = \"bold\"),\n",
    "    axis.text = element_text(size = 10),\n",
    "    legend.title = element_text(size = 11, face = \"bold\"),\n",
    "    legend.text = element_text(size = 10),\n",
    "    strip.text = element_text(size = 10, face = \"bold\"),\n",
    "    panel.grid.minor = element_blank()\n",
    "  )\n",
    "\n",
    "# Define consistent color palettes\n",
    "race_colors <- RColorBrewer::brewer.pal(6, \"Set2\")\n",
    "pay_colors <- RColorBrewer::brewer.pal(6, \"Dark2\")\n",
    "region_colors <- RColorBrewer::brewer.pal(4, \"Set1\")\n",
    "\n",
    "cat(\"Setup complete - consistent theme and colors defined\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hospital Characteristics Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hospital Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Hospital distribution by region and characteristics\n",
    "if(all(c(\"HOSP_REGION\", \"HOSP_BEDSIZE_CAT\", \"HOSP_LOCATION\", \"HOSP_TEACH\") %in% names(NASS))) {\n",
    "  \n",
    "  # Get unique hospital characteristics\n",
    "  hospital_chars <- unique(NASS[, .(HOSP_NASS, HOSP_LOCATION, HOSP_TEACH, \n",
    "                                   HOSP_REGION, HOSP_BEDSIZE_CAT)])\n",
    "  \n",
    "  # Define labels\n",
    "  bed_labels <- c(\"1\" = \"Small (0-99)\", \"2\" = \"Medium (100-299)\", \"3\" = \"Large (300+)\")\n",
    "  region_labels <- c(\"1\" = \"Northeast\", \"2\" = \"Midwest\", \"3\" = \"South\", \"4\" = \"West\")\n",
    "  teach_labels <- c(\"0\" = \"Non-Teaching\", \"1\" = \"Teaching\")\n",
    "  location_labels <- c(\"0\" = \"Rural\", \"1\" = \"Urban\")\n",
    "  \n",
    "  p1 <- ggplot(hospital_chars, aes(x = factor(HOSP_REGION), fill = factor(HOSP_BEDSIZE_CAT))) + \n",
    "    geom_bar(alpha = 0.8, color = \"white\", size = 0.3) + \n",
    "    theme_nass + \n",
    "    labs(\n",
    "      x = \"US Region\", \n",
    "      y = \"Number of Hospitals\",\n",
    "      title = \"Hospital Distribution in NASS 2020 Dataset\", \n",
    "      subtitle = \"By Region, Location, Teaching Status, and Bed Size\",\n",
    "      fill = \"Bed Size Category\"\n",
    "    ) + \n",
    "    scale_fill_manual(values = region_colors, labels = bed_labels) + \n",
    "    scale_x_discrete(labels = region_labels) +\n",
    "    facet_grid(HOSP_LOCATION ~ HOSP_TEACH, \n",
    "               labeller = labeller(HOSP_TEACH = teach_labels, \n",
    "                                 HOSP_LOCATION = location_labels)) +\n",
    "    theme(legend.position = \"bottom\")\n",
    "  \n",
    "  print(p1)\n",
    "  \n",
    "  cat(\"Hospital distribution plot generated for\", nrow(hospital_chars), \"hospitals\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hospital Volume Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#  Hospital encounter volume distribution\n",
    "if(\"TOTAL_AS_ENCOUNTERS\" %in% names(NASS)) {\n",
    "  \n",
    "  hospital_volumes <- unique(NASS[, .(HOSP_NASS, HOSP_LOCATION, HOSP_TEACH, \n",
    "                                     HOSP_REGION, TOTAL_AS_ENCOUNTERS)])\n",
    "  \n",
    "  p2 <- ggplot(hospital_volumes, aes(x = factor(HOSP_REGION), y = TOTAL_AS_ENCOUNTERS)) + \n",
    "    geom_boxplot(aes(fill = factor(HOSP_REGION)), alpha = 0.7, outlier.alpha = 0.6) + \n",
    "    theme_nass + \n",
    "    labs(\n",
    "      x = \"US Region\", \n",
    "      y = \"Total Ambulatory Surgery Encounters\",\n",
    "      title = \"Hospital Ambulatory Surgery Volume Distribution\", \n",
    "      subtitle = \"By Region, Location, and Teaching Status\"\n",
    "    ) + \n",
    "    scale_fill_manual(values = region_colors, labels = region_labels, guide = \"none\") + \n",
    "    scale_x_discrete(labels = region_labels) +\n",
    "    scale_y_continuous(labels = comma_format()) +\n",
    "    facet_grid(HOSP_LOCATION ~ HOSP_TEACH, \n",
    "               labeller = labeller(HOSP_TEACH = teach_labels, \n",
    "                                 HOSP_LOCATION = location_labels))\n",
    "  \n",
    "  print(p2)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Procedures Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Procedures Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Calculate and display top 10 procedures\n",
    "if(\"CPTCCS1\" %in% names(NASS)) {\n",
    "  \n",
    "  # Calculate top procedures\n",
    "  top_procedures <- NASS[, .N, by = CPTCCS1][order(-N)][1:10]\n",
    "  TopCPT <- top_procedures$CPTCCS1\n",
    "  \n",
    "  cat(\"Top 10 procedures (CPTCCS1):\\n\")\n",
    "  print(top_procedures)\n",
    "  \n",
    "  # Calculate coverage\n",
    "  coverage <- sum(top_procedures$N) / nrow(NASS)\n",
    "  cat(\"\\nTop 10 procedures represent\", round(coverage * 100, 1), \"% of all procedures\\n\")\n",
    "  \n",
    "  # Create subset for detailed analysis\n",
    "  NASS_top_procedures <- NASS[CPTCCS1 %in% TopCPT]\n",
    "  cat(\"Created subset with\", nrow(NASS_top_procedures), \"records for top procedures\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedures by Income Quartile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Top procedures by income quartile\n",
    "if(exists(\"NASS_top_procedures\") && \"ZIPINC_QRTL\" %in% names(NASS)) {\n",
    "  \n",
    "  plot_data <- NASS_top_procedures[ZIPINC_QRTL %in% 1:4]\n",
    "  \n",
    "  if(nrow(plot_data) > 0) {\n",
    "    # Order procedures by frequency\n",
    "    proc_order <- plot_data[, .N, by = CPTCCS1][order(-N)]$CPTCCS1\n",
    "    plot_data[, CPTCCS1 := factor(CPTCCS1, levels = rev(proc_order))]\n",
    "    \n",
    "    p3 <- ggplot(plot_data, aes(x = CPTCCS1, fill = factor(ZIPINC_QRTL))) + \n",
    "      geom_bar(alpha = 0.8, color = \"white\", size = 0.2) + \n",
    "      coord_flip() +\n",
    "      theme_nass + \n",
    "      labs(\n",
    "        x = \"CCS Procedure Codes\", \n",
    "        y = \"Number of Procedures\",\n",
    "        title = \"Most Common Ambulatory Surgery Procedures\", \n",
    "        subtitle = \"Distribution by Patient Income Quartile (by ZIP code)\",\n",
    "        fill = \"Income Quartile\"\n",
    "      ) +\n",
    "      scale_fill_manual(values = pay_colors[1:4], \n",
    "                       labels = c(\"Q1 (Lowest)\", \"Q2\", \"Q3\", \"Q4 (Highest)\")) +\n",
    "      theme(legend.position = \"bottom\")\n",
    "    \n",
    "    print(p3)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedures by Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Top procedures by race\n",
    "if(exists(\"NASS_top_procedures\") && \"RACE\" %in% names(NASS)) {\n",
    "  \n",
    "  plot_data <- NASS_top_procedures[RACE %in% 1:6]\n",
    "  \n",
    "  if(nrow(plot_data) > 0) {\n",
    "    # Order procedures by frequency\n",
    "    proc_order <- plot_data[, .N, by = CPTCCS1][order(-N)]$CPTCCS1\n",
    "    plot_data[, CPTCCS1 := factor(CPTCCS1, levels = rev(proc_order))]\n",
    "    \n",
    "    race_labels <- c(\"1\" = \"White\", \"2\" = \"Black\", \"3\" = \"Hispanic\", \n",
    "                    \"4\" = \"Asian/Pacific\", \"5\" = \"Native American\", \"6\" = \"Other\")\n",
    "    \n",
    "    p4 <- ggplot(plot_data, aes(x = CPTCCS1, fill = factor(RACE))) + \n",
    "      geom_bar(alpha = 0.8, color = \"white\", size = 0.2) + \n",
    "      coord_flip() +\n",
    "      theme_nass + \n",
    "      labs(\n",
    "        x = \"CCS Procedure Codes\", \n",
    "        y = \"Number of Procedures\",\n",
    "        title = \"Most Common Ambulatory Surgery Procedures\", \n",
    "        subtitle = \"Distribution by Patient Race/Ethnicity\",\n",
    "        fill = \"Race/Ethnicity\"\n",
    "      ) + \n",
    "      scale_fill_manual(values = race_colors, labels = race_labels) +\n",
    "      theme(legend.position = \"bottom\")\n",
    "    \n",
    "    print(p4)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patient Characteristics Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age Distribution by Race and Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Age distribution by race and region - COUNT CURVES instead of density\n",
    "if(all(c(\"AGE\", \"RACE\", \"HOSP_REGION\", \"PL_NCHS\") %in% names(NASS))) {\n",
    "  \n",
    "  age_data <- NASS[RACE %in% 1:6 & PL_NCHS %in% 1:6 & AGE >= 0 & AGE <= 100]\n",
    "  \n",
    "  if(nrow(age_data) > 0) {\n",
    "    \n",
    "    race_labels <- c(\"1\" = \"White\", \"2\" = \"Black\", \"3\" = \"Hispanic\", \n",
    "                    \"4\" = \"Asian/Pacific\", \"5\" = \"Native American\", \"6\" = \"Other\")\n",
    "    region_labels <- c(\"1\" = \"Northeast\", \"2\" = \"Midwest\", \"3\" = \"South\", \"4\" = \"West\")\n",
    "    pl_nchs_labels <- c(\"1\" = \"Large Central\", \"2\" = \"Large Fringe\", \"3\" = \"Medium\", \n",
    "                       \"4\" = \"Small\", \"5\" = \"Micro\", \"6\" = \"Non-core\")\n",
    "    \n",
    "    p5 <- ggplot(age_data, aes(x = AGE)) + \n",
    "      geom_density(aes(fill = factor(RACE), y = after_stat(count)), alpha = 0.7, color = \"white\", \n",
    "                   linewidth = 0.2) + \n",
    "      theme_nass + \n",
    "      labs(\n",
    "        x = \"Age (years)\", \n",
    "        y = \"Count\",\n",
    "        title = \"Patient Age Distribution in NASS 2020\", \n",
    "        subtitle = \"By Urban-Rural Classification and US Region, Segmented by Race\",\n",
    "        fill = \"Race/Ethnicity\"\n",
    "      ) + \n",
    "      scale_fill_manual(values = race_colors, labels = race_labels) + \n",
    "      xlim(0, 100) +\n",
    "      facet_grid(HOSP_REGION ~ PL_NCHS, \n",
    "                 labeller = labeller(HOSP_REGION = region_labels, PL_NCHS = pl_nchs_labels)) + \n",
    "      theme(legend.position = \"bottom\", \n",
    "            strip.text = element_text(size = 8))\n",
    "    \n",
    "    print(p5)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age Distribution by Payer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Age distribution by payer and region \n",
    "if(all(c(\"AGE\", \"PAY1\", \"HOSP_REGION\", \"PL_NCHS\") %in% names(NASS))) {\n",
    "  \n",
    "  age_payer_data <- NASS[PAY1 %in% 1:6 & PL_NCHS %in% 1:6 & AGE >= 0 & AGE <= 100]\n",
    "  \n",
    "  if(nrow(age_payer_data) > 0) {\n",
    "    \n",
    "    pay_labels <- c(\"1\" = \"Medicare\", \"2\" = \"Medicaid\", \"3\" = \"Private\", \n",
    "                   \"4\" = \"Self-pay\", \"5\" = \"No Charge\", \"6\" = \"Other\")\n",
    "    \n",
    "    p6 <- ggplot(age_payer_data, aes(x = AGE)) + \n",
    "      geom_density(aes(fill = factor(PAY1), y = after_stat(count)), alpha = 0.7, color = \"white\", \n",
    "                   linewidth = 0.2) + \n",
    "      theme_nass + \n",
    "      labs(\n",
    "        x = \"Age (years)\", \n",
    "        y = \"Count\",\n",
    "        title = \"Patient Age Distribution in NASS 2020\", \n",
    "        subtitle = \"By Urban-Rural Classification and US Region, Segmented by Payer\",\n",
    "        fill = \"Primary Payer\"\n",
    "      ) + \n",
    "      scale_fill_manual(values = pay_colors, labels = pay_labels) + \n",
    "      xlim(0, 100) +\n",
    "      facet_grid(HOSP_REGION ~ PL_NCHS, \n",
    "                 labeller = labeller(HOSP_REGION = region_labels, PL_NCHS = pl_nchs_labels)) + \n",
    "      theme(legend.position = \"bottom\",\n",
    "            strip.text = element_text(size = 8))\n",
    "    \n",
    "    print(p6)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Payer Distribution by Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Payer distribution by race and age group\n",
    "if(all(c(\"RACE\", \"PAY1\", \"AGE\") %in% names(NASS))) {\n",
    "  \n",
    "  # Create better age groups: 0-17, 18-64, 65+\n",
    "  plot_data <- NASS[RACE %in% 1:6 & PAY1 %in% 1:6 & !is.na(AGE)]\n",
    "  \n",
    "  if(nrow(plot_data) > 0) {\n",
    "    \n",
    "    # Create simplified age groups\n",
    "    plot_data[, AGE_GROUP_SIMPLE := cut(AGE, \n",
    "                                       breaks = c(-Inf, 17, 64, Inf),\n",
    "                                       labels = c(\"0-17\", \"18-64\", \"65+\"),\n",
    "                                       right = TRUE)]\n",
    "    \n",
    "    # Remove any missing age groups\n",
    "    plot_data <- plot_data[!is.na(AGE_GROUP_SIMPLE)]\n",
    "    \n",
    "    # Define cleaner labels\n",
    "    race_labels <- c(\"1\" = \"White\", \"2\" = \"Black\", \"3\" = \"Hispanic\", \n",
    "                    \"4\" = \"Asian/Pacific\", \"5\" = \"Native American\", \"6\" = \"Other\")\n",
    "    pay_labels <- c(\"1\" = \"Medicare\", \"2\" = \"Medicaid\", \"3\" = \"Private\", \n",
    "                   \"4\" = \"Self-pay\", \"5\" = \"No Charge\", \"6\" = \"Other\")\n",
    "    \n",
    "    p7 <- ggplot(plot_data, aes(x = factor(RACE), fill = factor(PAY1))) + \n",
    "      geom_bar(position = \"fill\", alpha = 0.8, color = \"white\", linewidth = 0.2) + \n",
    "      theme_nass + \n",
    "      labs(\n",
    "        x = \"Race/Ethnicity\", \n",
    "        y = \"Proportion\",\n",
    "        title = \"Primary Payer Distribution by Demographics\", \n",
    "        subtitle = \"Insurance Coverage Patterns by Age Group and Race/Ethnicity\",\n",
    "        fill = \"Primary Payer\"\n",
    "      ) + \n",
    "      scale_fill_manual(values = pay_colors, labels = pay_labels) + \n",
    "      scale_x_discrete(labels = race_labels) +\n",
    "      scale_y_continuous(labels = percent_format()) +\n",
    "      facet_wrap(~ AGE_GROUP_SIMPLE, ncol = 3) +\n",
    "      theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 9),\n",
    "            legend.position = \"bottom\",\n",
    "            strip.text = element_text(size = 11, face = \"bold\"),\n",
    "            panel.spacing = unit(1, \"lines\"))\n",
    "    \n",
    "    print(p7)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Payer Distribution by Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Payer distribution by income quartile and age group\n",
    "if(all(c(\"ZIPINC_QRTL\", \"PAY1\", \"AGE\") %in% names(NASS))) {\n",
    "  \n",
    "  # Create data with income quartiles and simplified age groups\n",
    "  plot_data <- NASS[ZIPINC_QRTL %in% 1:4 & PAY1 %in% 1:6 & !is.na(AGE)]\n",
    "  \n",
    "  if(nrow(plot_data) > 0) {\n",
    "    \n",
    "    # Create simplified age groups: 0-17, 18-64, 65+\n",
    "    plot_data[, AGE_GROUP_SIMPLE := cut(AGE, \n",
    "                                       breaks = c(-Inf, 17, 64, Inf),\n",
    "                                       labels = c(\"0-17\", \"18-64\", \"65+\"),\n",
    "                                       right = TRUE)]\n",
    "    \n",
    "    # Remove any missing age groups\n",
    "    plot_data <- plot_data[!is.na(AGE_GROUP_SIMPLE)]\n",
    "    \n",
    "    # Define cleaner labels\n",
    "    income_labels <- c(\"1\" = \"Q1 (Lowest)\", \"2\" = \"Q2\", \"3\" = \"Q3\", \"4\" = \"Q4 (Highest)\")\n",
    "    pay_labels <- c(\"1\" = \"Medicare\", \"2\" = \"Medicaid\", \"3\" = \"Private\", \n",
    "                   \"4\" = \"Self-pay\", \"5\" = \"No Charge\", \"6\" = \"Other\")\n",
    "    \n",
    "    p8 <- ggplot(plot_data, aes(x = factor(ZIPINC_QRTL), fill = factor(PAY1))) + \n",
    "      geom_bar(position = \"fill\", alpha = 0.8, color = \"white\", linewidth = 0.2) + \n",
    "      theme_nass + \n",
    "      labs(\n",
    "        x = \"Income Quartile (by ZIP Code)\", \n",
    "        y = \"Proportion\",\n",
    "        title = \"Primary Payer Distribution by Socioeconomic Status\", \n",
    "        subtitle = \"Insurance Coverage Patterns by Age Group and Income Quartile\",\n",
    "        fill = \"Primary Payer\"\n",
    "      ) + \n",
    "      scale_fill_manual(values = pay_colors, labels = pay_labels) + \n",
    "      scale_x_discrete(labels = income_labels) +\n",
    "      scale_y_continuous(labels = percent_format()) +\n",
    "      facet_wrap(~ AGE_GROUP_SIMPLE, ncol = 3) +\n",
    "      theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 9),\n",
    "            legend.position = \"bottom\",\n",
    "            strip.text = element_text(size = 11, face = \"bold\"),\n",
    "            panel.spacing = unit(1, \"lines\"))\n",
    "    \n",
    "    print(p8)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehensive Dataset Summary\n",
    "\n",
    "Final summary statistics providing an overview of all key variables in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# ===============================================\n",
    "# COMPREHENSIVE NASS 2020 DATASET SUMMARY\n",
    "# ===============================================\n",
    "\n",
    "cat(\"================================================================\\n\")\n",
    "cat(\"                    NASS 2020 DATASET SUMMARY                  \\n\")\n",
    "cat(\"================================================================\\n\\n\")\n",
    "\n",
    "# Dataset Overview\n",
    "cat(\"DATASET OVERVIEW\\n\")\n",
    "cat(\"================\\n\")\n",
    "cat(\"Total observations:\", format(nrow(NASS), big.mark = \",\"), \"\\n\")\n",
    "cat(\"Total variables:\", ncol(NASS), \"\\n\")\n",
    "cat(\"Memory usage:\", round(object.size(NASS)/1024^2, 1), \"MB\\n\")\n",
    "cat(\"Missing data patterns:\", sum(is.na(NASS)), \"total missing values\\n\\n\")\n",
    "\n",
    "# Key variables for comprehensive summary\n",
    "summary_vars <- c(\"AGE\", \"FEMALE\", \"RACE\", \"WHITE\", \"ZIPINC_QRTL\", \"PAY1\", \n",
    "                 \"HOSP_LOCATION\", \"HOSP_TEACH\", \"HOSP_REGION\", \"HOSP_BEDSIZE_CAT\",\n",
    "                 \"PL_NCHS\", \"CPTCCS1\", \"TOTCHG\", \"DISCWT\")\n",
    "\n",
    "available_vars <- summary_vars[summary_vars %in% names(NASS)]\n",
    "missing_vars <- summary_vars[!summary_vars %in% names(NASS)]\n",
    "\n",
    "if(length(missing_vars) > 0) {\n",
    "  cat(\"WARNING: Variables not available:\", paste(missing_vars, collapse = \", \"), \"\\n\\n\")\n",
    "}\n",
    "\n",
    "# Variable-by-variable analysis\n",
    "for(var in available_vars) {\n",
    "  cat(\"----------------------------------------------------------------\\n\")\n",
    "  cat(\"Variable:\", toupper(var), \"\\n\")\n",
    "  cat(\"----------------------------------------------------------------\\n\")\n",
    "  \n",
    "  if(is.numeric(NASS[[var]])) {\n",
    "    # Enhanced numeric variable summary\n",
    "    valid_values <- NASS[[var]][!is.na(NASS[[var]])]\n",
    "    missing_count <- sum(is.na(NASS[[var]]))\n",
    "    \n",
    "    cat(\"Type: Continuous/Numeric\\n\")\n",
    "    cat(\"Valid observations:\", format(length(valid_values), big.mark = \",\"), \n",
    "        \"(\", round(100 * length(valid_values) / nrow(NASS), 1), \"%)\\n\")\n",
    "    cat(\"Missing values:\", format(missing_count, big.mark = \",\"), \n",
    "        \"(\", round(100 * missing_count / nrow(NASS), 1), \"%)\\n\")\n",
    "    \n",
    "    if(length(valid_values) > 0) {\n",
    "      # Central tendency\n",
    "      cat(\"\\nCENTRAL TENDENCY:\\n\")\n",
    "      cat(\"   Mean:\", format(round(mean(valid_values), 2), big.mark = \",\"), \"\\n\")\n",
    "      cat(\"   Median:\", format(round(median(valid_values), 2), big.mark = \",\"), \"\\n\")\n",
    "      cat(\"   Mode region:\", format(round(median(valid_values), 2), big.mark = \",\"), \n",
    "          \" +/-\", format(round(mad(valid_values), 2), big.mark = \",\"), \"\\n\")\n",
    "      \n",
    "      # Variability\n",
    "      cat(\"\\nVARIABILITY:\\n\")\n",
    "      cat(\"   Standard Dev:\", format(round(sd(valid_values), 2), big.mark = \",\"), \"\\n\")\n",
    "      cat(\"   IQR:\", format(round(quantile(valid_values, 0.25), 2), big.mark = \",\"), \"to\", \n",
    "          format(round(quantile(valid_values, 0.75), 2), big.mark = \",\"), \"\\n\")\n",
    "      cat(\"   Range:\", format(round(min(valid_values), 2), big.mark = \",\"), \"to\", \n",
    "          format(round(max(valid_values), 2), big.mark = \",\"), \"\\n\")\n",
    "      \n",
    "      # Distribution shape\n",
    "      cat(\"\\nDISTRIBUTION:\\n\")\n",
    "      q1 <- quantile(valid_values, 0.25)\n",
    "      q3 <- quantile(valid_values, 0.75)\n",
    "      skewness_approx <- (mean(valid_values) - median(valid_values)) / sd(valid_values)\n",
    "      \n",
    "      cat(\"   Skewness (approx):\", round(skewness_approx, 3), \n",
    "          ifelse(abs(skewness_approx) < 0.5, \"(approximately symmetric)\", \n",
    "                ifelse(skewness_approx > 0, \"(right-skewed)\", \"(left-skewed)\")), \"\\n\")\n",
    "      \n",
    "      # Outliers (using IQR method)\n",
    "      iqr <- q3 - q1\n",
    "      lower_fence <- q1 - 1.5 * iqr\n",
    "      upper_fence <- q3 + 1.5 * iqr\n",
    "      outliers <- sum(valid_values < lower_fence | valid_values > upper_fence)\n",
    "      cat(\"   Potential outliers:\", outliers, \n",
    "          \"(\", round(100 * outliers / length(valid_values), 1), \"%)\\n\")\n",
    "      \n",
    "      # Percentile breakdown for key variables\n",
    "      if(var %in% c(\"AGE\", \"TOTCHG\", \"DISCWT\")) {\n",
    "        cat(\"\\nPERCENTILE BREAKDOWN:\\n\")\n",
    "        percentiles <- c(0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99)\n",
    "        for(p in percentiles) {\n",
    "          cat(\"   P\", round(p*100), \":\", format(round(quantile(valid_values, p), 1), big.mark = \",\"), \"\\n\")\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \n",
    "  } else {\n",
    "    # Enhanced categorical variable summary\n",
    "    freq_table <- table(NASS[[var]], useNA = \"ifany\")\n",
    "    total_n <- sum(freq_table)\n",
    "    missing_count <- sum(is.na(NASS[[var]]))\n",
    "    \n",
    "    cat(\"Type: Categorical/Factor\\n\")\n",
    "    cat(\"Total categories:\", length(freq_table), \"\\n\")\n",
    "    cat(\"Valid observations:\", format(total_n - missing_count, big.mark = \",\"), \n",
    "        \"(\", round(100 * (total_n - missing_count) / nrow(NASS), 1), \"%)\\n\")\n",
    "    \n",
    "    if(missing_count > 0) {\n",
    "      cat(\"Missing values:\", format(missing_count, big.mark = \",\"), \n",
    "          \"(\", round(100 * missing_count / nrow(NASS), 1), \"%)\\n\")\n",
    "    }\n",
    "    \n",
    "    # Sort by frequency\n",
    "    sorted_freq <- sort(freq_table, decreasing = TRUE)\n",
    "    \n",
    "    # Show distribution\n",
    "    cat(\"\\nFREQUENCY DISTRIBUTION:\\n\")\n",
    "    max_show <- min(15, length(sorted_freq))  # Show more categories\n",
    "    \n",
    "    for(i in 1:max_show) {\n",
    "      category_name <- names(sorted_freq)[i]\n",
    "      count <- sorted_freq[i]\n",
    "      percentage <- round(100 * count / total_n, 1)\n",
    "      \n",
    "      # Create a simple bar visualization\n",
    "      bar_length <- min(20, round(20 * count / max(sorted_freq)))\n",
    "      bar <- paste(rep(\"*\", bar_length), collapse = \"\")\n",
    "      \n",
    "      cat(\"   \", sprintf(\"%-20s\", category_name), \":\", \n",
    "          sprintf(\"%8s\", format(count, big.mark = \",\")), \n",
    "          sprintf(\"(%5.1f%%)\", percentage), \" \", bar, \"\\n\")\n",
    "    }\n",
    "    \n",
    "    if(length(sorted_freq) > max_show) {\n",
    "      remaining_count <- sum(sorted_freq[(max_show+1):length(sorted_freq)])\n",
    "      remaining_pct <- round(100 * remaining_count / total_n, 1)\n",
    "      cat(\"   ... \", length(sorted_freq) - max_show, \" more categories:\", \n",
    "          format(remaining_count, big.mark = \",\"), \"(\", remaining_pct, \"%)\\n\")\n",
    "    }\n",
    "    \n",
    "    # Diversity metrics\n",
    "    cat(\"\\nDIVERSITY METRICS:\\n\")\n",
    "    # Simpson's diversity index (1 - sum of squared proportions)\n",
    "    proportions <- as.numeric(freq_table) / sum(freq_table)\n",
    "    simpson_diversity <- 1 - sum(proportions^2)\n",
    "    cat(\"   Simpson's Diversity:\", round(simpson_diversity, 3), \n",
    "        \"(0=no diversity, 1=max diversity)\\n\")\n",
    "    \n",
    "    # Effective number of categories (inverse Simpson)\n",
    "    effective_categories <- 1 / sum(proportions^2)\n",
    "    cat(\"   Effective categories:\", round(effective_categories, 1), \n",
    "        \"out of\", length(freq_table), \"total\\n\")\n",
    "    \n",
    "    # Concentration ratio (top 3 categories)\n",
    "    top3_concentration <- sum(sorted_freq[1:min(3, length(sorted_freq))]) / total_n\n",
    "    cat(\"   Top-3 concentration:\", round(100 * top3_concentration, 1), \"%\\n\")\n",
    "    \n",
    "    # Special insights for key variables\n",
    "    if(var == \"RACE\") {\n",
    "      cat(\"\\nRACE/ETHNICITY INSIGHTS:\\n\")\n",
    "      cat(\"   Racial diversity reflects ambulatory surgery access patterns\\n\")\n",
    "      if(\"1\" %in% names(freq_table)) {\n",
    "        white_vs_nonwhite <- round(freq_table[\"1\"] / sum(freq_table[names(freq_table) != \"1\"]), 2)\n",
    "        cat(\"   White vs Non-White ratio:\", white_vs_nonwhite, \":1\\n\")\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    if(var == \"PAY1\") {\n",
    "      cat(\"\\nPAYER MIX INSIGHTS:\\n\")\n",
    "      public_payers <- sum(freq_table[c(\"1\", \"2\")], na.rm = TRUE)  # Medicare + Medicaid\n",
    "      cat(\"   Public insurance coverage:\", \n",
    "          round(100 * public_payers / total_n, 1), \"%\\n\")\n",
    "    }\n",
    "    \n",
    "    if(var == \"ZIPINC_QRTL\") {\n",
    "      cat(\"\\nINCOME DISTRIBUTION INSIGHTS:\\n\")\n",
    "      low_income <- sum(freq_table[c(\"1\", \"2\")], na.rm = TRUE)  # Q1 + Q2\n",
    "      cat(\"   Lower-income representation:\", \n",
    "          round(100 * low_income / total_n, 1), \"%\\n\")\n",
    "    }\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Cross-tabulation insights\n",
    "cat(\"----------------------------------------------------------------\\n\")\n",
    "cat(\"KEY RELATIONSHIPS & CROSS-TABULATIONS\\n\")\n",
    "cat(\"----------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "# Race vs Payer cross-tab\n",
    "if(all(c(\"RACE\", \"PAY1\") %in% available_vars)) {\n",
    "  cat(\"RACE x PAYER DISTRIBUTION:\\n\")\n",
    "  cross_tab <- table(NASS$RACE, NASS$PAY1)\n",
    "  prop_tab <- round(100 * prop.table(cross_tab, 1), 1)\n",
    "  \n",
    "  race_labels <- c(\"1\"=\"White\", \"2\"=\"Black\", \"3\"=\"Hispanic\", \"4\"=\"Asian/Pacific\", \"5\"=\"Native Am\", \"6\"=\"Other\")\n",
    "  pay_labels <- c(\"1\"=\"Medicare\", \"2\"=\"Medicaid\", \"3\"=\"Private\", \"4\"=\"Self-pay\", \"5\"=\"No Charge\", \"6\"=\"Other\")\n",
    "  \n",
    "  for(race in rownames(prop_tab)) {\n",
    "    if(race %in% names(race_labels)) {\n",
    "      cat(\"   \", race_labels[race], \"patients:\\n\")\n",
    "      race_row <- prop_tab[race, ]\n",
    "      sorted_payers <- sort(race_row, decreasing = TRUE)\n",
    "      for(i in 1:min(3, length(sorted_payers))) {\n",
    "        payer <- names(sorted_payers)[i]\n",
    "        if(payer %in% names(pay_labels)) {\n",
    "          cat(\"     \", pay_labels[payer], \":\", sorted_payers[i], \"%\\n\")\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Age vs Income relationship\n",
    "if(all(c(\"AGE\", \"ZIPINC_QRTL\") %in% available_vars)) {\n",
    "  cat(\"AGE x INCOME PATTERNS:\\n\")\n",
    "  age_income <- NASS[!is.na(AGE) & !is.na(ZIPINC_QRTL), .(mean_age = round(mean(AGE), 1)), by = ZIPINC_QRTL]\n",
    "  setorder(age_income, ZIPINC_QRTL)\n",
    "  \n",
    "  income_labels <- c(\"1\"=\"Q1 (Lowest)\", \"2\"=\"Q2\", \"3\"=\"Q3\", \"4\"=\"Q4 (Highest)\")\n",
    "  for(i in 1:nrow(age_income)) {\n",
    "    quartile <- as.character(age_income$ZIPINC_QRTL[i])\n",
    "    if(quartile %in% names(income_labels)) {\n",
    "      cat(\"   \", income_labels[quartile], \"- Average age:\", age_income$mean_age[i], \"years\\n\")\n",
    "    }\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Hospital characteristics summary\n",
    "if(all(c(\"HOSP_REGION\", \"HOSP_TEACH\", \"HOSP_LOCATION\") %in% available_vars)) {\n",
    "  cat(\"HOSPITAL CHARACTERISTICS:\\n\")\n",
    "  \n",
    "  # Unique hospital count\n",
    "  if(\"HOSP_NASS\" %in% names(NASS)) {\n",
    "    unique_hospitals <- length(unique(NASS$HOSP_NASS))\n",
    "    cat(\"   Total hospitals in sample:\", unique_hospitals, \"\\n\")\n",
    "  }\n",
    "  \n",
    "  # Teaching hospital distribution\n",
    "  teaching_dist <- table(NASS$HOSP_TEACH)\n",
    "  if(\"1\" %in% names(teaching_dist)) {\n",
    "    cat(\"   Teaching hospitals:\", round(100 * teaching_dist[\"1\"] / sum(teaching_dist), 1), \"%\\n\")\n",
    "  }\n",
    "  \n",
    "  # Urban/Rural distribution\n",
    "  location_dist <- table(NASS$HOSP_LOCATION)\n",
    "  if(\"1\" %in% names(location_dist)) {\n",
    "    cat(\"   Urban hospitals:\", round(100 * location_dist[\"1\"] / sum(location_dist), 1), \"%\\n\")\n",
    "  }\n",
    "  \n",
    "  # Regional distribution\n",
    "  region_dist <- table(NASS$HOSP_REGION)\n",
    "  region_labels <- c(\"1\"=\"Northeast\", \"2\"=\"Midwest\", \"3\"=\"South\", \"4\"=\"West\")\n",
    "  cat(\"   Regional distribution:\\n\")\n",
    "  for(region in names(region_dist)) {\n",
    "    if(region %in% names(region_labels)) {\n",
    "      pct <- round(100 * region_dist[region] / sum(region_dist), 1)\n",
    "      cat(\"     \", region_labels[region], \":\", pct, \"%\\n\")\n",
    "    }\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Data quality assessment\n",
    "cat(\"----------------------------------------------------------------\\n\")\n",
    "cat(\"DATA QUALITY ASSESSMENT\\n\")\n",
    "cat(\"----------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "# Missing data patterns\n",
    "total_cells <- nrow(NASS) * ncol(NASS)\n",
    "missing_cells <- sum(is.na(NASS))\n",
    "complete_cases <- sum(complete.cases(NASS))\n",
    "\n",
    "cat(\"COMPLETENESS METRICS:\\n\")\n",
    "cat(\"   Overall completeness:\", round(100 * (1 - missing_cells/total_cells), 2), \"%\\n\")\n",
    "cat(\"   Complete cases (no missing):\", format(complete_cases, big.mark = \",\"), \n",
    "    \"(\", round(100 * complete_cases / nrow(NASS), 1), \"%)\\n\")\n",
    "cat(\"   Variables with missing data:\", sum(sapply(NASS, function(x) any(is.na(x)))), \n",
    "    \"out of\", ncol(NASS), \"\\n\\n\")\n",
    "\n",
    "# Variables with highest missing rates\n",
    "missing_rates <- sapply(NASS, function(x) round(100 * sum(is.na(x)) / length(x), 1))\n",
    "high_missing <- missing_rates[missing_rates > 0]\n",
    "if(length(high_missing) > 0) {\n",
    "  cat(\"VARIABLES WITH MISSING DATA:\\n\")\n",
    "  sorted_missing <- sort(high_missing, decreasing = TRUE)\n",
    "  for(i in 1:min(10, length(sorted_missing))) {\n",
    "    cat(\"   \", names(sorted_missing)[i], \":\", sorted_missing[i], \"% missing\\n\")\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Survey weights summary\n",
    "if(\"DISCWT\" %in% available_vars) {\n",
    "  cat(\"SURVEY WEIGHTS SUMMARY:\\n\")\n",
    "  weights <- NASS$DISCWT[!is.na(NASS$DISCWT)]\n",
    "  cat(\"   Weight range:\", round(min(weights), 2), \"to\", round(max(weights), 2), \"\\n\")\n",
    "  cat(\"   Effective sample size:\", round(sum(weights)^2 / sum(weights^2)), \"\\n\")\n",
    "  cat(\"   Design effect (approx):\", round(nrow(NASS) / (sum(weights)^2 / sum(weights^2)), 2), \"\\n\\n\")\n",
    "}\n",
    "\n",
    "cat(\"================================================================\\n\")\n",
    "cat(\"                     SUMMARY COMPLETE                          \\n\")\n",
    "cat(\"     Dataset ready for advanced statistical analysis           \\n\")\n",
    "cat(\"================================================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Census Data Comparisons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20a6a8a8"
   },
   "source": [
    "### Census API Setup\n",
    "\n",
    "Set up Census API integration and pull 2020 DHC population data for comparison with NASS sample proportions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ Please Register at the Census website for an API Key ](https://api.census.gov/data/key_signup.html). Enter your Census API key for data retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6d0c69b9"
   },
   "outputs": [],
   "source": [
    "import getpass, os, json, textwrap\n",
    "os.environ[\"CENSUS_API_KEY\"] = getpass.getpass(\"Enter your Census API key (will not echo):\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3c39ad38"
   },
   "source": [
    "### Census Data Retrieval and Processing\n",
    "\n",
    "Pull 2020 DHC population data by age, gender, and race for states included in NASS sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "240fd569"
   },
   "outputs": [],
   "source": [
    "%%R -i VERBOSE_PRINTS\n",
    "\n",
    "# Install and load required packages for Census analysis\n",
    "required_packages <- c(\"tidycensus\", \"dplyr\", \"tidyr\", \"survey\")\n",
    "\n",
    "for(pkg in required_packages) {\n",
    "  if(!require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "    install.packages(pkg, quiet = TRUE)\n",
    "    library(pkg, character.only = TRUE, quietly = TRUE)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Set Census API key\n",
    "census_api_key(Sys.getenv(\"CENSUS_API_KEY\"), overwrite = FALSE, install = FALSE)\n",
    "\n",
    "# Define states included in NASS 2020 dataset\n",
    "states_in_nass <- c(\"Alaska\", \"California\", \"Colorado\", \"Connecticut\", \"District of Columbia\", \n",
    "                    \"Florida\", \"Georgia\", \"Hawaii\", \"Iowa\", \"Illinois\", \"Indiana\", \"Kansas\", \n",
    "                    \"Kentucky\", \"Maryland\", \"Maine\", \"Michigan\", \"Minnesota\", \"Missouri\", \n",
    "                    \"North Carolina\", \"North Dakota\", \"Nebraska\", \"New Jersey\", \"Nevada\", \n",
    "                    \"New York\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"South Carolina\", \n",
    "                    \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Wisconsin\")\n",
    "\n",
    "cat(\"Defined\", length(states_in_nass), \"states included in NASS sample\\n\")\n",
    "\n",
    "# Function to construct population variables for Census queries\n",
    "get_population_variables <- function(base_variable) {\n",
    "    variables <- paste0(base_variable, \"_\", sprintf(\"%03dN\", 1:49))\n",
    "    \n",
    "    labels <- c(\n",
    "        \"Total\",\n",
    "        \"Male: Total\", \"Male: Under 5 years\", \"Male: 5 to 9 years\", \"Male: 10 to 14 years\",\n",
    "        \"Male: 15 to 17 years\", \"Male: 18 and 19 years\", \"Male: 20 years\", \"Male: 21 years\",\n",
    "        \"Male: 22 to 24 years\", \"Male: 25 to 29 years\", \"Male: 30 to 34 years\", \"Male: 35 to 39 years\",\n",
    "        \"Male: 40 to 44 years\", \"Male: 45 to 49 years\", \"Male: 50 to 54 years\", \"Male: 55 to 59 years\",\n",
    "        \"Male: 60 and 61 years\", \"Male: 62 to 64 years\", \"Male: 65 and 66 years\", \"Male: 67 to 69 years\",\n",
    "        \"Male: 70 to 74 years\", \"Male: 75 to 79 years\", \"Male: 80 to 84 years\", \"Male: 85 years and over\",\n",
    "        \"Female: Total\", \"Female: Under 5 years\", \"Female: 5 to 9 years\", \"Female: 10 to 14 years\",\n",
    "        \"Female: 15 to 17 years\", \"Female: 18 and 19 years\", \"Female: 20 years\", \"Female: 21 years\",\n",
    "        \"Female: 22 to 24 years\", \"Female: 25 to 29 years\", \"Female: 30 to 34 years\", \"Female: 35 to 39 years\",\n",
    "        \"Female: 40 to 44 years\", \"Female: 45 to 49 years\", \"Female: 50 to 54 years\", \"Female: 55 to 59 years\",\n",
    "        \"Female: 60 and 61 years\", \"Female: 62 to 64 years\", \"Female: 65 and 66 years\", \"Female: 67 to 69 years\",\n",
    "        \"Female: 70 to 74 years\", \"Female: 75 to 79 years\", \"Female: 80 to 84 years\", \"Female: 85 years and over\"\n",
    "    )\n",
    "    \n",
    "    names(labels) <- variables\n",
    "    return(list(variables = variables, labels = labels))\n",
    "}\n",
    "\n",
    "# Function to get population data from Census\n",
    "get_population_data <- function(variables, labels) {\n",
    "    population_data <- get_decennial(\n",
    "        geography = \"state\",\n",
    "        variables = variables,\n",
    "        year = 2020,\n",
    "        sumfile = \"dhc\"\n",
    "    )\n",
    "    \n",
    "    # Replace variable codes with descriptive labels\n",
    "    population_data <- population_data %>% \n",
    "        mutate(variable = recode(variable, !!!setNames(labels, variables)))\n",
    "    \n",
    "    # Reshape data for analysis\n",
    "    population_data <- population_data %>% \n",
    "        pivot_wider(names_from = variable, values_from = value)\n",
    "    \n",
    "    return(population_data)\n",
    "}\n",
    "\n",
    "# Get total population data (all races)\n",
    "cat(\"Retrieving total population data from 2020 Census...\\n\")\n",
    "population_info_total <- get_population_variables(\"P12\")\n",
    "total_population_by_age_gender <- get_population_data(population_info_total$variables, population_info_total$labels)\n",
    "\n",
    "# Get white alone population data\n",
    "cat(\"Retrieving white alone population data from 2020 Census...\\n\")\n",
    "population_info_white <- get_population_variables(\"P12I\")\n",
    "total_population_by_age_gender_white <- get_population_data(population_info_white$variables, population_info_white$labels)\n",
    "\n",
    "cat(\"Census data retrieval complete\\n\")\n",
    "\n",
    "if(VERBOSE_PRINTS) {\n",
    "  cat(\"\\nTotal population data structure:\\n\")\n",
    "  str(total_population_by_age_gender)\n",
    "  cat(\"\\nWhite population data structure:\\n\")\n",
    "  str(total_population_by_age_gender_white)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Comparison: NASS vs Census Proportions\n",
    "\n",
    "Compare unadjusted and weighted proportions of white individuals in NASS sample against Census benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# ========================================\n",
    "# Stage 1a: Unadjusted Proportion Analysis\n",
    "# ========================================\n",
    "\n",
    "cat(\"=== STAGE 1A: UNADJUSTED PROPORTION ANALYSIS ===\\n\")\n",
    "\n",
    "# Calculate unadjusted proportion of white individuals in NASS\n",
    "unadjusted_proportion_white <- mean(NASS$WHITE, na.rm = TRUE)\n",
    "cat(\"Unadjusted proportion of WHITE in NASS:\", round(unadjusted_proportion_white, 4), \"\\n\")\n",
    "\n",
    "# Calculate reference proportion from entire US Census\n",
    "us_census_white_proportion <- sum(total_population_by_age_gender_white$Total, na.rm = TRUE) / \n",
    "                             sum(total_population_by_age_gender$Total, na.rm = TRUE)\n",
    "cat(\"US Census White alone proportion:\", round(us_census_white_proportion, 4), \"\\n\")\n",
    "\n",
    "# Statistical test for unadjusted proportion\n",
    "unadjusted_test <- prop.test(sum(NASS$WHITE, na.rm = TRUE), \n",
    "                            sum(!is.na(NASS$WHITE)), \n",
    "                            p = us_census_white_proportion)\n",
    "cat(\"\\nUnadjusted proportion test results:\\n\")\n",
    "print(unadjusted_test)\n",
    "\n",
    "# ========================================\n",
    "# Stage 1b: Weighted Proportion Analysis\n",
    "# ========================================\n",
    "\n",
    "cat(\"\\n=== STAGE 1B: WEIGHTED PROPORTION ANALYSIS ===\\n\")\n",
    "\n",
    "# Filter Census data for NASS-included states only\n",
    "filtered_total_population <- total_population_by_age_gender %>% \n",
    "    filter(NAME %in% states_in_nass)\n",
    "\n",
    "filtered_white_population <- total_population_by_age_gender_white %>% \n",
    "    filter(NAME %in% states_in_nass)\n",
    "\n",
    "# Calculate true proportion for NASS states\n",
    "total_population_nass_states <- sum(filtered_total_population$Total, na.rm = TRUE)\n",
    "total_white_population_nass_states <- sum(filtered_white_population$Total, na.rm = TRUE)\n",
    "true_proportion_white_nass_states <- total_white_population_nass_states / total_population_nass_states\n",
    "\n",
    "cat(\"True proportion of WHITE in NASS states:\", round(true_proportion_white_nass_states, 4), \"\\n\")\n",
    "\n",
    "# Calculate weighted proportion using survey design\n",
    "survey_design <- svydesign(ids = ~1, weights = ~DISCWT, data = NASS)\n",
    "weighted_proportion_white <- svymean(~WHITE, design = survey_design)\n",
    "cat(\"Weighted proportion of WHITE in NASS:\", round(coef(weighted_proportion_white), 4), \"\\n\")\n",
    "\n",
    "# Statistical test for weighted proportion\n",
    "weighted_test <- svyttest(WHITE ~ 1, design = survey_design, mu = true_proportion_white_nass_states)\n",
    "cat(\"\\nWeighted proportion test results:\\n\")\n",
    "print(weighted_test)\n",
    "\n",
    "cat(\"\\n=== PROPORTION ANALYSIS COMPLETE ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age-Gender Stratified Analysis\n",
    "\n",
    "Detailed comparison of white proportions by age group and gender between NASS sample and Census data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# ========================================\n",
    "# Stage 2: Age-Gender Stratified Analysis\n",
    "# ========================================\n",
    "\n",
    "cat(\"=== STAGE 2: AGE-GENDER STRATIFIED ANALYSIS ===\\n\")\n",
    "\n",
    "# Define age groups matching Census categories\n",
    "age_breaks <- c(-Inf, 4, 9, 14, 17, 19, 20, 21, 24, 29, 34, 39, 44, 49, 54, 59, 61, 64, 66, 69, 74, 79, 84, Inf)\n",
    "age_labels <- c(\"Under 5 years\", \"5 to 9 years\", \"10 to 14 years\", \"15 to 17 years\", \"18 and 19 years\",\n",
    "                \"20 years\", \"21 years\", \"22 to 24 years\", \"25 to 29 years\", \"30 to 34 years\",\n",
    "                \"35 to 39 years\", \"40 to 44 years\", \"45 to 49 years\", \"50 to 54 years\", \"55 to 59 years\",\n",
    "                \"60 and 61 years\", \"62 to 64 years\", \"65 and 66 years\", \"67 to 69 years\", \"70 to 74 years\",\n",
    "                \"75 to 79 years\", \"80 to 84 years\", \"85 years and over\")\n",
    "\n",
    "# Create age group variable in NASS dataset\n",
    "NASS[, AGE_GROUP := cut(AGE, breaks = age_breaks, labels = age_labels, right = TRUE)]\n",
    "NASS[, GENDER := ifelse(FEMALE == 0, \"Male\", \"Female\")]\n",
    "\n",
    "cat(\"Created age groups and gender variables\\n\")\n",
    "\n",
    "# Calculate NASS proportions by age group and gender\n",
    "nass_proportions <- NASS[!is.na(AGE_GROUP) & !is.na(WHITE), \n",
    "                        .(total = .N,\n",
    "                          white = sum(WHITE, na.rm = TRUE),\n",
    "                          proportion_white = mean(WHITE, na.rm = TRUE)), \n",
    "                        by = .(AGE_GROUP, GENDER)]\n",
    "\n",
    "# Add confidence intervals\n",
    "nass_proportions[, ':='(\n",
    "  ci_lower = proportion_white - 1.96 * sqrt((proportion_white * (1 - proportion_white)) / total),\n",
    "  ci_upper = proportion_white + 1.96 * sqrt((proportion_white * (1 - proportion_white)) / total)\n",
    ")]\n",
    "\n",
    "cat(\"Calculated NASS proportions by age-gender groups\\n\")\n",
    "\n",
    "# Process Census data for comparison\n",
    "census_proportions <- total_population_by_age_gender_white %>% \n",
    "    select(NAME, starts_with(\"Male\"), starts_with(\"Female\")) %>% \n",
    "    pivot_longer(cols = -NAME, names_to = \"age_gender\", values_to = \"white_population\") %>% \n",
    "    separate(age_gender, into = c(\"gender\", \"age_group\"), sep = \": \") %>% \n",
    "    left_join(\n",
    "        total_population_by_age_gender %>% \n",
    "            select(NAME, starts_with(\"Male\"), starts_with(\"Female\")) %>% \n",
    "            pivot_longer(cols = -NAME, names_to = \"age_gender\", values_to = \"total_population\") %>% \n",
    "            separate(age_gender, into = c(\"gender\", \"age_group\"), sep = \": \"),\n",
    "        by = c(\"NAME\", \"gender\", \"age_group\")\n",
    "    ) %>% \n",
    "    filter(NAME %in% states_in_nass) %>%  # Filter for NASS states only\n",
    "    group_by(gender, age_group) %>% \n",
    "    summarize(\n",
    "        total_population = sum(total_population, na.rm = TRUE),\n",
    "        white_population = sum(white_population, na.rm = TRUE),\n",
    "        proportion_white = white_population / total_population,\n",
    "        .groups = 'drop'\n",
    "    ) %>% \n",
    "    filter(!is.na(age_group) & age_group != \"Total\")\n",
    "\n",
    "cat(\"Processed Census proportions by age-gender groups\\n\")\n",
    "\n",
    "# Convert to data.table for easier manipulation\n",
    "setDT(census_proportions)\n",
    "setDT(nass_proportions)\n",
    "\n",
    "# Convert age groups to factors for proper plotting\n",
    "census_proportions[, age_group := factor(age_group, levels = age_labels)]\n",
    "nass_proportions[, AGE_GROUP := factor(AGE_GROUP, levels = age_labels)]\n",
    "\n",
    "# Remove any missing age groups\n",
    "census_proportions <- census_proportions[!is.na(age_group)]\n",
    "nass_proportions <- nass_proportions[!is.na(AGE_GROUP)]\n",
    "\n",
    "cat(\"Data preparation complete\\n\")\n",
    "cat(\"NASS age-gender groups:\", nrow(nass_proportions), \"\\n\")\n",
    "cat(\"Census age-gender groups:\", nrow(census_proportions), \"\\n\")\n",
    "\n",
    "if(VERBOSE_PRINTS) {\n",
    "  cat(\"\\nSample NASS proportions:\\n\")\n",
    "  print(head(nass_proportions))\n",
    "  cat(\"\\nSample Census proportions:\\n\")\n",
    "  print(head(census_proportions))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Testing and Visualization\n",
    "\n",
    "Generate statistical tests comparing NASS and Census proportions across age-gender groups, with visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# ========================================\n",
    "# Statistical Testing by Age-Gender Groups\n",
    "# ========================================\n",
    "\n",
    "cat(\"=== STATISTICAL TESTING BY AGE-GENDER GROUPS ===\\n\")\n",
    "\n",
    "# Perform statistical tests for each age-gender combination\n",
    "test_results <- merge(nass_proportions, census_proportions, \n",
    "                     by.x = c(\"AGE_GROUP\", \"GENDER\"), \n",
    "                     by.y = c(\"age_group\", \"gender\"),\n",
    "                     all.x = TRUE)\n",
    "\n",
    "# Function to perform proportion test safely\n",
    "safe_prop_test <- function(white_count, total_count, census_prop) {\n",
    "  if(is.na(white_count) || is.na(total_count) || is.na(census_prop) || \n",
    "     total_count == 0 || census_prop == 0 || census_prop == 1) {\n",
    "    return(list(p.value = NA, significant = FALSE))\n",
    "  }\n",
    "  \n",
    "  tryCatch({\n",
    "    test_result <- prop.test(white_count, total_count, p = census_prop)\n",
    "    return(list(p.value = test_result$p.value, \n",
    "               significant = test_result$p.value < 0.05))\n",
    "  }, error = function(e) {\n",
    "    return(list(p.value = NA, significant = FALSE))\n",
    "  })\n",
    "}\n",
    "\n",
    "# Apply tests\n",
    "test_results[, c(\"p_value\", \"significant\") := {\n",
    "  test_res = safe_prop_test(white, total, proportion_white.y)\n",
    "  list(test_res$p.value, test_res$significant)\n",
    "}, by = 1:nrow(test_results)]\n",
    "\n",
    "cat(\"Statistical tests completed\\n\")\n",
    "\n",
    "# Summary of significant differences\n",
    "sig_count <- sum(test_results$significant, na.rm = TRUE)\n",
    "total_tests <- sum(!is.na(test_results$p_value))\n",
    "cat(\"Significant differences found:\", sig_count, \"out of\", total_tests, \"tests\\n\")\n",
    "\n",
    "# Display results table\n",
    "results_summary <- test_results[, .(\n",
    "  AGE_GROUP, GENDER,\n",
    "  NASS_count = total,\n",
    "  NASS_white_prop = round(proportion_white.x, 3),\n",
    "  Census_white_prop = round(proportion_white.y, 3),\n",
    "  p_value = round(p_value, 6),\n",
    "  significant = significant\n",
    ")][order(AGE_GROUP, GENDER)]\n",
    "\n",
    "cat(\"\\nDetailed results by age-gender group:\\n\")\n",
    "print(results_summary)\n",
    "\n",
    "# ========================================\n",
    "# Visualization\n",
    "# ========================================\n",
    "\n",
    "cat(\"\\n=== GENERATING VISUALIZATIONS ===\\n\")\n",
    "\n",
    "# Check if ggplot2 is available for plotting\n",
    "if(require(\"ggplot2\", quietly = TRUE)) {\n",
    "  \n",
    "  # Prepare data for plotting\n",
    "  plot_data_nass <- nass_proportions[, .(\n",
    "    AGE_GROUP, GENDER, proportion_white, ci_lower, ci_upper, source = \"NASS\"\n",
    "  )]\n",
    "  \n",
    "  plot_data_census <- census_proportions[, .(\n",
    "    AGE_GROUP = age_group, GENDER = gender, proportion_white, source = \"Census\"\n",
    "  )]\n",
    "  plot_data_census[, ':='(ci_lower = proportion_white, ci_upper = proportion_white)]\n",
    "  \n",
    "  plot_data <- rbind(plot_data_nass, plot_data_census, fill = TRUE)\n",
    "  \n",
    "  # Create the plot\n",
    "  age_gender_plot <- ggplot(plot_data, aes(x = AGE_GROUP, y = proportion_white, \n",
    "                                          color = source, group = interaction(source, GENDER))) +\n",
    "    geom_line(linewidth = 1) +\n",
    "    geom_point() +\n",
    "    geom_ribbon(data = plot_data[source == \"NASS\"], \n",
    "                aes(ymin = ci_lower, ymax = ci_upper, fill = source), \n",
    "                alpha = 0.2, color = NA) +\n",
    "    facet_wrap(~GENDER, ncol = 1) +\n",
    "    labs(title = \"White Proportion by Age Group: NASS vs Census\",\n",
    "         subtitle = \"Comparison across age groups and gender\",\n",
    "         x = \"Age Group\",\n",
    "         y = \"Proportion White\",\n",
    "         color = \"Data Source\",\n",
    "         fill = \"Confidence Interval\") +\n",
    "    theme_minimal() +\n",
    "    theme(axis.text.x = element_text(angle = 45, hjust = 1),\n",
    "          legend.position = \"bottom\") +\n",
    "    scale_y_continuous(limits = c(0, 1), labels = scales::percent)\n",
    "  \n",
    "  print(age_gender_plot)\n",
    "  cat(\"Visualization generated successfully\\n\")\n",
    "  \n",
    "} else {\n",
    "  cat(\"ggplot2 not available - using base R plotting\\n\")\n",
    "  \n",
    "  # Base R fallback plotting\n",
    "  par(mfrow = c(2, 1), mar = c(8, 4, 4, 2))\n",
    "  \n",
    "  # Males plot\n",
    "  male_nass <- nass_proportions[GENDER == \"Male\"]\n",
    "  male_census <- census_proportions[gender == \"Male\"]\n",
    "  \n",
    "  plot(1:nrow(male_nass), male_nass$proportion_white, type = \"b\", col = \"blue\",\n",
    "       xlab = \"\", ylab = \"Proportion White\", main = \"Males: NASS vs Census\",\n",
    "       ylim = c(0, 1), xaxt = \"n\")\n",
    "  lines(1:nrow(male_census), male_census$proportion_white, type = \"b\", col = \"red\")\n",
    "  axis(1, at = 1:nrow(male_nass), labels = male_nass$AGE_GROUP, las = 2, cex.axis = 0.8)\n",
    "  legend(\"topright\", legend = c(\"NASS\", \"Census\"), col = c(\"blue\", \"red\"), lty = 1)\n",
    "  \n",
    "  # Females plot\n",
    "  female_nass <- nass_proportions[GENDER == \"Female\"]\n",
    "  female_census <- census_proportions[gender == \"Female\"]\n",
    "  \n",
    "  plot(1:nrow(female_nass), female_nass$proportion_white, type = \"b\", col = \"blue\",\n",
    "       xlab = \"Age Group\", ylab = \"Proportion White\", main = \"Females: NASS vs Census\",\n",
    "       ylim = c(0, 1), xaxt = \"n\")\n",
    "  lines(1:nrow(female_census), female_census$proportion_white, type = \"b\", col = \"red\")\n",
    "  axis(1, at = 1:nrow(female_nass), labels = female_nass$AGE_GROUP, las = 2, cex.axis = 0.8)\n",
    "  legend(\"topright\", legend = c(\"NASS\", \"Census\"), col = c(\"blue\", \"red\"), lty = 1)\n",
    "  \n",
    "  par(mfrow = c(1, 1))\n",
    "  cat(\"Base R visualization generated\\n\")\n",
    "}\n",
    "\n",
    "cat(\"\\n=== CENSUS AGE-GENDER ANALYSIS COMPLETE ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-level Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
