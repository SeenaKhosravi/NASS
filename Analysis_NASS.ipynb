{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbe6c3e6"
      },
      "source": [
        "# Socioeconomic and Demographic Drivers of Ambulatory Surgery Usage\n",
        "### HCUP NASS 2020 – Reproducible Pipeline (Python + R)\n",
        "\n",
        "**Author:** Seena Khosravi, MD  \n",
        "**LLMs Utilized:** Claude Sonnet 4, Opus 4; ChatGPT 4o, o4; Deepseek 3.1; Gemini 2.5 Pro  \n",
        "**Last Updated:** September 13, 2025  \n",
        "\n",
        "**Data Source:**  \n",
        "Department of Health & Human Services (HHS)  \n",
        "Agency for Healthcare Research and Quality (AHRQ)  \n",
        "Healthcare Cost and Utilization Project (HCUP)  \n",
        "National Ambulatory Surgical Sample (NASS)\n",
        "Year - 2020\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "This notebook provides a reproducible analysis pipeline for examining socioeconomic and demographic factors influencing ambulatory surgery usage patterns. The analysis combines Python for data processing and R for statistical modeling.\n",
        "\n",
        "### Data Usage Agreement\n",
        "**DUA Compliant Online Implementation** — This notebook uses a simulated, artificial, smaller dataset with identical structure to the file created by [Raw_NASS_Processing.R](https://github.com/SeenaKhosravi/NASS/blob/a7764ce80be8a82fc449831821c27d957176c410/Raw%20NASS%20%20Processing.R). The simulated dataset production methodology is also in [Generate_Simulated_NASS.R](https://github.com/SeenaKhosravi/NASS/blob/161bf2b5c149da9654c0e887655b361fa2176db0/Generate_Simulated_NASS.R). If DUA signed and data purchased from HCUP, this notebook can run on full dataset loaded from your local or cloud storage.\n",
        "\n",
        "[Please see the DUA Agreement here.](https://hcup-us.ahrq.gov/team/NationwideDUA.jsp)\n",
        "\n",
        "### Key Features\n",
        "- **Multiple Platform:** Works on jupyter implementations via local environments, server, cloud VM instance, or platform as a service.\n",
        "- **Flexible Data Storage:** GitHub (simulated, static, open access), Google Drive, Google Cloud Storage, or local file\n",
        "- **Reproducible:** All dependencies and environment setup included; assumes new, unmodified colab/vertex instances\n",
        "- **Scalable:** Handles both simulated (0.2GB, 139k rows) and full dataset (12 GB, 7.8M rows). Scalable cloud options."
      ],
      "metadata": {
        "id": "aOmuZCsusrTj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "019b6fe9"
      },
      "source": [
        "---\n",
        "\n",
        "## Design Notes\n",
        "\n",
        "### Architecture\n",
        "- **Python primary, w/ R run via rpy2 python extension**\n",
        "- **Python cells** handle \"plumbing\" (file I/O, environment setup, rpy2 configuration, data previews)\n",
        "- **R cells** (prefixed by `%%R`) perform statistical analysis: survey weights, Census lookups, multilevel models, plots, classifiers, etc.\n",
        "\n",
        "### Data Sources\n",
        "- **Default:** Simulated dataset (1GB) from GitHub releases\n",
        "- **Local:** Switch to locally stored files via configuration\n",
        "- **Drive:** Google Drive (Only availble in Colab)\n",
        "- **Cloud:** Google Cloud Storage support for large datasets\n",
        "\n",
        "\n",
        "### Environment Support\n",
        "- Local (Jupyterlab w/ Python 3.11.5 kernel)\n",
        "- Jupyter Server (may require some configuring depending on your implementation)\n",
        "- Google Colab (Pro recommended, high-ram option)\n",
        "- Vertex AI Workbench (JupyterLab 3, Python 3 kernel) (used for full analysis)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "IvQPKX8GrfZr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUEIOQlXrPrd"
      },
      "source": [
        "---\n",
        "## 1. Configuration\n",
        "\n",
        "Configure all settings here prior to run - data sources, debugging options, and file paths. Defaults to simulated dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUU1wwMKrPre"
      },
      "outputs": [],
      "source": [
        "# ==================== CONFIGURATION ====================\n",
        "# Data Source Options\n",
        "DATA_SOURCE = \"github\"      # Options: \"github\", \"local\", \"gcs\", \"drive\"\n",
        "VERBOSE_PRINTS = True       # False → suppress debug output\n",
        "\n",
        "# GitHub source (default - simulated data)\n",
        "GITHUB_URL = \"https://github.com/SeenaKhosravi/NASS/releases/download/v1.0.0/nass_2020_simulated.csv\"\n",
        "\n",
        "# Local file options\n",
        "LOCAL_FILENAME = \"nass_2020_local.csv\"\n",
        "\n",
        "# Google Cloud Storage options\n",
        "GCS_BUCKET = \"nass_2020\"\n",
        "GCS_BLOB = \"nass_2020_all.csv\"\n",
        "GCS_SERVICE_ACCOUNT_KEY = \"/path/to/service-account-key.json\"  # Optional\n",
        "\n",
        "# Google Drive options (for Colab)\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/NASS/nass_2020_full.csv\"\n",
        "# ======================================================\n",
        "\n",
        "print(\"✓ Configuration loaded\")\n",
        "print(f\"  Data source: {DATA_SOURCE}\")\n",
        "print(f\"  Verbose mode: {VERBOSE_PRINTS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WJ8_5sRrPre"
      },
      "source": [
        "---\n",
        "## 2. Environment Setup & Package Installation\n",
        "\n",
        "Detect environment, and install python packages, via Conda if present, with fallbacks. If in colab, mount google drive.\n",
        "\n",
        "Note: If in Vertex VM instance for the first time, you likely need to install R. Please skip following cell, and return after completing subsequent cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9RcJjrWrPrf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "class EnvironmentManager:\n",
        "    def __init__(self):\n",
        "        self.detect_environment()\n",
        "        self.setup_packages()\n",
        "\n",
        "    def detect_environment(self):\n",
        "        \"\"\"Detect runtime environment\"\"\"\n",
        "        self.is_colab = 'COLAB_GPU' in os.environ or 'google.colab' in sys.modules\n",
        "        self.is_vertex = 'DL_ANACONDA_HOME' in os.environ\n",
        "\n",
        "        if self.is_colab:\n",
        "            self.env_type = \"Google Colab\"\n",
        "        elif self.is_vertex:\n",
        "            self.env_type = \"Vertex AI\"\n",
        "        else:\n",
        "            self.env_type = \"Local/Jupyter\"\n",
        "\n",
        "        print(f\"Environment: {self.env_type}\")\n",
        "\n",
        "    def check_conda_available(self):\n",
        "        \"\"\"Check if conda is available\"\"\"\n",
        "        try:\n",
        "            subprocess.check_call(['conda', '--version'],\n",
        "                                stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "            return True\n",
        "        except (subprocess.CalledProcessError, FileNotFoundError):\n",
        "            return False\n",
        "\n",
        "    def install_package(self, package, conda_name=None):\n",
        "        \"\"\"Smart package installation with fallback\"\"\"\n",
        "        try:\n",
        "            __import__(package)\n",
        "            return True\n",
        "        except ImportError:\n",
        "            print(f\"Installing {package}...\")\n",
        "\n",
        "            # Try conda first if available and not in Colab\n",
        "            if conda_name and not self.is_colab and self.check_conda_available():\n",
        "                try:\n",
        "                    subprocess.check_call(['conda', 'install', '-y', conda_name],\n",
        "                                        stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "                    return True\n",
        "                except subprocess.CalledProcessError:\n",
        "                    print(f\"  Conda install failed for {conda_name}, trying pip...\")\n",
        "\n",
        "            # Fallback to pip\n",
        "            try:\n",
        "                subprocess.check_call([sys.executable, '-m', 'pip', 'install', package],\n",
        "                                    stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "                return True\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                print(f\"  Pip install failed for {package}: {e}\")\n",
        "                return False\n",
        "\n",
        "    def setup_packages(self):\n",
        "        \"\"\"Install required packages efficiently\"\"\"\n",
        "        packages = {\n",
        "            'pandas': 'pandas',\n",
        "            'requests': 'requests',\n",
        "            'rpy2': 'rpy2',\n",
        "            'google.cloud.storage': 'google-cloud-storage'\n",
        "        }\n",
        "\n",
        "        print(\"Installing/checking packages...\")\n",
        "        failed = []\n",
        "\n",
        "        for pkg, install_name in packages.items():\n",
        "            if not self.install_package(pkg, install_name):\n",
        "                failed.append(pkg)\n",
        "\n",
        "        # Store failed packages globally for recovery\n",
        "        globals()['failed_packages'] = failed\n",
        "\n",
        "        if failed:\n",
        "            print(f\"⚠️  Failed to install: {', '.join(failed)}\")\n",
        "            print(\"Some features may not work\")\n",
        "\n",
        "            # Provide specific guidance for rpy2\n",
        "            if 'rpy2' in failed:\n",
        "                print(\"\\n💡 For rpy2 installation issues:\")\n",
        "                if self.is_vertex:\n",
        "                    print(\"   - Vertex AI: R may not be installed by default\")\n",
        "                    print(\"   - Run the next cell for automated R setup\")\n",
        "                else:\n",
        "                    print(\"   - On Windows: May need Visual Studio Build Tools\")\n",
        "                    print(\"   - Try: conda install -c conda-forge rpy2\")\n",
        "                    print(\"   - Or: pip install rpy2 (requires R to be installed)\")\n",
        "        else:\n",
        "            print(\"✓ All packages ready\")\n",
        "\n",
        "        # Mount Google Drive if needed (check if DATA_SOURCE exists)\n",
        "        try:\n",
        "            if globals().get('DATA_SOURCE') == \"drive\" and self.is_colab:\n",
        "                self.mount_drive()\n",
        "        except NameError:\n",
        "            pass  # DATA_SOURCE not defined yet\n",
        "\n",
        "    def mount_drive(self):\n",
        "        \"\"\"Mount Google Drive in Colab\"\"\"\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"✓ Google Drive mounted\")\n",
        "        except:\n",
        "            print(\"❌ Failed to mount Google Drive\")\n",
        "\n",
        "# Initialize environment\n",
        "env_manager = EnvironmentManager()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyZ9A_kurPrf"
      },
      "source": [
        "If Running in Vertex Workbench Instance for first time (or if last cell gave error), run this cell to install R, and re-run last cell.\n",
        "\n",
        "Otherwise, skip this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4eMeiiMrPrg"
      },
      "outputs": [],
      "source": [
        "# Streamlined R Installation for Vertex AI (and other Linux environments)\n",
        "def install_latest_r():\n",
        "    \"\"\"Install or verify latest R installation\"\"\"\n",
        "    print(\"🔧 R Installation Check...\")\n",
        "\n",
        "    # Check current R version\n",
        "    r_version = None\n",
        "    try:\n",
        "        result = subprocess.check_output(['R', '--version'], text=True)\n",
        "        r_version = result.split('\\n')[0]\n",
        "        print(f\"✓ R found: {r_version}\")\n",
        "\n",
        "        # Check if it's reasonably recent (R 4.0+)\n",
        "        if \"R version 4.\" in r_version and not r_version.startswith(\"R version 4.0\"):\n",
        "            print(\"✓ R version is current - no update needed\")\n",
        "            return True\n",
        "        elif \"R version 4.0\" in r_version:\n",
        "            print(\"⚠️ R 4.0 detected - will upgrade to latest for better package compatibility\")\n",
        "        else:\n",
        "            print(\"⚠️ Older R version detected - will upgrade\")\n",
        "\n",
        "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
        "        print(\"❌ R not found - installing latest version\")\n",
        "\n",
        "    # Install/upgrade R\n",
        "    print(\"📦 Installing latest R from CRAN...\")\n",
        "\n",
        "    commands = [\n",
        "        # Update system\n",
        "        \"sudo apt-get update -qq\",\n",
        "\n",
        "        # Install prerequisites\n",
        "        \"sudo apt-get install -y software-properties-common dirmngr\",\n",
        "\n",
        "        # Add CRAN key and repository\n",
        "        \"sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9\",\n",
        "\n",
        "        # Add repository (auto-detect Ubuntu version)\n",
        "        f\"sudo add-apt-repository -y 'deb https://cloud.r-project.org/bin/linux/ubuntu $(lsb_release -cs)-cran40/'\",\n",
        "\n",
        "        # Update and install R\n",
        "        \"sudo apt-get update -qq\",\n",
        "        \"sudo apt-get install -y r-base r-base-dev\",\n",
        "\n",
        "        # Install essential build tools\n",
        "        \"sudo apt-get install -y build-essential libcurl4-openssl-dev libssl-dev libxml2-dev\"\n",
        "    ]\n",
        "\n",
        "    for cmd in commands:\n",
        "        try:\n",
        "            subprocess.run(cmd, shell=True, check=True,\n",
        "                         stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"⚠️ Command failed: {cmd}\")\n",
        "            if \"add-apt-repository\" in cmd:\n",
        "                # Fallback to Ubuntu repo if CRAN fails\n",
        "                print(\"📦 Falling back to Ubuntu repository R...\")\n",
        "                subprocess.run(\"sudo apt-get install -y r-base r-base-dev\",\n",
        "                             shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "                break\n",
        "            continue\n",
        "\n",
        "    # Verify installation\n",
        "    try:\n",
        "        result = subprocess.check_output(['R', '--version'], text=True)\n",
        "        new_version = result.split('\\n')[0]\n",
        "        print(f\"✅ R installation complete: {new_version}\")\n",
        "        return True\n",
        "    except:\n",
        "        print(\"❌ R installation verification failed\")\n",
        "        return False\n",
        "\n",
        "def install_rpy2():\n",
        "    \"\"\"Install rpy2 with multiple fallback methods\"\"\"\n",
        "    print(\"🐍 Installing rpy2...\")\n",
        "\n",
        "    methods = [\n",
        "        ([sys.executable, '-m', 'pip', 'install', '--upgrade', 'pip', 'setuptools'], \"upgrade pip\"),\n",
        "        ([sys.executable, '-m', 'pip', 'install', 'rpy2'], \"pip install rpy2\"),\n",
        "        (['conda', 'install', '-c', 'conda-forge', 'rpy2', '-y'], \"conda install rpy2\")\n",
        "    ]\n",
        "\n",
        "    for cmd, desc in methods:\n",
        "        try:\n",
        "            subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "            # Test import\n",
        "            import rpy2\n",
        "            print(f\"✅ rpy2 installed successfully via {desc}\")\n",
        "            return True\n",
        "        except (subprocess.CalledProcessError, ImportError, FileNotFoundError):\n",
        "            continue\n",
        "\n",
        "    print(\"❌ rpy2 installation failed\")\n",
        "    return False\n",
        "\n",
        "# Main execution\n",
        "if env_manager.is_vertex or 'rpy2' in globals().get('failed_packages', []):\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Install R if needed\n",
        "    r_success = install_latest_r()\n",
        "\n",
        "    # Install rpy2 if needed\n",
        "    if r_success and 'rpy2' in globals().get('failed_packages', []):\n",
        "        rpy2_success = install_rpy2()\n",
        "        if rpy2_success:\n",
        "            print(\"🎉 Setup complete! Please restart Python kernel and re-run environment setup.\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "else:\n",
        "    print(\"✓ Skipping R installation (not needed for this environment)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm4dTQb1rPrh"
      },
      "source": [
        "---\n",
        "## 3. R Environment Setup\n",
        "\n",
        "Load R integration and install R packages efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wpUbizDrPrh"
      },
      "outputs": [],
      "source": [
        "# Load rpy2 extension for R integration\n",
        "try:\n",
        "    %load_ext rpy2.ipython\n",
        "    print(\"✓ R integration loaded\")\n",
        "    globals()['R_AVAILABLE'] = True\n",
        "except Exception as e:\n",
        "    print(f\"❌ Failed to load R integration: {e}\")\n",
        "\n",
        "    # Windows-specific troubleshooting\n",
        "    if \"R.dll\" in str(e) or \"error 0x7e\" in str(e):\n",
        "        print(\"\\n💡 Windows R.dll loading issue detected:\")\n",
        "        print(\"   This is a common Windows + rpy2 compatibility issue\")\n",
        "        print(\"   Solutions:\")\n",
        "        print(\"   1. Restart Python kernel and try again\")\n",
        "        print(\"   2. Check R version compatibility with rpy2\")\n",
        "        print(\"   3. Try reinstalling R and rpy2\")\n",
        "        print(\"   4. Use Python-only analysis (fallback available)\")\n",
        "        globals()['R_AVAILABLE'] = False\n",
        "    else:\n",
        "        print(\"Install rpy2: pip install rpy2\")\n",
        "        globals()['R_AVAILABLE'] = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5iP0V_1rPri"
      },
      "source": [
        "Install Essential R packages, try for a few optional packages.\n",
        "\n",
        "Note: Other packages needed for specific analysis (ie. advanced modeling packages) will be installed and called as needed later in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGFfQQC5rPri"
      },
      "outputs": [],
      "source": [
        "%%R -i VERBOSE_PRINTS\n",
        "\n",
        "# Environment-aware R package setup\n",
        "# Standard approach, then Vertex AI fallback if needed\n",
        "\n",
        "# Detect environment\n",
        "is_vertex <- Sys.getenv(\"DL_ANACONDA_HOME\") != \"\"\n",
        "is_colab <- Sys.getenv(\"COLAB_GPU\") != \"\"\n",
        "\n",
        "if(is_colab) {\n",
        "  cat(\" Google Colab detected...\\n\")\n",
        "} else if(is_vertex) {\n",
        "  cat(\" Vertex AI detected...\\n\")\n",
        "} else {\n",
        "  cat(\" Local environment detected...\\n\")\n",
        "}\n",
        "\n",
        "# Standard clean setup (for all environments initially)\n",
        "essential_packages <- c(\n",
        "  \"data.table\",    # Fast data manipulation\n",
        "  \"ggplot2\",       # Plotting\n",
        "  \"scales\"         # For ggplot2 percentage scales\n",
        ")\n",
        "\n",
        "optional_packages <- c(\n",
        "  \"survey\",        # Survey statistics\n",
        "  \"broom\"          # Model tidying\n",
        ")\n",
        "\n",
        "# Fast installation settings\n",
        "repos <- \"https://cloud.r-project.org\"\n",
        "options(repos = repos)\n",
        "Sys.setenv(MAKEFLAGS = paste0(\"-j\", parallel::detectCores()))\n",
        "\n",
        "# Package check and load functions\n",
        "pkg_available <- function(pkg) {\n",
        "  tryCatch({\n",
        "    find.package(pkg, quiet = TRUE)\n",
        "    TRUE\n",
        "  }, error = function(e) FALSE)\n",
        "}\n",
        "\n",
        "load_pkg <- function(pkg) {\n",
        "  tryCatch({\n",
        "    suppressMessages(library(pkg, character.only = TRUE, quietly = TRUE))\n",
        "    TRUE\n",
        "  }, error = function(e) FALSE)\n",
        "}\n",
        "\n",
        "# Install missing essential packages\n",
        "missing_essential <- essential_packages[!sapply(essential_packages, pkg_available)]\n",
        "\n",
        "if(length(missing_essential) > 0) {\n",
        "  cat(\"Installing essential packages:\", paste(missing_essential, collapse = \", \"), \"\\n\")\n",
        "\n",
        "  tryCatch({\n",
        "    install.packages(missing_essential,\n",
        "                    repos = repos,\n",
        "                    type = getOption(\"pkgType\"),\n",
        "                    dependencies = FALSE,\n",
        "                    quiet = !VERBOSE_PRINTS,\n",
        "                    Ncpus = parallel::detectCores())\n",
        "  }, error = function(e) {\n",
        "    cat(\"Binary install failed, trying source...\\n\")\n",
        "    install.packages(missing_essential,\n",
        "                    repos = repos,\n",
        "                    type = \"source\",\n",
        "                    dependencies = FALSE,\n",
        "                    quiet = !VERBOSE_PRINTS)\n",
        "  })\n",
        "}\n",
        "\n",
        "# Load essential packages\n",
        "essential_loaded <- sapply(essential_packages, load_pkg)\n",
        "essential_success <- sum(essential_loaded)\n",
        "\n",
        "cat(\"✓ Essential packages loaded:\", essential_success, \"/\", length(essential_packages), \"\\n\")\n",
        "\n",
        "# Quick install optional packages (30s timeout)\n",
        "missing_optional <- optional_packages[!sapply(optional_packages, pkg_available)]\n",
        "\n",
        "if(length(missing_optional) > 0) {\n",
        "  cat(\"Installing optional packages...\\n\")\n",
        "\n",
        "  for(pkg in missing_optional) {\n",
        "    tryCatch({\n",
        "      setTimeLimit(cpu = 30, elapsed = 30, transient = TRUE)\n",
        "      install.packages(pkg, repos = repos,\n",
        "                      type = getOption(\"pkgType\"),\n",
        "                      dependencies = FALSE,\n",
        "                      quiet = TRUE)\n",
        "      cat(\"✓\", pkg, \"installed\\n\")\n",
        "    }, error = function(e) {\n",
        "      cat(\"⚠️\", pkg, \"skipped (timeout)\\n\")\n",
        "    })\n",
        "\n",
        "    setTimeLimit(cpu = Inf, elapsed = Inf, transient = FALSE)\n",
        "  }\n",
        "}\n",
        "\n",
        "# Load optional packages\n",
        "optional_loaded <- sapply(optional_packages, load_pkg)\n",
        "optional_success <- sum(optional_loaded)\n",
        "\n",
        "cat(\"✓ Optional packages loaded:\", optional_success, \"/\", length(optional_packages), \"\\n\")\n",
        "\n",
        "# Check if we need Vertex AI aggressive installation\n",
        "has_datatable <- require(\"data.table\", quietly = TRUE)\n",
        "has_ggplot <- require(\"ggplot2\", quietly = TRUE)\n",
        "\n",
        "if(is_vertex && (!has_datatable || !has_ggplot)) {\n",
        "  cat(\"\\n🔧 Standard installation incomplete on Vertex AI - using aggressive method...\\n\")\n",
        "\n",
        "  # Check R version for compatibility\n",
        "  r_version <- R.version.string\n",
        "  r_numeric <- as.numeric(R.version$major) + as.numeric(R.version$minor)/10\n",
        "  cat(\"R version:\", r_version, \"(numeric:\", r_numeric, \")\\n\")\n",
        "\n",
        "  # System dependencies for Vertex AI\n",
        "  system_deps <- c(\n",
        "    \"apt-get update -qq\",\n",
        "    \"apt-get install -y libfontconfig1-dev libcairo2-dev\",\n",
        "    \"apt-get install -y libxml2-dev libcurl4-openssl-dev libssl-dev\",\n",
        "    \"apt-get install -y libharfbuzz-dev libfribidi-dev\",\n",
        "    \"apt-get install -y libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev\"\n",
        "  )\n",
        "\n",
        "  cat(\"Installing system dependencies...\\n\")\n",
        "  for(cmd in system_deps) {\n",
        "    system(paste(\"sudo\", cmd), ignore.stdout = TRUE, ignore.stderr = TRUE)\n",
        "  }\n",
        "\n",
        "  # Aggressive package installation for failed packages\n",
        "  failed_packages <- c()\n",
        "  if(!has_datatable) failed_packages <- c(failed_packages, \"data.table\")\n",
        "  if(!has_ggplot) failed_packages <- c(failed_packages, \"ggplot2\", \"scales\")\n",
        "\n",
        "  repos_vertex <- c(\"https://cran.rstudio.com/\", \"https://cloud.r-project.org\")\n",
        "\n",
        "  for(pkg in failed_packages) {\n",
        "    cat(\"Aggressively installing\", pkg, \"...\")\n",
        "    installed <- FALSE\n",
        "\n",
        "    # For ggplot2/scales, try version-specific installation if R < 4.1\n",
        "    if((pkg == \"ggplot2\" || pkg == \"scales\") && r_numeric < 4.1) {\n",
        "      cat(\"(R < 4.1 detected - trying compatible versions)...\")\n",
        "\n",
        "      # First try to install remotes if not available\n",
        "      if(!require(\"remotes\", quietly = TRUE)) {\n",
        "        tryCatch({\n",
        "          install.packages(\"remotes\", repos = repos_vertex[1], quiet = TRUE)\n",
        "        }, error = function(e) NULL)\n",
        "      }\n",
        "\n",
        "      # Try installing older compatible versions\n",
        "      if(pkg == \"ggplot2\" && require(\"remotes\", quietly = TRUE)) {\n",
        "        # Try ggplot2 versions compatible with older R\n",
        "        old_versions <- c(\"3.4.4\", \"3.4.3\", \"3.4.2\", \"3.4.0\", \"3.3.6\")\n",
        "        for(ver in old_versions) {\n",
        "          tryCatch({\n",
        "            remotes::install_version(\"ggplot2\", version = ver, repos = repos_vertex[1], quiet = TRUE)\n",
        "            if(require(\"ggplot2\", quietly = TRUE)) {\n",
        "              cat(\" ✓ (v\", ver, \")\\n\")\n",
        "              installed <- TRUE\n",
        "              break\n",
        "            }\n",
        "          }, error = function(e) NULL)\n",
        "        }\n",
        "      } else if(pkg == \"scales\" && require(\"remotes\", quietly = TRUE)) {\n",
        "        # Try scales versions compatible with older R\n",
        "        old_versions <- c(\"1.3.0\", \"1.2.1\", \"1.2.0\", \"1.1.1\")\n",
        "        for(ver in old_versions) {\n",
        "          tryCatch({\n",
        "            remotes::install_version(\"scales\", version = ver, repos = repos_vertex[1], quiet = TRUE)\n",
        "            if(require(\"scales\", quietly = TRUE)) {\n",
        "              cat(\" ✓ (v\", ver, \")\\n\")\n",
        "              installed <- TRUE\n",
        "              break\n",
        "            }\n",
        "          }, error = function(e) NULL)\n",
        "        }\n",
        "      }\n",
        "\n",
        "      # Fallback: try archived CRAN packages if remotes failed\n",
        "      if(!installed && pkg == \"ggplot2\") {\n",
        "        cat(\"(trying archived versions)...\")\n",
        "        archived_urls <- c(\n",
        "          \"https://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_3.4.4.tar.gz\",\n",
        "          \"https://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_3.4.0.tar.gz\",\n",
        "          \"https://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_3.3.6.tar.gz\"\n",
        "        )\n",
        "        for(url in archived_urls) {\n",
        "          tryCatch({\n",
        "            install.packages(url, repos = NULL, type = \"source\", quiet = TRUE)\n",
        "            if(require(\"ggplot2\", quietly = TRUE)) {\n",
        "              cat(\" ✓ (archived)\\n\")\n",
        "              installed <- TRUE\n",
        "              break\n",
        "            }\n",
        "          }, error = function(e) NULL)\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "\n",
        "    # Standard installation if version-specific didn't work\n",
        "    if(!installed) {\n",
        "      for(repo in repos_vertex) {\n",
        "        tryCatch({\n",
        "          install.packages(pkg, repos = repo, dependencies = TRUE, quiet = TRUE)\n",
        "          if(require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
        "            cat(\" ✓\\n\")\n",
        "            installed <- TRUE\n",
        "            break\n",
        "          }\n",
        "        }, error = function(e) NULL)\n",
        "      }\n",
        "    }\n",
        "\n",
        "    if(!installed) cat(\" FAILED\\n\")\n",
        "  }\n",
        "\n",
        "  # Re-check after aggressive installation\n",
        "  has_datatable <- require(\"data.table\", quietly = TRUE)\n",
        "  has_ggplot <- require(\"ggplot2\", quietly = TRUE)\n",
        "\n",
        "  cat(\"📦 After aggressive installation: data.table =\", has_datatable, \"| ggplot2 =\", has_ggplot, \"\\n\")\n",
        "}\n",
        "\n",
        "# Final status check (universal)\n",
        "if(has_datatable && has_ggplot) {\n",
        "  cat(\"🎉 Core environment ready! (data.table + ggplot2)\\n\")\n",
        "  setDTthreads(0)  # Use all cores\n",
        "\n",
        "} else if(has_datatable) {\n",
        "  cat(\"⚠️ Partial setup: data.table ready, plotting may be limited\\n\")\n",
        "  setDTthreads(0)\n",
        "\n",
        "} else {\n",
        "  cat(\"❌ Critical failure: data.table not available\\n\")\n",
        "  stop(\"Cannot proceed without data.table\")\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAtjPad_rPri"
      },
      "source": [
        "Check if R setup is complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMh-C5gvrPrj"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "\n",
        "# Quick verification and setup\n",
        "cat(\"Verifying R environment...\\n\")\n",
        "\n",
        "# Test core functionality\n",
        "tryCatch({\n",
        "  # Test data.table (essential)\n",
        "  dt_test <- data.table(x = 1:3, y = letters[1:3])\n",
        "  cat(\"✓ data.table ready\\n\")\n",
        "\n",
        "  # Test ggplot2 (optional)\n",
        "  if(require(\"ggplot2\", quietly = TRUE)) {\n",
        "    cat(\"✓ ggplot2 ready\\n\")\n",
        "  } else {\n",
        "    cat(\"⚠️ ggplot2 not available (plots disabled)\\n\")\n",
        "  }\n",
        "\n",
        "  # Set up data.table options for performance\n",
        "  setDTthreads(0)  # Use all cores\n",
        "\n",
        "  cat(\"✓ R environment optimized and ready!\\n\")\n",
        "\n",
        "}, error = function(e) {\n",
        "  cat(\"❌ R environment verification failed:\", e$message, \"\\n\")\n",
        "  stop(\"R setup incomplete\")\n",
        "})\n",
        "\n",
        "# Clean up test objects\n",
        "rm(list = ls()[!ls() %in% c(\"VERBOSE_PRINTS\")])\n",
        "invisible(gc())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8pUXVbsrPrg"
      },
      "source": [
        "---\n",
        "## 4. Data Loading\n",
        "\n",
        "Config based data loader with error handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-KUtlK-rPrh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "class DataLoader:\n",
        "    def __init__(self):\n",
        "        self.base_path = Path('/content') if env_manager.is_colab else Path.home()\n",
        "        self.data_dir = self.base_path / 'data'\n",
        "        self.data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load data based on configuration\"\"\"\n",
        "        loaders = {\n",
        "            'github': self._load_github,\n",
        "            'local': self._load_local,\n",
        "            'gcs': self._load_gcs,\n",
        "            'drive': self._load_drive\n",
        "        }\n",
        "\n",
        "        if DATA_SOURCE not in loaders:\n",
        "            raise ValueError(f\"Invalid DATA_SOURCE: {DATA_SOURCE}\")\n",
        "\n",
        "        print(f\"Loading data from: {DATA_SOURCE.upper()}\")\n",
        "        return loaders[DATA_SOURCE]()\n",
        "\n",
        "    def _load_github(self):\n",
        "        \"\"\"Load from GitHub releases\"\"\"\n",
        "        response = requests.get(GITHUB_URL, timeout=60)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data_path = self.data_dir / \"nass_data.csv\"\n",
        "        data_path.write_bytes(response.content)\n",
        "\n",
        "        print(f\"✓ Downloaded from GitHub ({response.headers.get('content-length', 'unknown')} bytes)\")\n",
        "        return pd.read_csv(data_path)\n",
        "\n",
        "    def _load_local(self):\n",
        "        \"\"\"Load from local file\"\"\"\n",
        "        search_paths = [\n",
        "            self.base_path / LOCAL_FILENAME,\n",
        "            self.data_dir / LOCAL_FILENAME,\n",
        "            Path.cwd() / LOCAL_FILENAME\n",
        "        ]\n",
        "\n",
        "        for path in search_paths:\n",
        "            if path.exists():\n",
        "                print(f\"✓ Found local file: {path}\")\n",
        "                return pd.read_csv(path)\n",
        "\n",
        "        raise FileNotFoundError(f\"File not found in: {[str(p) for p in search_paths]}\")\n",
        "\n",
        "    def _load_gcs(self):\n",
        "        \"\"\"Load from Google Cloud Storage\"\"\"\n",
        "        from google.cloud import storage\n",
        "\n",
        "        # Smart authentication\n",
        "        if Path(GCS_SERVICE_ACCOUNT_KEY).exists():\n",
        "            client = storage.Client.from_service_account_json(GCS_SERVICE_ACCOUNT_KEY)\n",
        "        else:\n",
        "            client = storage.Client()  # Use default credentials\n",
        "\n",
        "        bucket = client.bucket(GCS_BUCKET)\n",
        "        blob = bucket.blob(GCS_BLOB)\n",
        "\n",
        "        data_path = self.data_dir / \"nass_data.csv\"\n",
        "        blob.download_to_filename(data_path)\n",
        "\n",
        "        print(f\"✓ Downloaded from GCS: {GCS_BUCKET}/{GCS_BLOB}\")\n",
        "        return pd.read_csv(data_path)\n",
        "\n",
        "    def _load_drive(self):\n",
        "        \"\"\"Load from Google Drive (Colab only)\"\"\"\n",
        "        if not env_manager.is_colab:\n",
        "            raise RuntimeError(\"Drive loading only available in Google Colab\")\n",
        "\n",
        "        drive_path = Path(DRIVE_PATH)\n",
        "        if not drive_path.exists():\n",
        "            raise FileNotFoundError(f\"Drive file not found: {DRIVE_PATH}\")\n",
        "\n",
        "        print(f\"✓ Loading from Google Drive: {DRIVE_PATH}\")\n",
        "        return pd.read_csv(drive_path)\n",
        "\n",
        "# Load data\n",
        "try:\n",
        "    loader = DataLoader()\n",
        "    df = loader.load_data()\n",
        "\n",
        "    print(f\"✅ Data loaded successfully!\")\n",
        "    print(f\"   Shape: {df.shape}\")\n",
        "    print(f\"   Memory: {df.memory_usage(deep=True).sum() / 1e6:.1f} MB\")\n",
        "\n",
        "    if VERBOSE_PRINTS:\n",
        "        print(f\"\\nColumns: {list(df.columns)}\")\n",
        "        print(f\"\\nFirst 3 rows:\")\n",
        "        print(df.head(3))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Data loading failed: {e}\")\n",
        "    print(f\"💡 Try changing DATA_SOURCE or check file paths\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxQ5fG5grPrj"
      },
      "source": [
        "Check if data has been loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqUzCuZOrPrj"
      },
      "outputs": [],
      "source": [
        "# Verify data is available before R processing\n",
        "try:\n",
        "    if 'df' not in globals():\n",
        "        print(\"❌ Data not loaded!\")\n",
        "        print(\"💡 Please run the 'Data Loading' section first (cell 12)\")\n",
        "        print(\"   This will create the 'df' variable needed for R analysis\")\n",
        "        raise NameError(\"df variable not found - run data loading first\")\n",
        "\n",
        "    print(f\"✅ Data verified: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
        "    print(\"✅ Ready for R analysis\")\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"❌ {e}\")\n",
        "    print(\"\\n🔄 Quick fix: Run these cells in order:\")\n",
        "    print(\"   1. Configuration (cell 5)\")\n",
        "    print(\"   2. Environment Setup (cell 7)\")\n",
        "    print(\"   3. Data Loading (cell 12)\")\n",
        "    print(\"   4. Then continue with R analysis\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j3Ms-OArPrj"
      },
      "source": [
        "## 5. Complete Data Preprocessing\n",
        "\n",
        "Streamlined preprocessing: remove variables, clean data types, and create new variables - in Python prior to passing to R for efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQzhWzohrPrj"
      },
      "outputs": [],
      "source": [
        "# Data Preprocessing in Python (before R transfer)\n",
        "print(\"Complete preprocessing: removing variables + cleaning data types...\")\n",
        "print(f\"Original shape: {df.shape}\")\n",
        "\n",
        "# ===== 1. REMOVE UNNECESSARY VARIABLES =====\n",
        "print(\"\\n1 Removing unnecessary variables...\")\n",
        "\n",
        "# Smart pattern-based removal in pandas (much faster than R)\n",
        "drop_patterns = [\n",
        "    r'^CPTCCS[2-9]$',      # CPTCCS2-CPTCCS9\n",
        "    r'^CPTCCS[1-3][0-9]$', # CPTCCS10-30\n",
        "    r'^CPT[2-9]$',         # CPT2-CPT9\n",
        "    r'^CPT[1-3][0-9]$',    # CPT10-30\n",
        "    r'^DXCCSR_',           # All DXCCSR columns (500+)\n",
        "]\n",
        "\n",
        "# Find columns to drop using vectorized operations\n",
        "drop_cols = []\n",
        "for pattern in drop_patterns:\n",
        "    matches = df.columns[df.columns.str.match(pattern)].tolist()\n",
        "    drop_cols.extend(matches)\n",
        "\n",
        "# Remove duplicates\n",
        "drop_cols = list(set(drop_cols))\n",
        "\n",
        "print(f\"   📊 Found {len(drop_cols)} columns to drop\")\n",
        "print(f\"   🗑️  Patterns: CPTCCS2-30, CPT2-30, all DXCCSR_*\")\n",
        "\n",
        "# Drop the columns\n",
        "df = df.drop(columns=drop_cols)\n",
        "print(f\"   ✅ Reduced from {df.shape[1] + len(drop_cols)} to {df.shape[1]} columns\")\n",
        "\n",
        "# ===== 2. CLEAN DATA TYPES FOR rpy2 =====\n",
        "print(\"\\n2️⃣ Cleaning data types for rpy2 compatibility...\")\n",
        "\n",
        "# Convert all object columns to strings (prevents mixed-type issues)\n",
        "object_columns = df.select_dtypes(include=['object']).columns\n",
        "if len(object_columns) > 0:\n",
        "    for col in object_columns:\n",
        "        df[col] = df[col].astype(str)\n",
        "    print(f\"   ✅ Converted {len(object_columns)} object columns to strings\")\n",
        "\n",
        "# Handle NaN/inf values consistently\n",
        "df = df.fillna('')  # Replace NaN with empty strings\n",
        "float_cols = df.select_dtypes(include=['float64']).columns\n",
        "if len(float_cols) > 0:\n",
        "    df[float_cols] = df[float_cols].replace([float('inf'), float('-inf')], '')\n",
        "    print(f\"   ✅ Cleaned inf values in {len(float_cols)} float columns\")\n",
        "\n",
        "# ===== 3. CREATE KEY ANALYTICAL VARIABLES IN PANDAS =====\n",
        "print(\"\\n3️⃣ Creating analytical variables...\")\n",
        "\n",
        "# Create WHITE indicator (1=White, 0=Non-White)\n",
        "if 'RACE' in df.columns:\n",
        "    df['WHITE'] = (df['RACE'].astype(str) == '1').astype(int)\n",
        "    print(\"   ✅ Created a race indicator boolean\")\n",
        "\n",
        "# Create age groups\n",
        "if 'AGE' in df.columns:\n",
        "    df['AGE'] = pd.to_numeric(df['AGE'], errors='coerce')  # Ensure numeric\n",
        "    df['AGE_GROUP'] = pd.cut(df['AGE'],\n",
        "                            bins=[0, 18, 30, 45, 65, float('inf')],\n",
        "                            labels=['0-17', '18-29', '30-44', '45-64', '65+'],\n",
        "                            right=False)\n",
        "    df['AGE_GROUP'] = df['AGE_GROUP'].astype(str)  # Convert to string for R\n",
        "    print(\"   ✅ Created AGE_GROUP categories\")\n",
        "\n",
        "# Create income level labels\n",
        "if 'ZIPINC_QRTL' in df.columns:\n",
        "    income_map = {1: 'Q1-Lowest', 2: 'Q2', 3: 'Q3', 4: 'Q4-Highest'}\n",
        "    df['INCOME_LEVEL'] = df['ZIPINC_QRTL'].astype(str).map(lambda x: income_map.get(int(x) if x.isdigit() else 0, 'Unknown'))\n",
        "    print(\"   ✅ Created INCOME_LEVEL labels\")\n",
        "\n",
        "# Ensure key numeric variables are properly typed\n",
        "numeric_vars = ['AGE', 'DISCWT', 'TOTCHG']\n",
        "for var in numeric_vars:\n",
        "    if var in df.columns:\n",
        "        df[var] = pd.to_numeric(df[var], errors='coerce')\n",
        "\n",
        "print(f\"\\n✅ PREPROCESSING COMPLETE!\")\n",
        "print(f\"📊 Final shape: {df.shape}\")\n",
        "print(f\"💾 Ready for R transfer!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1smsFldXrPrj"
      },
      "source": [
        "## 6. Final R Transfer & Processing\n",
        "\n",
        "Transfer the clean data to R and apply any final R-specific formatting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10dad708"
      },
      "outputs": [],
      "source": [
        "%%R -i df -i VERBOSE_PRINTS\n",
        "\n",
        "# Convert to data.table and apply R types\n",
        "NASS <- as.data.table(df)\n",
        "\n",
        "# Factor variables\n",
        "factor_vars <- c(\"ZIPINC_QRTL\", \"PAY1\", \"CPTCCS1\", \"HOSP_LOCATION\",\n",
        "                 \"HOSP_TEACH\", \"HOSP_NASS\", \"RACE\", \"AGE_GROUP\", \"INCOME_LEVEL\")\n",
        "existing_factors <- factor_vars[factor_vars %in% names(NASS)]\n",
        "NASS[, (existing_factors) := lapply(.SD, as.factor), .SDcols = existing_factors]\n",
        "\n",
        "# Boolean variables\n",
        "if(\"FEMALE\" %in% names(NASS)) NASS[, FEMALE := as.logical(as.numeric(FEMALE))]\n",
        "if(\"WHITE\" %in% names(NASS)) NASS[, WHITE := as.logical(as.numeric(WHITE))]\n",
        "\n",
        "# Compact output\n",
        "cat(\"✅ R Complete:\", nrow(NASS), \"rows,\", ncol(NASS), \"cols,\",\n",
        "    round(object.size(NASS)/1024^2, 1), \"MB\\n\")\n",
        "cat(\"Converted\", length(existing_factors), \"factors + 2 booleans\\n\")\n",
        "\n",
        "if(VERBOSE_PRINTS) {\n",
        "  cat(\"\\nColumns:\\n\")\n",
        "  print(colnames(NASS))\n",
        "}\n",
        "\n",
        "cat(\"Ready for analysis!\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis"
      ],
      "metadata": {
        "id": "QbLoLZoAsHKG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43230586"
      },
      "source": [
        "---\n",
        "## 1. R Analysis - Income Quartile vs Procedure\n",
        "\n",
        "Visualize income distribution within the most common procedures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee3556ca"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "\n",
        "# ===== GENERATE SUMMARY STATISTICS =====\n",
        "cat(\"\\n=== DATASET ANALYSIS SUMMARY ===\\n\")\n",
        "cat(\"📊 Data shape:\", nrow(NASS), \"rows x\", ncol(NASS), \"columns\\n\")\n",
        "flush.console()\n",
        "\n",
        "# Create top 10 procedures table\n",
        "if(\"CPTCCS1\" %in% names(NASS)) {\n",
        "  top10 <- NASS[, .N, by = CPTCCS1][order(-N)][1:10]\n",
        "  cat(\"\\n🔝 Top 10 procedures (CPTCCS1):\\n\")\n",
        "  print(top10)\n",
        "} else {\n",
        "  cat(\"⚠️ CPTCCS1 variable not found\\n\")\n",
        "}\n",
        "\n",
        "# Basic demographic summary\n",
        "if(\"WHITE\" %in% names(NASS)) {\n",
        "  white_pct <- round(mean(NASS$WHITE, na.rm = TRUE) * 100, 1)\n",
        "  cat(\"\\n👥 Race: White =\", white_pct, \"%, Non-White =\", 100 - white_pct, \"%\\n\")\n",
        "}\n",
        "\n",
        "if(\"AGE\" %in% names(NASS)) {\n",
        "  age_summary <- summary(NASS$AGE)\n",
        "  cat(\"\\n📅 Age distribution:\\n\")\n",
        "  print(age_summary)\n",
        "}\n",
        "\n",
        "if(\"INCOME_LEVEL\" %in% names(NASS)) {\n",
        "  income_dist <- NASS[, .N, by = INCOME_LEVEL][order(INCOME_LEVEL)]\n",
        "  cat(\"\\n💰 Income quartiles:\\n\")\n",
        "  print(income_dist)\n",
        "}\n",
        "\n",
        "cat(\"\\n✅ Summary complete - ready for analysis!\\n\")\n",
        "flush.console()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20a6a8a8"
      },
      "source": [
        "---\n",
        "## 10. Census API Setup\n",
        "\n",
        "Set up environment variable for Census API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d0c69b9"
      },
      "outputs": [],
      "source": [
        "import getpass, os, json, textwrap\n",
        "os.environ[\"CENSUS_API_KEY\"] = getpass.getpass(\"Enter your Census API key (will not echo):\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c39ad38"
      },
      "source": [
        "---\n",
        "## 11. R | Set Census Key & Pull 2020 DHC Totals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "240fd569"
      },
      "outputs": [],
      "source": [
        "%%R -i states_in_nass=character() -i VERBOSE_PRINTS\n",
        "# If you've already installed the key once, this is a no-op\n",
        "tidycensus::census_api_key(Sys.getenv(\"CENSUS_API_KEY\"), overwrite = FALSE, install = FALSE)\n",
        "\n",
        "get_vars <- function(base) sprintf(\"%s_%03dN\", base, 1:49)\n",
        "\n",
        "vars_total <- get_vars(\"P12\")\n",
        "vars_white <- get_vars(\"P12I\")\n",
        "\n",
        "pull_state_totals <- function(vars){\n",
        "  get_decennial(geography = \"state\",\n",
        "                variables = vars,\n",
        "                year = 2020, sumfile = \"dhc\") |>\n",
        "  group_by(NAME) |> summarise(total = sum(value))\n",
        "}\n",
        "\n",
        "total_pop  <- pull_state_totals(vars_total)\n",
        "white_pop  <- pull_state_totals(vars_white)\n",
        "\n",
        "census_prop <- merge(total_pop, white_pop, by = \"NAME\",\n",
        "                     suffixes = c(\"_all\",\"_white\"))\n",
        "census_prop[, prop_white := total_white / total_all]\n",
        "\n",
        "if (VERBOSE_PRINTS) head(census_prop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6239ac5"
      },
      "source": [
        "---\n",
        "## 12. R | Weighted vs Unweighted Proportion Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9a539f9"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "library(survey)\n",
        "\n",
        "# Survey design using provided discharge weight\n",
        "des <- svydesign(ids = ~1, weights = ~DISCWT, data = NASS)\n",
        "\n",
        "unweighted_hat <- mean(NASS$WHITE)\n",
        "weighted_hat   <- svymean(~WHITE, des)[1]\n",
        "\n",
        "us_prop <- weighted.mean(census_prop$prop_white,\n",
        "                         w = census_prop$total_all)\n",
        "\n",
        "cat(sprintf(\"Unweighted NASS white %%: %.3f\\n\", unweighted_hat))\n",
        "cat(sprintf(\"Weighted   NASS white %%: %.3f\\n\", weighted_hat))\n",
        "cat(sprintf(\"2020 Census (all NASS states) white %%: %.3f\\n\", us_prop))\n",
        "\n",
        "svytest <- svyciprop(~WHITE, des,\n",
        "                     method = \"likelihood\", level = 0.95)\n",
        "print(svytest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d30e6ec7"
      },
      "source": [
        "---\n",
        "## 13. R | Age-by-sex plot vs Census (adapted from `agesociodiv.r`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfb4756f"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "age_breaks <- c(-Inf,4,9,14,17,19,20,21,24,29,34,39,44,49,54,59,61,64,\n",
        "                66,69,74,79,84,Inf)\n",
        "age_labels <- c(\"U5\",\"5-9\",\"10-14\",\"15-17\",\"18-19\",\"20\",\"21\",\n",
        "                \"22-24\",\"25-29\",\"30-34\",\"35-39\",\"40-44\",\"45-49\",\n",
        "                \"50-54\",\"55-59\",\"60-61\",\"62-64\",\"65-66\",\"67-69\",\n",
        "                \"70-74\",\"75-79\",\"80-84\",\"85+\")\n",
        "\n",
        "NASS[, AGE_GROUP := cut(AGE, breaks = age_breaks,\n",
        "                        labels = age_labels, right = TRUE)]\n",
        "\n",
        "plot_df <- NASS[, .(white = sum(WHITE),\n",
        "                    n     = .N),\n",
        "                by = .(SEX = factor(FEMALE, labels=c(\"Male\",\"Female\")),\n",
        "                       AGE_GROUP)]\n",
        "plot_df[, prop := white/n]\n",
        "\n",
        "# Create plot with intelligent fallback\n",
        "if(require(\"ggplot2\", quietly = TRUE) && require(\"scales\", quietly = TRUE)) {\n",
        "  # Primary: ggplot2 with scales\n",
        "  gg_gender <- ggplot(plot_df, aes(x = AGE_GROUP, y = prop,\n",
        "                                   group = SEX, color = SEX)) +\n",
        "    geom_line(linewidth=1) +\n",
        "    geom_point() +\n",
        "    scale_y_continuous(labels = scales::percent) +\n",
        "    labs(y = \"% White (NASS, simulated)\", x = \"Age-group\",\n",
        "         title = \"Crude white proportion by age & sex\") +\n",
        "    theme_minimal() +\n",
        "    theme(axis.text.x = element_text(angle=45, hjust=1))\n",
        "  print(gg_gender)\n",
        "  cat(\"📊 ggplot2 plot generated\\n\")\n",
        "\n",
        "} else if(require(\"ggplot2\", quietly = TRUE)) {\n",
        "  # Fallback: ggplot2 without scales\n",
        "  gg_gender <- ggplot(plot_df, aes(x = AGE_GROUP, y = prop,\n",
        "                                   group = SEX, color = SEX)) +\n",
        "    geom_line(linewidth=1) +\n",
        "    geom_point() +\n",
        "    labs(y = \"Proportion White (NASS, simulated)\", x = \"Age-group\",\n",
        "         title = \"Crude white proportion by age & sex\") +\n",
        "    theme_minimal() +\n",
        "    theme(axis.text.x = element_text(angle=45, hjust=1))\n",
        "  print(gg_gender)\n",
        "  cat(\"📊 ggplot2 plot generated (without percentage scaling)\\n\")\n",
        "\n",
        "} else {\n",
        "  # Base R fallback\n",
        "  cat(\"📊 Using base R plotting (ggplot2 not available)\\n\")\n",
        "\n",
        "  male_data <- plot_df[SEX == \"Male\"]\n",
        "  female_data <- plot_df[SEX == \"Female\"]\n",
        "\n",
        "  plot(1:nrow(male_data), male_data$prop, type = \"b\", col = \"blue\",\n",
        "       xlab = \"Age Group\", ylab = \"Proportion White\",\n",
        "       main = \"Crude white proportion by age & sex\",\n",
        "       ylim = c(0, 1), xaxt = \"n\")\n",
        "  lines(1:nrow(female_data), female_data$prop, type = \"b\", col = \"red\")\n",
        "  axis(1, at = 1:nrow(male_data), labels = male_data$AGE_GROUP, las = 2)\n",
        "  legend(\"topright\", legend = c(\"Male\", \"Female\"), col = c(\"blue\", \"red\"), lty = 1)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "add395e2"
      },
      "source": [
        "---\n",
        "## 14. R | Multilevel logistic models (hospital nested, 3 tiers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8574bf3a"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "features <- NASS[, .(WHITE,\n",
        "                     FEMALE,\n",
        "                     ZIPINC_QRTL,\n",
        "                     PAY1,\n",
        "                     CPTCCS1,\n",
        "                     HOSP_LOCATION,\n",
        "                     HOSP_TEACH,\n",
        "                     HOSP_NASS)]\n",
        "\n",
        "features[, c(names(features)) := lapply(.SD, as.factor)]\n",
        "\n",
        "formulas <- list(\n",
        "  m1 = WHITE ~ FEMALE + (1|HOSP_NASS),\n",
        "  m2 = WHITE ~ FEMALE + ZIPINC_QRTL + (1|HOSP_NASS),\n",
        "  m3 = WHITE ~ FEMALE + ZIPINC_QRTL + PAY1 + CPTCCS1 +\n",
        "                    HOSP_LOCATION + HOSP_TEACH + (1|HOSP_NASS)\n",
        ")\n",
        "\n",
        "fit <- lapply(formulas, glmer, family = binomial, data = features,\n",
        "              control = glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e4)))\n",
        "\n",
        "sapply(fit, function(m) broom::tidy(m, effects = \"fixed\")[1:5,])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa999863"
      },
      "source": [
        "---\n",
        "## 15. R | Compare AUC across the three models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9781417"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "library(pROC)\n",
        "auc_vals <- sapply(fit, function(m){\n",
        "  preds <- predict(m, type=\"response\")\n",
        "  roc(features$WHITE, preds)$auc\n",
        "})\n",
        "knitr::kable(data.frame(model = names(auc_vals), AUC = auc_vals),\n",
        "             caption = \"AUC (in-sample, simulated data)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Poster Generation"
      ],
      "metadata": {
        "id": "8L7G5GKSsSX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Poster genreated here\n"
      ],
      "metadata": {
        "id": "EOTnhQoRse_k"
      }
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}