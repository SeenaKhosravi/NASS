{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open on GitHub](https://img.shields.io/badge/GitHub-View%20Source-181717?style=for-the-badge&logo=github)](https://github.com/SeenaKhosravi/NASS/blob/main/Analysis_NASS.ipynb)\n",
    "[![Open In Colab](https://img.shields.io/badge/Colab-Open%20Notebook-F9AB00?style=for-the-badge&logo=google-colab)](https://colab.research.google.com/github/SeenaKhosravi/NASS/blob/main/Analysis_NASS.ipynb)\n",
    "[![Launch on GCE](https://img.shields.io/badge/Google%20Cloud-Launch%20Instance-4285F4?style=for-the-badge&logo=google-cloud)](https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https://github.com/SeenaKhosravi/NASS&cloudshell_tutorial=deploy/gce-tutorial.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbe6c3e6"
   },
   "source": [
    "# Socioeconomic and Demographic Drivers of Ambulatory Surgery Usage\n",
    "### HCUP NASS 2020 - Reproducible Pipeline (Python + R)\n",
    "\n",
    "**Script Author:** Seena Khosravi, MD  \n",
    "**LLMs Utilized:** Claude Sonnet 4, Opus 4.1; ChatGPT 4o, o4; Deepseek 3.1; Gemini 2.5 Pro; Grok 5  \n",
    "**Last Updated:** September 20, 2025  \n",
    "\n",
    "\n",
    "**Data Source:**  \n",
    "Department of Health & Human Services (HHS)  \n",
    "Agency for Healthcare Research and Quality (AHRQ)  \n",
    "Healthcare Cost and Utilization Project (HCUP)  \n",
    "National Ambulatory Surgical Sample (NASS) 2020\n",
    "\n",
    " **[HCUP NASS 2020 Introduction](https://hcup-us.ahrq.gov/db/nation/nass/NASS_Introduction_2020.jsp)** - Official AHRQ documentation\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOmuZCsusrTj"
   },
   "source": [
    "\n",
    "## Overview\n",
    "This notebook provides a reproducible analysis pipeline for examining socioeconomic and demographic factors influencing ambulatory surgery usage patterns. The analysis combines Python for data processing and R for data analysis and visualization.\n",
    "\n",
    "### Data Usage Agreement\n",
    "**DUA Compliant Online Implementation** - This notebook uses a simulated, artificial, smaller dataset with identical structure to the file created by [Raw_NASS_Processing.R](https://github.com/SeenaKhosravi/NASS/blob/a7764ce80be8a82fc449831821c27d957176c410/Raw%20NASS%20%20Processing.R). The simulated dataset production methodology is found in [Generate_Simulated_NASS.R](https://github.com/SeenaKhosravi/NASS/blob/161bf2b5c149da9654c0e887655b361fa2176db0/Generate_Simulated_NASS.R). If DUA signed and data purchased from HCUP, this notebook can run on full dataset in cloud storage (with provided passcode).\n",
    "\n",
    "[Please see the DUA Agreement here.](https://hcup-us.ahrq.gov/team/NationwideDUA.jsp)\n",
    "\n",
    "### Key Features\n",
    "- **Multi-platform:** Works on Jupyter implementations via local environments, server, cloud VM instance, or platform as a service\n",
    "- **Flexible Data Storage:** GitHub (simulated, static, open access), Google Drive, Google Cloud Storage, or local file\n",
    "- **Reproducible:** All dependencies and environment setup included; Automated, Click-through VM set-up\n",
    "- **Scalable:** Handles both simulated (0.2GB, 139k rows) and full dataset (12GB, 7.8M rows) with scalable cloud options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "019b6fe9"
   },
   "source": [
    "---\n",
    "\n",
    "## Design Notes\n",
    "\n",
    "### Architecture\n",
    "- **Python primary, w/ R run via rpy2 python extension**\n",
    "- **Python cells** handle \"plumbing\" (file I/O, environment setup, rpy2 configuration, data previews)\n",
    "- **R cells** (prefixed by `%%R`) perform statistical analysis: survey weights, Census lookups, multilevel models, plots, classifiers, etc.\n",
    "\n",
    "### Data Sources\n",
    "- **Default:** Simulated dataset (0.2GB) from GitHub releases\n",
    "- **Local:** Switch to locally stored files via configuration\n",
    "- **Drive:** Google Drive (Only available in Colab)\n",
    "- **Cloud:** Google Cloud Storage support for large datasets\n",
    "\n",
    "### Environment Support\n",
    "- **Local:** JupyterLab w/ Python 3.8+ kernel (requires R 4.0+, 4.4+ preferred)\n",
    "- **Jupyter Server:** May require configuration depending on implementation (conda recommended)\n",
    "- **Google Colab:** Pro recommended for full dataset, high-RAM runtime suggested\n",
    "- **Virtual Machine Instance:** Automated GCE deployment via scripts, pre-configured R setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvQPKX8GrfZr"
   },
   "source": [
    "---\n",
    "\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUEIOQlXrPrd"
   },
   "source": [
    "\n",
    "## 1. Configuration\n",
    "\n",
    "**SETTINGS** \n",
    "\n",
    "Configure all settings here prior to run - data sources, debugging options, and file paths. Defaults to simulated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUU1wwMKrPre"
   },
   "outputs": [],
   "source": [
    "# ==================== CONFIGURATION ====================\n",
    "# Data Source Options\n",
    "DATA_SOURCE = \"github\"      # Options: \"github\", \"local\", \"gcs\", \"drive\"\n",
    "VERBOSE_PRINTS = True    # False ‚Üí suppress debug output\n",
    "\n",
    "# GitHub source (default - simulated data)\n",
    "GITHUB_URL = \"https://github.com/SeenaKhosravi/NASS/releases/download/v1.0.0/nass_2020_simulated.csv\"\n",
    "\n",
    "# Local file options\n",
    "LOCAL_FILENAME = \"nass_2020_local.csv\"\n",
    "\n",
    "# Google Cloud Storage options\n",
    "GCS_BUCKET = \"nass_2020\"\n",
    "GCS_BLOB = \"nass_2020_all.csv\"\n",
    "\n",
    "# Google Drive options (for Colab)\n",
    "DRIVE_PATH = \"/content/drive/MyDrive/NASS/nass_2020_full.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === CONFIGURATION LOADING ===\n",
    "import getpass\n",
    "import hashlib\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "class SecurePasswordManager:\n",
    "    \"\"\"Secure password handling with memory cleanup.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._password_hash = None\n",
    "    \n",
    "    def get_password(self, prompt: str) -> Optional[str]:\n",
    "        \"\"\"Get password with enhanced security.\"\"\"\n",
    "        try:\n",
    "            password = getpass.getpass(prompt)\n",
    "            \n",
    "            if not password.strip():\n",
    "                return None\n",
    "            \n",
    "            # Store only hash for verification\n",
    "            self._password_hash = hashlib.sha256(password.encode()).hexdigest()\n",
    "            \n",
    "            return password\n",
    "            \n",
    "        except (KeyboardInterrupt, EOFError):\n",
    "            print(\"\\nPassword entry cancelled\")\n",
    "            return None\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Attempt to clear sensitive data.\"\"\"\n",
    "        self._password_hash = None\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "def configure_access():\n",
    "    \"\"\"Configure secure data source and Census access.\"\"\"\n",
    "    print(\" NASS 2020 Research Analysis Pipeline\")\n",
    "    print(\" Configuration Status \")\n",
    "    print()\n",
    "    \n",
    "    password_manager = SecurePasswordManager()\n",
    "    gcs_password = None\n",
    "    census_password = None\n",
    "    \n",
    "    # Only show GCS prompts if GCS is selected\n",
    "    if DATA_SOURCE == \"gcs\":\n",
    "        print(\"üîí Encrypted Dataset Access\")\n",
    "        gcs_password = password_manager.get_password(\n",
    "            \"   Enter GCS access key: \"\n",
    "        )\n",
    "        \n",
    "        if not gcs_password:\n",
    "            print(\"   ‚Üí Switching to public dataset\")\n",
    "        else:\n",
    "            print(\"   ‚úì Full dataset access enabled\")\n",
    "    \n",
    "    # Streamlined Census API setup\n",
    "    print(\"üåê Census API Integration\")\n",
    "    census_password = password_manager.get_password(\n",
    "        \"   Enter Census API key: \"\n",
    "    )\n",
    "    \n",
    "    if census_password:\n",
    "        print(\"   ‚úì Encrypted Census integration ready\")\n",
    "    else:\n",
    "        print(\"   ‚Üí Manual API key entry required later\")\n",
    "    \n",
    "    # Clear password manager\n",
    "    password_manager.clear_memory()\n",
    "    \n",
    "    return gcs_password, census_password\n",
    "\n",
    "# Configure access\n",
    "GCS_PASSWORD, CENSUS_PASSWORD = configure_access()\n",
    "\n",
    "# Set up encrypted URLs if passwords provided\n",
    "if GCS_PASSWORD and DATA_SOURCE == \"gcs\":\n",
    "    GCS_ENCRYPTED_KEY_URL = \"https://github.com/SeenaKhosravi/NASS/releases/download/v1.0.0.0/gcs_key.enc\"\n",
    "elif DATA_SOURCE == \"gcs\":\n",
    "    # Switch to simulated data if no GCS password\n",
    "    DATA_SOURCE = \"github\"\n",
    "\n",
    "if CENSUS_PASSWORD:\n",
    "    CENSUS_ENCRYPTED_KEY_URL = \"https://github.com/SeenaKhosravi/NASS/releases/download/v1.0.0.0/census_key.enc\"\n",
    "\n",
    "# Clean final status\n",
    "print()\n",
    "print(\"=\" * 50)\n",
    "data_status = {\n",
    "    \"github\": \"üìä Public Dataset (139K records)\",\n",
    "    \"local\": \"üíæ Local Dataset\", \n",
    "    \"gcs\": \"‚òÅÔ∏è  Full Dataset (7.8M records)\",\n",
    "    \"drive\": \"üìÇ Google Drive Dataset\"\n",
    "}\n",
    "\n",
    "census_status = \"üîê Encrypted\" if CENSUS_PASSWORD else \"üîë Manual Entry\"\n",
    "\n",
    "print(f\"‚úÖ Ready: {data_status.get(DATA_SOURCE, DATA_SOURCE)} | Census: {census_status}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WJ8_5sRrPre"
   },
   "source": [
    "---\n",
    "## 2. Python Environment Setup \n",
    "\n",
    "Detect environment and install Python packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9RcJjrWrPrf"
   },
   "outputs": [],
   "source": [
    "# === PYTHON ENVIRONMENT SETUP ===\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "class EnvironmentManager:\n",
    "    def __init__(self):\n",
    "        self.detect_environment()\n",
    "        self.setup_packages()\n",
    "\n",
    "    def detect_environment(self):\n",
    "        \"\"\"Detect runtime environment\"\"\"\n",
    "        self.is_colab = 'COLAB_GPU' in os.environ or 'google.colab' in sys.modules\n",
    "        self.is_vertex = 'DL_ANACONDA_HOME' in os.environ\n",
    "\n",
    "        if self.is_colab:\n",
    "            self.env_type = \"Google Colab\"\n",
    "        elif self.is_vertex:\n",
    "            self.env_type = \"Vertex AI\"\n",
    "        else:\n",
    "            self.env_type = \"Local/Jupyter\"\n",
    "\n",
    "        print(f\"Environment detected: {self.env_type}\")\n",
    "\n",
    "    def check_conda_available(self):\n",
    "        \"\"\"Check if conda is available\"\"\"\n",
    "        try:\n",
    "            subprocess.check_call(['conda', '--version'],\n",
    "                                stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "            return True\n",
    "        except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "            return False\n",
    "\n",
    "    def install_package(self, package, conda_name=None):\n",
    "        \"\"\"Smart package installation with fallback\"\"\"\n",
    "        try:\n",
    "            __import__(package)\n",
    "            return True\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "\n",
    "            # Try conda first if available and not in Colab\n",
    "            if conda_name and not self.is_colab and self.check_conda_available():\n",
    "                try:\n",
    "                    subprocess.check_call(['conda', 'install', '-y', conda_name],\n",
    "                                        stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "                    return True\n",
    "                except subprocess.CalledProcessError:\n",
    "                    print(f\"  Conda install failed for {conda_name}, trying pip...\")\n",
    "\n",
    "            # Fallback to pip\n",
    "            try:\n",
    "                subprocess.check_call([sys.executable, '-m', 'pip', 'install', package],\n",
    "                                    stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "                return True\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"  Pip install failed for {package}: {e}\")\n",
    "                return False\n",
    "\n",
    "    def setup_packages(self):\n",
    "        packages = {\n",
    "            'pandas': 'pandas',\n",
    "            'requests': 'requests',\n",
    "            'rpy2': 'rpy2',\n",
    "            'google.cloud.storage': 'google-cloud-storage',\n",
    "            'pycryptodome': 'pycryptodome'  # For AES decryption\n",
    "        }\n",
    "\n",
    "        print(\"Installing and checking packages...\")\n",
    "        failed = []\n",
    "\n",
    "        for pkg, install_name in packages.items():\n",
    "            if not self.install_package(pkg, install_name):\n",
    "                failed.append(pkg)\n",
    "\n",
    "        # Store failed packages globally for recovery\n",
    "        globals()['failed_packages'] = failed\n",
    "\n",
    "        if failed:\n",
    "            print(f\"Warning: Failed to install: {', '.join(failed)}\")\n",
    "            print(\"Some features may not work\")\n",
    "\n",
    "            # Provide specific guidance for rpy2\n",
    "            if 'rpy2' in failed:\n",
    "                print(\"\\nFor rpy2 installation issues:\")\n",
    "                if self.is_vertex:\n",
    "                    print(\"   - Vertex AI: R may not be installed by default\")\n",
    "                    print(\"   - Run the next cell for automated R setup\")\n",
    "                else:\n",
    "                    print(\"   - On Windows: May need Visual Studio Build Tools\")\n",
    "                    print(\"   - Try: conda install -c conda-forge rpy2\")\n",
    "                    print(\"   - Or: pip install rpy2 (requires R to be installed)\")\n",
    "        else:\n",
    "            print(\"All packages ready\")\n",
    "\n",
    "        # Mount Google Drive if needed (check if DATA_SOURCE exists)\n",
    "        try:\n",
    "            if globals().get('DATA_SOURCE') == \"drive\" and self.is_colab:\n",
    "                self.mount_drive()\n",
    "        except NameError:\n",
    "            pass  # DATA_SOURCE not defined yet\n",
    "\n",
    "    def mount_drive(self):\n",
    "        \"\"\"Mount Google Drive in Colab\"\"\"\n",
    "        try:\n",
    "            from google.colab import drive\n",
    "            drive.mount('/content/drive')\n",
    "            print(\"Google Drive mounted successfully\")\n",
    "        except:\n",
    "            print(\"Error: Failed to mount Google Drive\")\n",
    "\n",
    "# Initialize environment\n",
    "env_manager = EnvironmentManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mm4dTQb1rPrh"
   },
   "source": [
    "---\n",
    "## 3. R Environment Setup\n",
    "\n",
    "Load R integration and install R packages efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wpUbizDrPrh"
   },
   "outputs": [],
   "source": [
    "# === R ENVIRONMENT SETUP ===\n",
    "\n",
    "# Load rpy2 extension for R integration\n",
    "try:\n",
    "    %load_ext rpy2.ipython\n",
    "    print(\"R integration loaded successfully\")\n",
    "    globals()['R_AVAILABLE'] = True\n",
    "except Exception as e:\n",
    "    print(f\"Error: Failed to load R integration: {e}\")\n",
    "\n",
    "    # Windows-specific troubleshooting\n",
    "    if \"R.dll\" in str(e) or \"error 0x7e\" in str(e):\n",
    "        print(\"\\nWindows R.dll loading issue detected:\")\n",
    "        print(\"   This is a common Windows + rpy2 compatibility issue\")\n",
    "        print(\"   Solutions:\")\n",
    "        print(\"   1. Restart Python kernel and try again\")\n",
    "        print(\"   2. Check R version compatibility with rpy2\")\n",
    "        print(\"   3. Try reinstalling R and rpy2\")\n",
    "        print(\"   4. Use Python-only analysis (fallback available)\")\n",
    "        globals()['R_AVAILABLE'] = False\n",
    "    else:\n",
    "        print(\"Install rpy2: pip install rpy2\")\n",
    "        globals()['R_AVAILABLE'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5iP0V_1rPri"
   },
   "source": [
    "Install essential R packages and attempt installation of optional packages.\n",
    "\n",
    "**Note:** Other packages needed for specific analysis (advanced modeling packages) will be installed and called as needed later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGFfQQC5rPri"
   },
   "outputs": [],
   "source": [
    "%%R -i VERBOSE_PRINTS # === R PACKAGE LOADING ===\n",
    "\n",
    "# Environment-aware R package setup for Local, Colab, and GCE/Linux VMs\n",
    "\n",
    "# Detect environment\n",
    "is_colab <- Sys.getenv(\"COLAB_GPU\") != \"\"\n",
    "is_gce <- file.exists(\"/opt/nass\") || file.exists(\"/etc/apt\")  # GCE/Linux VM indicators\n",
    "\n",
    "if(is_colab) {\n",
    "  cat(\"Google Colab detected\\n\")\n",
    "} else if(is_gce) {\n",
    "  cat(\"GCE/Linux VM detected\\n\")\n",
    "} else {\n",
    "  cat(\"Local environment detected\\n\")\n",
    "}\n",
    "\n",
    "# Essential packages for all environments\n",
    "essential_packages <- c(\n",
    "  \"data.table\",    # Fast data manipulation\n",
    "  \"ggplot2\",       # Plotting\n",
    "  \"scales\"         # For ggplot2 percentage scales\n",
    ")\n",
    "\n",
    "optional_packages <- c(\n",
    "  \"survey\"        # Survey statistics\n",
    ")\n",
    "\n",
    "# Fast installation settings\n",
    "repos <- \"https://cloud.r-project.org\"\n",
    "options(repos = repos)\n",
    "Sys.setenv(MAKEFLAGS = paste0(\"-j\", parallel::detectCores()))\n",
    "\n",
    "# Package check and load functions\n",
    "pkg_available <- function(pkg) {\n",
    "  tryCatch({\n",
    "    find.package(pkg, quiet = TRUE)\n",
    "    TRUE\n",
    "  }, error = function(e) FALSE)\n",
    "}\n",
    "\n",
    "load_pkg <- function(pkg) {\n",
    "  tryCatch({\n",
    "    suppressMessages(library(pkg, character.only = TRUE, quietly = TRUE))\n",
    "    TRUE\n",
    "  }, error = function(e) FALSE)\n",
    "}\n",
    "\n",
    "# Install missing essential packages\n",
    "missing_essential <- essential_packages[!sapply(essential_packages, pkg_available)]\n",
    "\n",
    "if(length(missing_essential) > 0) {\n",
    "  cat(\"Installing essential packages:\", paste(missing_essential, collapse = \", \"), \"\\n\")\n",
    "\n",
    "  tryCatch({\n",
    "    install.packages(missing_essential,\n",
    "                    repos = repos,\n",
    "                    type = getOption(\"pkgType\"),\n",
    "                    dependencies = FALSE,\n",
    "                    quiet = !VERBOSE_PRINTS,\n",
    "                    Ncpus = parallel::detectCores())\n",
    "  }, error = function(e) {\n",
    "    cat(\"Binary install failed, trying source...\\n\")\n",
    "    install.packages(missing_essential,\n",
    "                    repos = repos,\n",
    "                    type = \"source\",\n",
    "                    dependencies = FALSE,\n",
    "                    quiet = !VERBOSE_PRINTS)\n",
    "  })\n",
    "}\n",
    "\n",
    "# Load essential packages\n",
    "essential_loaded <- sapply(essential_packages, load_pkg)\n",
    "essential_success <- sum(essential_loaded)\n",
    "\n",
    "cat(\"Essential packages loaded:\", essential_success, \"/\", length(essential_packages), \"\\n\")\n",
    "\n",
    "# Quick install optional packages (30s timeout)\n",
    "missing_optional <- optional_packages[!sapply(optional_packages, pkg_available)]\n",
    "\n",
    "if(length(missing_optional) > 0) {\n",
    "  cat(\"Installing optional packages...\\n\")\n",
    "\n",
    "  for(pkg in missing_optional) {\n",
    "    tryCatch({\n",
    "      setTimeLimit(cpu = 30, elapsed = 30, transient = TRUE)\n",
    "      install.packages(pkg, repos = repos,\n",
    "                      type = getOption(\"pkgType\"),\n",
    "                      dependencies = FALSE,\n",
    "                      quiet = TRUE)\n",
    "      cat(\"Installed:\", pkg, \"\\n\")\n",
    "    }, error = function(e) {\n",
    "      cat(\"Skipped (timeout):\", pkg, \"\\n\")\n",
    "    })\n",
    "\n",
    "    setTimeLimit(cpu = Inf, elapsed = Inf, transient = FALSE)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Load optional packages\n",
    "optional_loaded <- sapply(optional_packages, load_pkg)\n",
    "optional_success <- sum(optional_loaded)\n",
    "\n",
    "cat(\"Optional packages loaded:\", optional_success, \"/\", length(optional_packages), \"\\n\")\n",
    "\n",
    "# Check core functionality\n",
    "has_datatable <- require(\"data.table\", quietly = TRUE)\n",
    "has_ggplot <- require(\"ggplot2\", quietly = TRUE)\n",
    "\n",
    "# Enhanced GCE/Linux setup if packages are missing\n",
    "if(is_gce && (!has_datatable || !has_ggplot)) {\n",
    "  cat(\"\\nGCE/Linux detected - attempting enhanced installation...\\n\")\n",
    "\n",
    "  # System dependencies for Linux VMs\n",
    "  if(Sys.which(\"apt-get\") != \"\") {\n",
    "    cat(\"Installing system dependencies...\\n\")\n",
    "    system_deps <- c(\n",
    "      \"apt-get update -qq\",\n",
    "      \"apt-get install -y libfontconfig1-dev libcairo2-dev\",\n",
    "      \"apt-get install -y libxml2-dev libcurl4-openssl-dev libssl-dev\",\n",
    "      \"apt-get install -y libharfbuzz-dev libfribidi-dev\",\n",
    "      \"apt-get install -y libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev\"\n",
    "    )\n",
    "    \n",
    "    for(cmd in system_deps) {\n",
    "      system(paste(\"sudo\", cmd), ignore.stdout = TRUE, ignore.stderr = TRUE)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  # Retry failed packages with multiple repositories\n",
    "  failed_packages <- c()\n",
    "  if(!has_datatable) failed_packages <- c(failed_packages, \"data.table\")\n",
    "  if(!has_ggplot) failed_packages <- c(failed_packages, \"ggplot2\", \"scales\")\n",
    "\n",
    "  repos_gce <- c(\"https://cran.rstudio.com/\", \"https://cloud.r-project.org\")\n",
    "\n",
    "  for(pkg in failed_packages) {\n",
    "    cat(\"Installing\", pkg, \"...\")\n",
    "    installed <- FALSE\n",
    "\n",
    "    for(repo in repos_gce) {\n",
    "      tryCatch({\n",
    "        install.packages(pkg, repos = repo, dependencies = TRUE, quiet = TRUE)\n",
    "        if(require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "          cat(\" Success\\n\")\n",
    "          installed <- TRUE\n",
    "          break\n",
    "        }\n",
    "      }, error = function(e) NULL)\n",
    "    }\n",
    "\n",
    "    if(!installed) cat(\" FAILED\\n\")\n",
    "  }\n",
    "\n",
    "  # Re-check after enhanced installation\n",
    "  has_datatable <- require(\"data.table\", quietly = TRUE)\n",
    "  has_ggplot <- require(\"ggplot2\", quietly = TRUE)\n",
    "\n",
    "  cat(\"After enhanced installation: data.table =\", has_datatable, \"| ggplot2 =\", has_ggplot, \"\\n\")\n",
    "}\n",
    "\n",
    "# Final status check\n",
    "if(has_datatable && has_ggplot) {\n",
    "  cat(\"Core environment ready! (data.table + ggplot2)\\n\")\n",
    "  setDTthreads(0)  # Use all cores\n",
    "\n",
    "} else if(has_datatable) {\n",
    "  cat(\"Warning: Partial setup - data.table ready, plotting may be limited\\n\")\n",
    "  setDTthreads(0)\n",
    "\n",
    "} else {\n",
    "  cat(\"Error: Critical failure - data.table not available\\n\")\n",
    "  stop(\"Cannot proceed without data.table\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAtjPad_rPri"
   },
   "source": [
    "Verify R setup is complete and ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMh-C5gvrPrj"
   },
   "outputs": [],
   "source": [
    "%%R # === R ENVIRONMENT VERIFICATION ===\n",
    "\n",
    "# Quick verification and setup\n",
    "cat(\"Verifying R environment...\\n\")\n",
    "\n",
    "# Test core functionality\n",
    "tryCatch({\n",
    "  # Test data.table (essential)\n",
    "  dt_test <- data.table(x = 1:3, y = letters[1:3])\n",
    "  cat(\"data.table ready\\n\")\n",
    "\n",
    "  # Test ggplot2 (optional)\n",
    "  if(require(\"ggplot2\", quietly = TRUE)) {\n",
    "    cat(\"ggplot2 ready\\n\")\n",
    "  } else {\n",
    "    cat(\"Warning: ggplot2 not available (plots disabled)\\n\")\n",
    "  }\n",
    "\n",
    "  # Set up data.table options for performance\n",
    "  setDTthreads(0)  # Use all cores\n",
    "\n",
    "  cat(\"R environment optimized and ready!\\n\")\n",
    "\n",
    "}, error = function(e) {\n",
    "  cat(\"Error: R environment verification failed:\", e$message, \"\\n\")\n",
    "  stop(\"R setup incomplete\")\n",
    "})\n",
    "\n",
    "# Clean up test objects\n",
    "rm(list = ls()[!ls() %in% c(\"VERBOSE_PRINTS\")])\n",
    "invisible(gc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8pUXVbsrPrg"
   },
   "source": [
    "---\n",
    "## 4. Data Loading\n",
    "\n",
    "Config based data loader with error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-KUtlK-rPrh"
   },
   "outputs": [],
   "source": [
    "# === DATA LOADING ===\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Optional, Dict, Any\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "class NASSDataLoader:\n",
    "    \"\"\"Secure data loader for NASS dataset with encryption support.\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose: bool = True):\n",
    "        self.verbose = verbose\n",
    "        self.data_dir = Path.home() / 'data'\n",
    "        self.data_dir.mkdir(exist_ok=True)\n",
    "        self._setup_environment()\n",
    "        \n",
    "        # Initialize encryption attributes\n",
    "        self._gcs_encrypted_url = None\n",
    "        self._gcs_password = None\n",
    "        self._census_encrypted_url = None\n",
    "        self._census_password = None\n",
    "    \n",
    "    def _setup_environment(self):\n",
    "        \"\"\"Detect environment and set appropriate settings.\"\"\"\n",
    "        self.is_colab = 'google.colab' in sys.modules\n",
    "        self.is_local = not self.is_colab\n",
    "        \n",
    "        if self.verbose:\n",
    "            env_name = \"Google Colab\" if self.is_colab else \"Local/Jupyter\"\n",
    "            print(f\"Environment: {env_name}\")\n",
    "    \n",
    "    def set_encryption_config(self, gcs_url=None, gcs_password=None, census_url=None, census_password=None):\n",
    "        \"\"\"Set encryption configuration securely.\"\"\"\n",
    "        self._gcs_encrypted_url = gcs_url\n",
    "        self._gcs_password = gcs_password\n",
    "        self._census_encrypted_url = census_url\n",
    "        self._census_password = census_password\n",
    "    \n",
    "    def _decrypt_key(self, encrypted_url: str, password: str) -> Optional[str]:\n",
    "        \"\"\"Decrypt key using pycryptodome with security measures.\"\"\"\n",
    "        decrypted_key = None\n",
    "        \n",
    "        try:\n",
    "            from Crypto.Cipher import AES\n",
    "            from Crypto.Hash import SHA256\n",
    "            import base64\n",
    "            import json\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"Downloading and decrypting access key...\")\n",
    "            \n",
    "            # Download encrypted data\n",
    "            response = requests.get(encrypted_url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            encrypted_data = base64.b64decode(response.content)\n",
    "            \n",
    "            # Derive key from password\n",
    "            key = SHA256.new(password.encode()).digest()\n",
    "            \n",
    "            # Decrypt (assuming AES-CBC with IV)\n",
    "            cipher = AES.new(key, AES.MODE_CBC, encrypted_data[:16])\n",
    "            decrypted = cipher.decrypt(encrypted_data[16:])\n",
    "            \n",
    "            # Remove PKCS7 padding\n",
    "            pad_len = decrypted[-1]\n",
    "            decrypted_key = decrypted[:-pad_len].decode()\n",
    "            \n",
    "            # Validate it's valid JSON (basic integrity check)\n",
    "            json.loads(decrypted_key)  # Will raise if invalid\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"‚úÖ Successfully decrypted access key\")\n",
    "            \n",
    "            return decrypted_key\n",
    "            \n",
    "        except ImportError:\n",
    "            self._report_error(\"pycryptodome not installed - cannot decrypt keys\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            if self.verbose:\n",
    "                print(\"‚ùå Decryption failed: Invalid password or corrupted data\")\n",
    "            return None\n",
    "            \n",
    "        finally:\n",
    "            # Security cleanup\n",
    "            try:\n",
    "                if 'password' in locals():\n",
    "                    password = 'X' * len(password)\n",
    "                if 'key' in locals():\n",
    "                    key = b'X' * len(key) \n",
    "                if 'decrypted' in locals():\n",
    "                    decrypted = b'X' * len(decrypted)\n",
    "                \n",
    "                import gc\n",
    "                gc.collect()\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    def load_data(self, source: str, **config) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Load data based on source configuration.\n",
    "        \n",
    "        Args:\n",
    "            source: One of 'github', 'local', 'drive', 'gcs'\n",
    "            **config: Source-specific configuration parameters\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame if successful, None if failed\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"Loading data from source: {source}\")\n",
    "        \n",
    "        try:\n",
    "            if source == \"github\":\n",
    "                return self._load_from_github(config.get('url'))\n",
    "            elif source == \"local\":\n",
    "                return self._load_from_local(config.get('filename'))\n",
    "            elif source == \"drive\" and self.is_colab:\n",
    "                return self._load_from_drive(config.get('path'))\n",
    "            elif source == \"gcs\":\n",
    "                return self._load_from_gcs(config)\n",
    "            else:\n",
    "                self._report_error(f\"Unsupported source '{source}' for current environment\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            self._report_error(f\"Failed to load from {source}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _load_from_github(self, url: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Load data from GitHub public URL.\"\"\"\n",
    "        if not url:\n",
    "            self._report_error(\"GitHub URL not provided in configuration\")\n",
    "            return None\n",
    "        \n",
    "        cache_file = self.data_dir / \"nass_github_cache.csv\"\n",
    "        \n",
    "        # Check cache first\n",
    "        if cache_file.exists():\n",
    "            cache_age = time.time() - cache_file.stat().st_mtime\n",
    "            if cache_age < 3600:  # 1 hour cache\n",
    "                if self.verbose:\n",
    "                    print(\"‚úÖ Using cached GitHub data\")\n",
    "                return pd.read_csv(cache_file)\n",
    "        \n",
    "        try:\n",
    "            if self.verbose:\n",
    "                print(\"Downloading from GitHub...\")\n",
    "            \n",
    "            # Stream download with progress\n",
    "            response = requests.get(url, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            with open(cache_file, 'wb') as f:\n",
    "                downloaded = 0\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        downloaded += len(chunk)\n",
    "                        if self.verbose and total_size > 0:\n",
    "                            progress = (downloaded / total_size) * 100\n",
    "                            print(f\"\\rProgress: {progress:.1f}%\", end='', flush=True)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"\\n‚úÖ Downloaded {downloaded / (1024*1024):.1f} MB\")\n",
    "            \n",
    "            return pd.read_csv(cache_file)\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            self._report_error(f\"GitHub download failed: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _load_from_local(self, filename: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Load data from local file.\"\"\"\n",
    "        if not filename:\n",
    "            self._report_error(\"Local filename not provided in configuration\")\n",
    "            return None\n",
    "        \n",
    "        # Try multiple possible locations\n",
    "        possible_paths = [\n",
    "            Path(filename),\n",
    "            self.data_dir / filename,\n",
    "            Path.cwd() / filename,\n",
    "            Path.cwd() / 'data' / filename\n",
    "        ]\n",
    "        \n",
    "        for file_path in possible_paths:\n",
    "            if file_path.exists():\n",
    "                try:\n",
    "                    if self.verbose:\n",
    "                        size_mb = file_path.stat().st_size / (1024*1024)\n",
    "                        print(f\"Loading local file: {file_path} ({size_mb:.1f} MB)\")\n",
    "                    \n",
    "                    # Use chunked reading for large files\n",
    "                    if file_path.stat().st_size > 100 * 1024 * 1024:  # 100MB\n",
    "                        return self._load_large_csv(file_path)\n",
    "                    else:\n",
    "                        return pd.read_csv(file_path)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    self._report_error(f\"Failed to read {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        self._report_error(f\"Local file '{filename}' not found in any expected location\")\n",
    "        return None\n",
    "    \n",
    "    def _load_from_drive(self, drive_path: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Load data from Google Drive (Colab only).\"\"\"\n",
    "        if not self.is_colab:\n",
    "            self._report_error(\"Google Drive access only available in Colab\")\n",
    "            return None\n",
    "        \n",
    "        if not drive_path:\n",
    "            self._report_error(\"Google Drive path not provided in configuration\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Mount Drive if not already mounted\n",
    "            if not Path('/content/drive').exists():\n",
    "                if self.verbose:\n",
    "                    print(\"Mounting Google Drive...\")\n",
    "                from google.colab import drive\n",
    "                drive.mount('/content/drive')\n",
    "            \n",
    "            file_path = Path(drive_path)\n",
    "            if not file_path.exists():\n",
    "                self._report_error(f\"File not found on Google Drive: {drive_path}\")\n",
    "                return None\n",
    "            \n",
    "            if self.verbose:\n",
    "                size_mb = file_path.stat().st_size / (1024*1024)\n",
    "                print(f\"Loading from Google Drive: {size_mb:.1f} MB\")\n",
    "            \n",
    "            return self._load_large_csv(file_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self._report_error(f\"Google Drive access failed: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _load_from_gcs(self, config: Dict[str, Any]) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Load data from Google Cloud Storage with encrypted authentication.\"\"\"\n",
    "        bucket_name = config.get('bucket')\n",
    "        blob_name = config.get('blob')\n",
    "        \n",
    "        if not bucket_name or not blob_name:\n",
    "            self._report_error(\"GCS bucket and blob names required in configuration\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            from google.cloud import storage\n",
    "            \n",
    "            # Try encrypted service account key first\n",
    "            client = self._try_encrypted_service_account()\n",
    "            \n",
    "            if not client:\n",
    "                # Fallback to other authentication methods\n",
    "                auth_methods = [\n",
    "                    self._try_default_credentials,\n",
    "                    self._try_service_account,\n",
    "                    self._try_anonymous_access\n",
    "                ]\n",
    "                \n",
    "                for auth_method in auth_methods:\n",
    "                    client = auth_method()\n",
    "                    if client:\n",
    "                        break\n",
    "            \n",
    "            if not client:\n",
    "                self._report_error(\"Failed to authenticate with Google Cloud Storage\")\n",
    "                return None\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Accessing gs://{bucket_name}/{blob_name}\")\n",
    "            \n",
    "            bucket = client.bucket(bucket_name)\n",
    "            blob = bucket.blob(blob_name)\n",
    "            \n",
    "            if not blob.exists():\n",
    "                self._report_error(f\"File not found: gs://{bucket_name}/{blob_name}\")\n",
    "                return None\n",
    "            \n",
    "            # Get file info\n",
    "            blob.reload()\n",
    "            size_gb = blob.size / (1024**3)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"File size: {size_gb:.2f} GB\")\n",
    "            \n",
    "            # Download with progress tracking\n",
    "            cache_file = self.data_dir / f\"nass_gcs_{bucket_name}_{blob_name.replace('/', '_')}\"\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"Downloading from GCS...\")\n",
    "            \n",
    "            # Use resumable download for large files\n",
    "            if blob.size > 100 * 1024 * 1024:  # 100MB\n",
    "                self._download_large_blob(blob, cache_file)\n",
    "            else:\n",
    "                blob.download_to_filename(cache_file)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"‚úÖ Download complete, loading into memory...\")\n",
    "            \n",
    "            return self._load_large_csv(cache_file)\n",
    "            \n",
    "        except ImportError:\n",
    "            self._report_error(\"google-cloud-storage package not installed\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self._report_error(f\"GCS access failed: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _try_encrypted_service_account(self):\n",
    "        \"\"\"Try encrypted service account key.\"\"\"\n",
    "        if (self._gcs_encrypted_url and self._gcs_password):\n",
    "            try:\n",
    "                from google.cloud import storage\n",
    "                import json\n",
    "                import tempfile\n",
    "                \n",
    "                # Decrypt the service account key\n",
    "                decrypted_key = self._decrypt_key(self._gcs_encrypted_url, self._gcs_password)\n",
    "                \n",
    "                if decrypted_key:\n",
    "                    # Parse as JSON and create temporary file\n",
    "                    key_data = json.loads(decrypted_key)\n",
    "                    \n",
    "                    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:\n",
    "                        json.dump(key_data, f)\n",
    "                        temp_key_file = f.name\n",
    "                    \n",
    "                    try:\n",
    "                        client = storage.Client.from_service_account_json(temp_key_file)\n",
    "                        if self.verbose:\n",
    "                            print(\"‚úÖ Authenticated using encrypted service account key\")\n",
    "                        return client\n",
    "                    finally:\n",
    "                        # Clean up temporary file\n",
    "                        try:\n",
    "                            os.unlink(temp_key_file)\n",
    "                        except:\n",
    "                            pass\n",
    "                            \n",
    "            except Exception as e:\n",
    "                if self.verbose:\n",
    "                    print(f\"Encrypted service account auth failed: {str(e)}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _try_default_credentials(self):\n",
    "        \"\"\"Try default application credentials.\"\"\"\n",
    "        try:\n",
    "            from google.cloud import storage\n",
    "            return storage.Client()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def _try_service_account(self):\n",
    "        \"\"\"Try service account key file.\"\"\"\n",
    "        try:\n",
    "            from google.cloud import storage\n",
    "            key_file = self.data_dir / 'gcs_service_account.json'\n",
    "            if key_file.exists():\n",
    "                return storage.Client.from_service_account_json(str(key_file))\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    def _try_anonymous_access(self):\n",
    "        \"\"\"Try anonymous access for public buckets.\"\"\"\n",
    "        try:\n",
    "            from google.cloud import storage\n",
    "            return storage.Client.create_anonymous_client()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def _download_large_blob(self, blob, cache_file):\n",
    "        \"\"\"Download large blob with progress tracking.\"\"\"\n",
    "        chunk_size = 1024 * 1024  # 1MB chunks\n",
    "        \n",
    "        with open(cache_file, 'wb') as f:\n",
    "            downloaded = 0\n",
    "            \n",
    "            # Download in chunks\n",
    "            for chunk_start in range(0, blob.size, chunk_size):\n",
    "                chunk_end = min(chunk_start + chunk_size - 1, blob.size - 1)\n",
    "                \n",
    "                chunk_data = blob.download_as_bytes(\n",
    "                    start=chunk_start,\n",
    "                    end=chunk_end\n",
    "                )\n",
    "                \n",
    "                f.write(chunk_data)\n",
    "                downloaded += len(chunk_data)\n",
    "                \n",
    "                if self.verbose:\n",
    "                    progress = (downloaded / blob.size) * 100\n",
    "                    print(f\"\\rProgress: {progress:.1f}%\", end='', flush=True)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print()  # New line after progress\n",
    "   \n",
    "    def _load_large_csv(self, file_path: Path) -> pd.DataFrame:\n",
    "        \"\"\"Load large CSV files efficiently with progress tracking.\"\"\"\n",
    "        file_size = file_path.stat().st_size\n",
    "        \n",
    "        if file_size < 500 * 1024 * 1024:  # Less than 500MB, load normally\n",
    "            return pd.read_csv(file_path)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Loading large file in chunks...\")\n",
    "        \n",
    "        try:\n",
    "            # Memory-efficient chunk processing - NO STORING ALL CHUNKS\n",
    "            print(\"üìä Loading data with memory optimization...\")\n",
    "            total_rows = 0\n",
    "            first_chunk = True\n",
    "\n",
    "            # Process chunks one at a time without storing them all\n",
    "            for i, chunk in enumerate(pd.read_csv(file_path,\n",
    "                                                chunksize=50000,  # Smaller chunks\n",
    "                                                low_memory=False,\n",
    "                                                dtype=str)):  # Force string type for mixed columns\n",
    "        \n",
    "                # Basic processing on each chunk\n",
    "                total_rows += len(chunk)\n",
    "                print(f\"   Processed chunk {i+1}: {len(chunk):,} rows (Total: {total_rows:,})\")\n",
    "        \n",
    "                # For the first chunk, establish the combined dataframe\n",
    "                if first_chunk:\n",
    "                    # Take a sample for analysis instead of full dataset\n",
    "                    sample_size = min(100000, len(chunk))  # Max 100k rows for analysis\n",
    "                    df = chunk.head(sample_size).copy()\n",
    "                    first_chunk = False\n",
    "                    print(f\"   ‚úÖ Using sample of {len(df):,} rows for analysis\")\n",
    "                    break  # Exit after first chunk for memory safety\n",
    "        \n",
    "                # Memory cleanup every few chunks\n",
    "                if i % 5 == 0:\n",
    "                    import gc\n",
    "                    gc.collect()\n",
    "                    \n",
    "                # Safety limit - stop if too many chunks\n",
    "                if i >= 20:\n",
    "                    print(\"   ‚ö†Ô∏è Stopping at 20 chunks for memory safety\")\n",
    "                    break\n",
    "        \n",
    "            print(f\"‚úÖ Data processing complete!\")\n",
    "            print(f\"   Full dataset size: {total_rows:,} rows\")\n",
    "            print(f\"   Analysis sample: {len(df):,} rows\")\n",
    "            \n",
    "            return df\n",
    "        \n",
    "        except MemoryError:\n",
    "            print(\"‚ùå Memory limit reached - using smaller sample\")\n",
    "            # Fallback: load just a small sample\n",
    "            df = pd.read_csv(file_path, nrows=50000, low_memory=False)\n",
    "            print(f\"   Fallback sample: {len(df):,} rows\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during data loading: {e}\")\n",
    "            # Last resort: very small sample\n",
    "            df = pd.read_csv(file_path, nrows=10000, low_memory=False)\n",
    "            print(f\"   Emergency sample: {len(df):,} rows\")\n",
    "            return df\n",
    "\n",
    "# Initialize the data loader\n",
    "data_loader = NASSDataLoader(verbose=VERBOSE_PRINTS)\n",
    "\n",
    "# Pass encrypted URLs and passwords to the data loader if available\n",
    "if 'GCS_ENCRYPTED_KEY_URL' in globals() and 'GCS_PASSWORD' in globals():\n",
    "    data_loader.set_encryption_config(\n",
    "        gcs_url=GCS_ENCRYPTED_KEY_URL,\n",
    "        gcs_password=GCS_PASSWORD\n",
    "    )\n",
    "\n",
    "if 'CENSUS_ENCRYPTED_KEY_URL' in globals() and 'CENSUS_PASSWORD' in globals():\n",
    "    data_loader.set_encryption_config(\n",
    "        census_url=CENSUS_ENCRYPTED_KEY_URL,\n",
    "        census_password=CENSUS_PASSWORD\n",
    "    )\n",
    "\n",
    "# Load data based on configuration\n",
    "config_map = {\n",
    "    \"github\": {\"url\": GITHUB_URL},\n",
    "    \"local\": {\"filename\": LOCAL_FILENAME},\n",
    "    \"drive\": {\"path\": DRIVE_PATH},\n",
    "    \"gcs\": {\n",
    "        \"bucket\": GCS_BUCKET,\n",
    "        \"blob\": GCS_BLOB\n",
    "    }\n",
    "}\n",
    "\n",
    "if DATA_SOURCE in config_map:\n",
    "    print(f\"Loading NASS data from {DATA_SOURCE}...\")\n",
    "    df = data_loader.load_data(DATA_SOURCE, **config_map[DATA_SOURCE])\n",
    "    \n",
    "    if df is not None:\n",
    "        print(f\"‚úÖ Successfully loaded: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "        print(f\"Memory usage: {df.memory_usage(deep=True).sum() / (1024*1024):.1f} MB\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to load data\")\n",
    "        print(\"\\nTroubleshooting:\")\n",
    "        if DATA_SOURCE == \"local\":\n",
    "            print(f\"  - Ensure file '{LOCAL_FILENAME}' exists in one of these locations:\")\n",
    "            print(f\"    ‚Ä¢ {Path.cwd()}\")\n",
    "            print(f\"    ‚Ä¢ {data_loader.data_dir}\")\n",
    "        elif DATA_SOURCE == \"gcs\":\n",
    "            print(f\"  - Verify bucket '{GCS_BUCKET}' and file '{GCS_BLOB}' exist\")\n",
    "            print(f\"  - Check decryption password or authentication\")\n",
    "        elif DATA_SOURCE == \"drive\":\n",
    "            print(f\"  - Ensure file exists at: {DRIVE_PATH}\")\n",
    "            print(f\"  - Google Drive must be mounted in Colab\")\n",
    "        elif DATA_SOURCE == \"github\":\n",
    "            print(f\"  - Check URL is accessible: {GITHUB_URL}\")\n",
    "            print(f\"  - Verify internet connection\")\n",
    "else:\n",
    "    print(f\"‚ùå Invalid data source: {DATA_SOURCE}\")\n",
    "    print(f\"Valid sources: {list(config_map.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxQ5fG5grPrj"
   },
   "source": [
    "Check if data has been loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqUzCuZOrPrj"
   },
   "outputs": [],
   "source": [
    "# === DATA LOAD VERIFICATION ===\n",
    "\n",
    "# Verify data is available before R processing\n",
    "try:\n",
    "    if 'df' not in globals():\n",
    "        print(\"‚ùå Data not loaded!\")\n",
    "        print(\"üí° Please run the 'Data Loading' section first (cell 12)\")\n",
    "        print(\"   This will create the 'df' variable needed for R analysis\")\n",
    "        raise NameError(\"df variable not found - run data loading first\")\n",
    "    \n",
    "    # Check if df exists but is None or empty\n",
    "    if df is None:\n",
    "        print(\"‚ùå Data loading failed!\")\n",
    "        print(\"üí° The data loader returned None - check the previous cell for errors\")\n",
    "        print(\"   Common issues:\")\n",
    "        print(\"   ‚Ä¢ Network connection problems\")\n",
    "        print(\"   ‚Ä¢ Invalid file path or URL\")\n",
    "        print(\"   ‚Ä¢ Authentication issues (for cloud storage)\")\n",
    "        raise ValueError(\"df is None - data loading failed\")\n",
    "    \n",
    "    # Check if df is empty\n",
    "    if hasattr(df, 'shape') and df.shape[0] == 0:\n",
    "        print(\"‚ùå Data is empty!\")\n",
    "        print(\"üí° The dataset loaded but contains no rows\")\n",
    "        raise ValueError(\"df is empty - no data to analyze\")\n",
    "    \n",
    "    # All checks passed\n",
    "    print(f\"‚úÖ Data verified: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "    print(\"‚úÖ Ready for R analysis\")\n",
    "\n",
    "except (NameError, ValueError, AttributeError) as e:\n",
    "    print(f\"‚ùå {e}\")\n",
    "    print(\"\\nüîÑ Quick fix: Run these cells in order:\")\n",
    "    print(\"   1. Configuration (cell 6)\")\n",
    "    print(\"   2. Environment Setup (cell 8)\")  \n",
    "    print(\"   3. Data Loading (cell 13)\")\n",
    "    print(\"   4. Then continue with R analysis\")\n",
    "    \n",
    "    print(\"\\nüîç Troubleshooting:\")\n",
    "    print(\"   ‚Ä¢ Check your DATA_SOURCE setting in configuration\")\n",
    "    print(\"   ‚Ä¢ Verify internet connection for GitHub/cloud sources\")\n",
    "    print(\"   ‚Ä¢ Ensure file exists for local sources\")\n",
    "    print(\"   ‚Ä¢ Check authentication for cloud storage\")\n",
    "    \n",
    "    # Don't raise the error - just warn and continue\n",
    "    print(\"\\n‚ö†Ô∏è  Continuing without data - subsequent cells may fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3j3Ms-OArPrj"
   },
   "source": [
    "## 5. Complete Data Preprocessing\n",
    "\n",
    "Streamlined preprocessing: remove variables, clean data types, and create new variables - in Python prior to passing to R for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQzhWzohrPrj"
   },
   "outputs": [],
   "source": [
    "# === DATA PREPROCESSING ===\n",
    "\n",
    "# Data Preprocessing in Python (before R transfer)\n",
    "print(\"Complete preprocessing: removing variables + cleaning data types...\")\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "\n",
    "# ===== 1. REMOVE UNNECESSARY VARIABLES =====\n",
    "print(\"\\n1Ô∏è‚É£ Removing unnecessary variables...\")\n",
    "\n",
    "# Smart pattern-based removal in pandas (much faster than R)\n",
    "drop_patterns = [\n",
    "    r'^CPTCCS[2-9]$',      # CPTCCS2-CPTCCS9\n",
    "    r'^CPTCCS[1-3][0-9]$', # CPTCCS10-30\n",
    "    r'^CPT[2-9]$',         # CPT2-CPT9\n",
    "    r'^CPT[1-3][0-9]$',    # CPT10-30\n",
    "    r'^DXCCSR_',           # All DXCCSR columns (500+)\n",
    "]\n",
    "\n",
    "# Find columns to drop using vectorized operations\n",
    "drop_cols = []\n",
    "for pattern in drop_patterns:\n",
    "    matches = df.columns[df.columns.str.match(pattern)].tolist()\n",
    "    drop_cols.extend(matches)\n",
    "\n",
    "# Remove duplicates\n",
    "drop_cols = list(set(drop_cols))\n",
    "\n",
    "print(f\"   Found {len(drop_cols)} columns to drop\")\n",
    "print(f\"   Patterns: CPTCCS2-30, CPT2-30, all DXCCSR_*\")\n",
    "\n",
    "# Drop the columns\n",
    "df = df.drop(columns=drop_cols)\n",
    "print(f\"   ‚úÖ Reduced from {df.shape[1] + len(drop_cols)} to {df.shape[1]} columns\")\n",
    "\n",
    "# ===== 2. CLEAN DATA TYPES FOR rpy2 =====\n",
    "print(\"\\n2Ô∏è‚É£ Cleaning data types for rpy2 compatibility...\")\n",
    "\n",
    "# Convert all object columns to strings (prevents mixed-type issues)\n",
    "object_columns = df.select_dtypes(include=['object']).columns\n",
    "if len(object_columns) > 0:\n",
    "    for col in object_columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "    print(f\"   ‚úÖ Converted {len(object_columns)} object columns to strings\")\n",
    "\n",
    "# Handle NaN/inf values consistently - FIXED FOR CATEGORICAL COLUMNS\n",
    "# First handle categorical columns separately\n",
    "categorical_columns = df.select_dtypes(include=['category']).columns\n",
    "if len(categorical_columns) > 0:\n",
    "    for col in categorical_columns:\n",
    "        # Convert categorical to string first, then handle NaN\n",
    "        df[col] = df[col].astype(str)\n",
    "    print(f\"   ‚úÖ Converted {len(categorical_columns)} categorical columns to strings\")\n",
    "\n",
    "# Now handle all non-categorical columns\n",
    "non_categorical_columns = df.select_dtypes(exclude=['category']).columns\n",
    "if len(non_categorical_columns) > 0:\n",
    "    # Replace NaN with empty strings for non-categorical columns\n",
    "    df[non_categorical_columns] = df[non_categorical_columns].fillna('')\n",
    "\n",
    "# Handle inf values in float columns\n",
    "float_cols = df.select_dtypes(include=['float64', 'float32']).columns\n",
    "if len(float_cols) > 0:\n",
    "    for col in float_cols:\n",
    "        df[col] = df[col].replace([float('inf'), float('-inf')], '')\n",
    "    print(f\"   ‚úÖ Cleaned inf values in {len(float_cols)} float columns\")\n",
    "\n",
    "print(f\"   ‚úÖ Cleaned NaN values in all columns\")\n",
    "\n",
    "# ===== 3. CREATE KEY ANALYTICAL VARIABLES IN PANDAS =====\n",
    "print(\"\\n3Ô∏è‚É£ Creating analytical variables...\")\n",
    "\n",
    "# Create WHITE indicator (1=White, 0=Non-White)\n",
    "if 'RACE' in df.columns:\n",
    "    df['WHITE'] = (df['RACE'].astype(str) == '1').astype(int)\n",
    "    print(\"   ‚úÖ Created race indicator boolean\")\n",
    "\n",
    "# Create age groups\n",
    "if 'AGE' in df.columns:\n",
    "    df['AGE'] = pd.to_numeric(df['AGE'], errors='coerce')  # Ensure numeric\n",
    "    df['AGE_GROUP'] = pd.cut(df['AGE'],\n",
    "                            bins=[0, 18, 30, 45, 65, float('inf')],\n",
    "                            labels=['0-17', '18-29', '30-44', '45-64', '65+'],\n",
    "                            right=False)\n",
    "    df['AGE_GROUP'] = df['AGE_GROUP'].astype(str)  # Convert to string for R\n",
    "    print(\"   ‚úÖ Created AGE_GROUP categories\")\n",
    "\n",
    "# Create income level labels\n",
    "if 'ZIPINC_QRTL' in df.columns:\n",
    "    income_map = {1: 'Q1-Lowest', 2: 'Q2', 3: 'Q3', 4: 'Q4-Highest'}\n",
    "    df['INCOME_LEVEL'] = df['ZIPINC_QRTL'].astype(str).map(lambda x: income_map.get(int(x) if x.isdigit() else 0, 'Unknown'))\n",
    "    print(\"   ‚úÖ Created INCOME_LEVEL labels\")\n",
    "\n",
    "# Ensure key numeric variables are properly typed\n",
    "numeric_vars = ['AGE', 'DISCWT', 'TOTCHG']\n",
    "for var in numeric_vars:\n",
    "    if var in df.columns:\n",
    "        df[var] = pd.to_numeric(df[var], errors='coerce')\n",
    "\n",
    "# save copy of cleaned data for R transfer\n",
    "cleaned_path = data_loader.data_dir / \"nass_data_cleaned.csv\"\n",
    "df.to_csv(cleaned_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ PREPROCESSING COMPLETE!\")\n",
    "print(f\"Final shape: {df.shape}\")\n",
    "print(f\"Cleaned data saved to: {cleaned_path}\")\n",
    "print(f\"Ready for R transfer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1smsFldXrPrj"
   },
   "source": [
    "## 6. Final R Transfer & Processing\n",
    "\n",
    "Transfer the clean data to R and apply any final R-specific formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10dad708"
   },
   "outputs": [],
   "source": [
    "%%R -i df -i VERBOSE_PRINTS # === R Transfer ===\n",
    "\n",
    "# Convert to data.table and apply R types\n",
    "NASS <- as.data.table(df)\n",
    "\n",
    "# Factor variables\n",
    "factor_vars <- c(\"ZIPINC_QRTL\", \"PAY1\", \"CPTCCS1\", \"HOSP_LOCATION\",\n",
    "                 \"HOSP_TEACH\", \"HOSP_NASS\", \"RACE\", \"AGE_GROUP\", \"INCOME_LEVEL\")\n",
    "existing_factors <- factor_vars[factor_vars %in% names(NASS)]\n",
    "NASS[, (existing_factors) := lapply(.SD, as.factor), .SDcols = existing_factors]\n",
    "\n",
    "# Boolean variables\n",
    "if(\"FEMALE\" %in% names(NASS)) NASS[, FEMALE := as.logical(as.numeric(FEMALE))]\n",
    "if(\"WHITE\" %in% names(NASS)) NASS[, WHITE := as.logical(as.numeric(WHITE))]\n",
    "\n",
    "# Compact output\n",
    "cat(\"> R Complete:\", nrow(NASS), \"rows,\", ncol(NASS), \"cols,\",\n",
    "    round(object.size(NASS)/1024^2, 1), \"MB\\n\")\n",
    "cat(\"Converted\", length(existing_factors), \"factors + 2 booleans\\n\")\n",
    "\n",
    "if(VERBOSE_PRINTS) {\n",
    "  cat(\"\\nColumns:\\n\")\n",
    "  print(colnames(NASS))\n",
    "}\n",
    "\n",
    "cat(\"Data in R!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Output Saving system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R # === OUTPUT SAVING SYSTEM ===\n",
    "\n",
    "# ========================================\n",
    "# OUTPUT SAVING SYSTEM FOR POSTER GENERATION\n",
    "# ========================================\n",
    "\n",
    "# Install and load required packages first\n",
    "required_packages <- c(\"ggplot2\", \"data.table\", \"scales\", \"RColorBrewer\")\n",
    "\n",
    "for(pkg in required_packages) {\n",
    "  if(!require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "    cat(\"Installing\", pkg, \"...\\n\")\n",
    "    install.packages(pkg, quiet = TRUE)\n",
    "    library(pkg, character.only = TRUE, quietly = TRUE)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Define theme and color schemes\n",
    "theme_nass <- theme_minimal() +\n",
    "  theme(\n",
    "    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n",
    "    plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray40\"),\n",
    "    axis.title = element_text(size = 11, face = \"bold\"),\n",
    "    axis.text = element_text(size = 10),\n",
    "    legend.title = element_text(size = 11, face = \"bold\"),\n",
    "    legend.text = element_text(size = 10),\n",
    "    strip.text = element_text(size = 10, face = \"bold\"),\n",
    "    panel.grid.minor = element_blank()\n",
    "  )\n",
    "\n",
    "# Define consistent color palettes\n",
    "race_colors <- RColorBrewer::brewer.pal(6, \"Set2\")\n",
    "pay_colors <- RColorBrewer::brewer.pal(6, \"Dark2\")\n",
    "region_colors <- RColorBrewer::brewer.pal(4, \"Set1\")\n",
    "\n",
    "# Create plots directory if it doesn't exist\n",
    "plot_dir <- \"saved_plots\"\n",
    "if (!dir.exists(plot_dir)) dir.create(plot_dir, recursive = TRUE)\n",
    "\n",
    "# Global plot counter and metadata (initialize if not exists)\n",
    "if(!exists(\"plot_counter\")) plot_counter <- 1\n",
    "if(!exists(\"plot_metadata\")) plot_metadata <- list()\n",
    "\n",
    "# Enhanced save function with metadata\n",
    "save_plot_for_poster <- function(plot_obj = NULL, \n",
    "                                name = NULL, \n",
    "                                width = 12, \n",
    "                                height = 8, \n",
    "                                dpi = 300,\n",
    "                                description = \"\",\n",
    "                                category = \"analysis\") {\n",
    "  \n",
    "  # Auto-generate name if not provided\n",
    "  if (is.null(name)) {\n",
    "    name <- paste0(\"plot_\", sprintf(\"%02d\", plot_counter))\n",
    "  }\n",
    "  \n",
    "  # File paths\n",
    "  png_file <- file.path(plot_dir, paste0(name, \".png\"))\n",
    "  pdf_file <- file.path(plot_dir, paste0(name, \".pdf\"))\n",
    "  \n",
    "  # Save current plot if no plot object provided\n",
    "  if (is.null(plot_obj)) {\n",
    "    ggsave(png_file, width = width, height = height, dpi = dpi, units = \"in\")\n",
    "    ggsave(pdf_file, width = width, height = height, dpi = dpi, units = \"in\")\n",
    "  } else {\n",
    "    # Save specific plot object\n",
    "    ggsave(png_file, plot = plot_obj, width = width, height = height, dpi = dpi, units = \"in\")\n",
    "    ggsave(pdf_file, plot = plot_obj, width = width, height = height, dpi = dpi, units = \"in\")\n",
    "  }\n",
    "  \n",
    "  # Store metadata\n",
    "  plot_metadata[[name]] <<- list(\n",
    "    file_png = png_file,\n",
    "    file_pdf = pdf_file,\n",
    "    width = width,\n",
    "    height = height,\n",
    "    dpi = dpi,\n",
    "    description = description,\n",
    "    category = category,\n",
    "    order = plot_counter\n",
    "  )\n",
    "  \n",
    "  plot_counter <<- plot_counter + 1\n",
    "  \n",
    "  cat(\"> Saved plot:\", name, \"->\", basename(png_file), \"\\n\")\n",
    "  cat(\"   Description:\", description, \"\\n\")\n",
    "  \n",
    "  return(invisible(png_file))\n",
    "}\n",
    "\n",
    "# Text output saving function\n",
    "save_text_output <- function(text_content, name, description = \"\", category = \"analysis\") {\n",
    "  \n",
    "  # Create text outputs directory\n",
    "  text_dir <- \"saved_outputs\"\n",
    "  if (!dir.exists(text_dir)) dir.create(text_dir, recursive = TRUE)\n",
    "  \n",
    "  # File path\n",
    "  text_file <- file.path(text_dir, paste0(name, \".txt\"))\n",
    "  \n",
    "  # Add metadata header\n",
    "  header <- paste0(\n",
    "    \"# \", description, \"\\n\",\n",
    "    \"# Generated: \", Sys.time(), \"\\n\",\n",
    "    \"# Category: \", category, \"\\n\",\n",
    "    \"# ========================================\\n\\n\"\n",
    "  )\n",
    "  \n",
    "  # Write content\n",
    "  writeLines(c(header, text_content), text_file)\n",
    "  \n",
    "  cat(\"> Saved text output:\", name, \"->\", basename(text_file), \"\\n\")\n",
    "  \n",
    "  return(invisible(text_file))\n",
    "}\n",
    "\n",
    "cat(\"> Output saving system initialized\\n\")\n",
    "cat(\"   Directory:\", plot_dir, \"\\n\")\n",
    "cat(\"   Packages loaded and themes defined\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Color and Theme Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R # === COLOR SYSTEM ===\n",
    "\n",
    "# ========================================\n",
    "# ENHANCED GLOBAL THEME AND COLOR SYSTEM\n",
    "# ========================================\n",
    "\n",
    "cat(\"> UPDATING GLOBAL THEME AND COLOR SYSTEM\\n\")\n",
    "cat(paste(rep(\"=\", 50), collapse = \"\"), \"\\n\")  # FIXED: Use paste(rep()) instead of \"=\" * 50\n",
    "\n",
    "# Install required packages if needed\n",
    "required_packages <- c(\"ggplot2\", \"scales\", \"RColorBrewer\")\n",
    "for(pkg in required_packages) {\n",
    "  if(!require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "    install.packages(pkg, quiet = TRUE)\n",
    "    library(pkg, character.only = TRUE, quietly = TRUE)\n",
    "  }\n",
    "}\n",
    "\n",
    "# ========================================\n",
    "# ENHANCED POSTER-READY THEME\n",
    "# ========================================\n",
    "\n",
    "theme_poster <- theme_minimal() +\n",
    "  theme(\n",
    "    # TITLES - Left-aligned, larger fonts\n",
    "    plot.title = element_text(\n",
    "      size = 18,           # Increased from 14\n",
    "      face = \"bold\", \n",
    "      hjust = 0,           # LEFT-ALIGNED (was 0.5 center)\n",
    "      margin = ggplot2::margin(b = 8, t = 5)\n",
    "    ),\n",
    "    plot.subtitle = element_text(\n",
    "      size = 14,           # Increased from 12  \n",
    "      hjust = 0,           # LEFT-ALIGNED (was 0.5 center)\n",
    "      color = \"gray30\",    # Darker for better readability\n",
    "      margin = ggplot2::margin(b = 15)\n",
    "    ),\n",
    "    \n",
    "    # AXIS - Larger fonts\n",
    "    axis.title = element_text(size = 14, face = \"bold\"),  # Increased from 11\n",
    "    axis.text = element_text(size = 12),                  # Increased from 10\n",
    "    axis.title.x = element_text(margin = ggplot2::margin(t = 8)),\n",
    "    axis.title.y = element_text(margin = ggplot2::margin(r = 8)),\n",
    "    \n",
    "    # LEGEND - Larger fonts\n",
    "    legend.title = element_text(size = 14, face = \"bold\"), # Increased from 11\n",
    "    legend.text = element_text(size = 12),                 # Increased from 10\n",
    "    legend.position = \"bottom\",\n",
    "    legend.margin = ggplot2::margin(t = 10),\n",
    "    \n",
    "    # FACET STRIPS - Larger fonts\n",
    "    strip.text = element_text(size = 12, face = \"bold\"),   # Increased from 10\n",
    "    \n",
    "    # GRID\n",
    "    panel.grid.minor = element_blank(),\n",
    "    panel.grid.major = element_line(color = \"gray90\", linewidth = 0.5),\n",
    "    \n",
    "    # PLOT MARGINS\n",
    "    plot.margin = ggplot2::margin(15, 15, 15, 15)\n",
    "  )\n",
    "\n",
    "# ========================================\n",
    "# UNIFIED COLOR PALETTES\n",
    "# ========================================\n",
    "\n",
    "# PRIMARY PALETTE: Professional blue/teal scheme for most plots\n",
    "primary_colors <- c(\n",
    "  \"#1f77b4\",  # Professional blue\n",
    "  \"#ff7f0e\",  # Orange\n",
    "  \"#2ca02c\",  # Green  \n",
    "  \"#d62728\",  # Red\n",
    "  \"#9467bd\",  # Purple\n",
    "  \"#8c564b\",  # Brown\n",
    "  \"#e377c2\",  # Pink\n",
    "  \"#7f7f7f\",  # Gray\n",
    "  \"#bcbd22\",  # Olive\n",
    "  \"#17becf\"   # Cyan\n",
    ")\n",
    "\n",
    "# RACE/ETHNICITY: Consistent across all demographic plots\n",
    "race_colors_unified <- c(\n",
    "  \"1\" = \"#1f77b4\",  # White - Professional blue\n",
    "  \"2\" = \"#ff7f0e\",  # Black - Orange  \n",
    "  \"3\" = \"#2ca02c\",  # Hispanic - Green\n",
    "  \"4\" = \"#d62728\",  # Asian/Pacific - Red\n",
    "  \"5\" = \"#9467bd\",  # Native American - Purple\n",
    "  \"6\" = \"#8c564b\"   # Other - Brown\n",
    ")\n",
    "\n",
    "# PAYER: Consistent insurance colors\n",
    "payer_colors_unified <- c(\n",
    "  \"1\" = \"#1f77b4\",  # Medicare - Blue\n",
    "  \"2\" = \"#ff7f0e\",  # Medicaid - Orange\n",
    "  \"3\" = \"#2ca02c\",  # Private - Green\n",
    "  \"4\" = \"#d62728\",  # Self-pay - Red\n",
    "  \"5\" = \"#9467bd\",  # No Charge - Purple\n",
    "  \"6\" = \"#8c564b\"   # Other - Brown\n",
    ")\n",
    "\n",
    "# INCOME: Blue gradient for income quartiles\n",
    "income_colors_unified <- c(\n",
    "  \"1\" = \"#1f77b4\",  # Q1 Lowest\n",
    "  \"2\" = \"#ff7f0e\",  # Q2\n",
    "  \"3\" = \"#2ca02c\",  # Q3\n",
    "  \"4\" = \"#d62728\"  # Q4 Highest\n",
    ")\n",
    "\n",
    "# REGION: Distinct colors for US regions\n",
    "region_colors_unified <- c(\n",
    "  \"1\" = \"#1f77b4\",  # Northeast - Blue\n",
    "  \"2\" = \"#ff7f0e\",  # Midwest - Orange\n",
    "  \"3\" = \"#2ca02c\",  # South - Green\n",
    "  \"4\" = \"#d62728\"   # West - Red\n",
    ")\n",
    "\n",
    "# ========================================\n",
    "# RED-WHITE-BLUE THEME for Census Plots and Heatmap\n",
    "# ========================================\n",
    "\n",
    "# Colors matching the hourglass plot for census comparisons\n",
    "census_colors <- c(\n",
    "  \"Female.Census\" = \"#ff9999\",  # Light red\n",
    "  \"Female.NASS\" = \"#87ceeb\",    # Light blue\n",
    "  \"Male.Census\" = \"#d62728\",    # Deep red\n",
    "  \"Male.NASS\" = \"#1f77b4\"       # Deep blue\n",
    ")\n",
    "\n",
    "# Heatmap: Red-white-blue gradient\n",
    "heatmap_colors <- list(\n",
    "  low = \"#1E3A8A\",     # Deep blue\n",
    "  mid = \"#F8F9FA\",     # Near white (better contrast than pure white)\n",
    "  high = \"#B22222\"     # Deep red\n",
    ")\n",
    "\n",
    "cat(\"> Enhanced theme and color system defined\\n\")\n",
    "cat(\"   - theme_poster: Left-aligned titles, larger fonts\\n\")\n",
    "cat(\"   - primary_colors: Professional 10-color palette\\n\") \n",
    "cat(\"   - race_colors_unified: Consistent demographic colors\\n\")\n",
    "cat(\"   - payer_colors_unified: Insurance type colors\\n\")\n",
    "cat(\"   - income_colors_unified: Blue gradient for SES\\n\")\n",
    "cat(\"   - region_colors_unified: Geographic colors\\n\")\n",
    "cat(\"   - census_colors: Red-white-blue for validation plots\\n\")\n",
    "cat(\"   - heatmap_colors: Red-white-blue gradient\\n\")\n",
    "\n",
    "cat(\"\\n > Ready to apply unified theming to all plots!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Status Check/Recovery\n",
    "\n",
    "Please use this cell to reset analysis environment in case of cell crash/hang. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STATUS CHECK & RECOVERY ===\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "def quick_health_check():\n",
    "    \"\"\"Quick, accurate health check focused on actual functionality\"\"\"\n",
    "    print(\"üîç ENVIRONMENT STATUS CHECK\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # 1. Check core data\n",
    "    if 'df' in globals():\n",
    "        df_shape = globals()['df'].shape\n",
    "        print(f\"‚úÖ Data: {df_shape[0]:,} rows √ó {df_shape[1]} columns\")\n",
    "    else:\n",
    "        print(\"‚ùå Data: Not loaded\")\n",
    "        issues.append(\"data_missing\")\n",
    "    \n",
    "    # 2. Check R integration properly\n",
    "    r_working = False\n",
    "    try:\n",
    "        # Actually test R functionality\n",
    "        get_ipython().run_cell_magic('R', '', 'cat(\"R test successful\\\\n\")')\n",
    "        r_working = True\n",
    "        print(\"‚úÖ R integration: Working\")\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå R integration: Failed\")\n",
    "        issues.append(\"r_failed\")\n",
    "    \n",
    "    # 3. Check R data if R works\n",
    "    if r_working and 'df' in globals():\n",
    "        try:\n",
    "            get_ipython().run_cell_magic('R', '', '''\n",
    "            if(exists(\"NASS\")) {\n",
    "                cat(\"‚úÖ R data: Available (\", nrow(NASS), \"rows)\\\\n\")\n",
    "            } else {\n",
    "                cat(\"‚ùå R data: Missing\\\\n\")\n",
    "            }\n",
    "            ''')\n",
    "        except:\n",
    "            print(\"‚ùå R data: Transfer failed\")\n",
    "            issues.append(\"r_data_missing\")\n",
    "    \n",
    "    # 4. Check configuration\n",
    "    config_ok = all(var in globals() for var in ['DATA_SOURCE', 'VERBOSE_PRINTS'])\n",
    "    print(f\"{'‚úÖ' if config_ok else '‚ùå'} Configuration: {'Set' if config_ok else 'Missing'}\")\n",
    "    if not config_ok:\n",
    "        issues.append(\"config_missing\")\n",
    "    \n",
    "    # 5. Check memory usage\n",
    "    try:\n",
    "        import psutil\n",
    "        memory_pct = psutil.virtual_memory().percent\n",
    "        memory_ok = memory_pct < 90\n",
    "        print(f\"{'‚úÖ' if memory_ok else '‚ö†Ô∏è '} Memory: {memory_pct:.1f}% used\")\n",
    "        if not memory_ok:\n",
    "            issues.append(\"high_memory\")\n",
    "    except:\n",
    "        print(\"? Memory: Cannot check\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if not issues:\n",
    "        print(\"‚úÖ All systems operational - Ready for analysis!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  {len(issues)} issues detected\")\n",
    "        return issues\n",
    "\n",
    "def quick_recovery():\n",
    "    \"\"\"Simple recovery for common issues\"\"\"\n",
    "    print(\"üîß ATTEMPTING RECOVERY...\")\n",
    "    \n",
    "    # Restore config if missing\n",
    "    if 'DATA_SOURCE' not in globals():\n",
    "        globals()['DATA_SOURCE'] = \"github\"\n",
    "        globals()['VERBOSE_PRINTS'] = True\n",
    "        print(\"   ‚úÖ Configuration restored\")\n",
    "    \n",
    "    # Reload data if missing\n",
    "    if 'df' not in globals():\n",
    "        try:\n",
    "            # Try to reload from cache\n",
    "            from pathlib import Path\n",
    "            cache_file = Path.home() / 'data' / 'nass_data_github.csv'\n",
    "            if cache_file.exists():\n",
    "                import pandas as pd\n",
    "                globals()['df'] = pd.read_csv(cache_file)\n",
    "                print(f\"   ‚úÖ Data reloaded: {globals()['df'].shape[0]:,} rows\")\n",
    "            else:\n",
    "                print(\"   ‚ùå No cached data found - run data loading section\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Data reload failed: {e}\")\n",
    "    \n",
    "    # Restore R data if possible\n",
    "    if globals().get('R_AVAILABLE', True) and 'df' in globals():\n",
    "        try:\n",
    "            get_ipython().run_cell_magic('R', '-i df', '''\n",
    "            if(!exists(\"NASS\") && exists(\"df\")) {\n",
    "                library(data.table)\n",
    "                NASS <- as.data.table(df)\n",
    "                cat(\"   ‚úÖ R data restored\\\\n\")\n",
    "            }\n",
    "            ''')\n",
    "        except:\n",
    "            print(\"   ‚ùå R data restore failed\")\n",
    "    \n",
    "    # Clear memory\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(\"   ‚úÖ Memory cleaned\")\n",
    "\n",
    "def mini_reset():\n",
    "    \"\"\"Clear variables but keep essential functions\"\"\"\n",
    "    keep = ['quick_health_check', 'quick_recovery', 'mini_reset', \n",
    "            'DATA_SOURCE', 'VERBOSE_PRINTS', 'GITHUB_URL']\n",
    "    \n",
    "    cleared = 0\n",
    "    for var in list(globals().keys()):\n",
    "        if not var.startswith('_') and var not in keep:\n",
    "            try:\n",
    "                del globals()[var]\n",
    "                cleared += 1\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(f\"üßπ Cleared {cleared} variables\")\n",
    "    print(\"üí° Re-run setup cells to restore environment\")\n",
    "\n",
    "# Run check\n",
    "issues = quick_health_check()\n",
    "\n",
    "if issues and issues != True:\n",
    "    response = input(\"\\nAttempt recovery? (y/n): \").lower()\n",
    "    if response == 'y':\n",
    "        quick_recovery()\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(\"Re-checking after recovery...\")\n",
    "        quick_health_check()\n",
    "\n",
    "print(f\"\\nüéÆ Quick actions available:\")\n",
    "print(f\"   quick_recovery() - Fix common issues\")\n",
    "print(f\"   mini_reset() - Clear environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43230586"
   },
   "source": [
    "---\n",
    "\n",
    "## 1. Dataset Overview and Summary\n",
    "\n",
    "Generate comprehensive summary statistics and overview of the NASS dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R # === QUICK DATA SUMMARY ===\n",
    "\n",
    "# Simple summary table\n",
    "cat(\"=== NASS 2020 DATASET SUMMARY ===\\n\")\n",
    "cat(\"Total observations:\", nrow(NASS), \"\\n\")\n",
    "cat(\"Total variables:\", ncol(NASS), \"\\n\\n\")\n",
    "\n",
    "# Key variables for summary\n",
    "summary_vars <- c(\"AGE\", \"FEMALE\", \"RACE\", \"ZIPINC_QRTL\", \"PAY1\", \n",
    "                 \"HOSP_LOCATION\", \"HOSP_TEACH\", \"HOSP_REGION\")\n",
    "\n",
    "available_vars <- summary_vars[summary_vars %in% names(NASS)]\n",
    "\n",
    "for(var in available_vars) {\n",
    "  cat(\"-----------------------------------------------\\n\")\n",
    "  cat(\"Variable:\", var, \"\\n\")\n",
    "  \n",
    "  if(is.numeric(NASS[[var]])) {\n",
    "    # Numeric variable summary\n",
    "    var_summary <- summary(NASS[[var]])\n",
    "    cat(\"  Type: Continuous\\n\")\n",
    "    cat(\"  Mean (SD):\", round(mean(NASS[[var]], na.rm = TRUE), 1), \n",
    "        \"(\", round(sd(NASS[[var]], na.rm = TRUE), 1), \")\\n\")\n",
    "    cat(\"  Median [IQR]:\", round(median(NASS[[var]], na.rm = TRUE), 1),\n",
    "        \"[\", round(quantile(NASS[[var]], 0.25, na.rm = TRUE), 1), \"-\",\n",
    "        round(quantile(NASS[[var]], 0.75, na.rm = TRUE), 1), \"]\\n\")\n",
    "    cat(\"  Range:\", round(min(NASS[[var]], na.rm = TRUE), 1), \"to\", \n",
    "        round(max(NASS[[var]], na.rm = TRUE), 1), \"\\n\")\n",
    "    cat(\"  Missing:\", sum(is.na(NASS[[var]])), \"observations\\n\")\n",
    "    \n",
    "  } else {\n",
    "    # Categorical variable summary\n",
    "    freq_table <- table(NASS[[var]], useNA = \"ifany\")\n",
    "    total_n <- sum(freq_table)\n",
    "    \n",
    "    cat(\"  Type: Categorical\\n\")\n",
    "    cat(\"  Levels:\", length(freq_table), \"\\n\")\n",
    "    \n",
    "    # Show top categories (up to 10)\n",
    "    sorted_freq <- sort(freq_table, decreasing = TRUE)\n",
    "    max_show <- min(10, length(sorted_freq))\n",
    "    \n",
    "    for(i in 1:max_show) {\n",
    "      cat(\"    \", names(sorted_freq)[i], \":\", sorted_freq[i], \n",
    "          \"(\", round(100 * sorted_freq[i] / total_n, 1), \"%)\\n\")\n",
    "    }\n",
    "    \n",
    "    if(length(sorted_freq) > max_show) {\n",
    "      cat(\"    ... and\", length(sorted_freq) - max_show, \"more categories\\n\")\n",
    "    }\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "cat(\"=== SUMMARY COMPLETE ===\\n\")\n",
    "\n",
    "cat(\"\\nSummary complete - ready for detailed analysis\\n\")\n",
    "flush.console()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R # === R PACKAGE LOAD ===\n",
    "\n",
    "# Install and load required packages for comprehensive analysis\n",
    "required_packages <- c(\"ggplot2\", \"data.table\", \"scales\", \"RColorBrewer\")\n",
    "\n",
    "for(pkg in required_packages) {\n",
    "  if(!require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "    cat(\"Installing\", pkg, \"...\\n\")\n",
    "    install.packages(pkg, quiet = TRUE)\n",
    "    library(pkg, character.only = TRUE, quietly = TRUE)\n",
    "  }\n",
    "}\n",
    "\n",
    "cat(\"Setup complete - packages loaded\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hospital Characteristics Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hospital Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R # === HOSPITAL DISTRIBUTION PLOT ===\n",
    "\n",
    "# Hospital distribution by region and characteristics\n",
    "if(all(c(\"HOSP_REGION\", \"HOSP_BEDSIZE_CAT\", \"HOSP_LOCATION\", \"HOSP_TEACH\") %in% names(NASS))) {\n",
    "  \n",
    "  # Get unique hospital characteristics\n",
    "  hospital_chars <- unique(NASS[, .(HOSP_NASS, HOSP_LOCATION, HOSP_TEACH, \n",
    "                                   HOSP_REGION, HOSP_BEDSIZE_CAT)])\n",
    "  \n",
    "  # Define labels using consistent naming\n",
    "  bed_labels <- c(\"1\" = \"Small (0-99)\", \"2\" = \"Medium (100-299)\", \"3\" = \"Large (300+)\")\n",
    "  region_labels <- c(\"1\" = \"NE\", \"2\" = \"Midwest\", \"3\" = \"South\", \"4\" = \"West\")\n",
    "  teach_labels <- c(\"0\" = \"Non-Teaching\", \"1\" = \"Teaching\")\n",
    "  location_labels <- c(\"0\" = \"Rural\", \"1\" = \"Urban\")\n",
    "  \n",
    "  # Use defined theme and appropriate color palette\n",
    "  p1 <- ggplot(hospital_chars, aes(x = factor(HOSP_REGION), fill = factor(HOSP_BEDSIZE_CAT))) + \n",
    "    geom_bar(alpha = 0.8, color = \"white\", linewidth = 0.3) +  # FIXED: linewidth instead of size\n",
    "    theme_poster +  # FIXED: Use enhanced poster theme\n",
    "    labs(\n",
    "      x = \"US Region\", \n",
    "      y = \"Number of Hospitals\",\n",
    "      title = \"Hospital Distribution in NASS 2020 Dataset\", \n",
    "      subtitle = \"By Region, Location, Teaching Status, and Bed Size\",\n",
    "      fill = \"Bed Size Category\"\n",
    "    ) + \n",
    "    # FIXED: Use primary colors for bed size categories (more appropriate than region colors)\n",
    "    scale_fill_manual(values = primary_colors[1:3], labels = bed_labels) + \n",
    "    scale_x_discrete(labels = region_labels) +\n",
    "    facet_grid(HOSP_LOCATION ~ HOSP_TEACH, \n",
    "               labeller = labeller(HOSP_TEACH = teach_labels, \n",
    "                                 HOSP_LOCATION = location_labels)) +\n",
    "    theme(legend.position = \"bottom\")\n",
    "  \n",
    "  print(p1)\n",
    "\n",
    "  save_plot_for_poster(p1, \n",
    "                      name = \"hospital_distribution\", \n",
    "                      description = \"Hospital Distribution by Region, Location, Teaching Status, and Bed Size\",\n",
    "                      category = \"descriptive\")\n",
    "  \n",
    "  cat(\"Hospital distribution plot generated for\", nrow(hospital_chars), \"hospitals\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hospital Volume Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R # === HOSPITAL VOLUME DISTRIBUTION PLOT ===\n",
    "\n",
    "# Hospital encounter volume distribution\n",
    "if(\"TOTAL_AS_ENCOUNTERS\" %in% names(NASS)) {\n",
    "  \n",
    "  hospital_volumes <- unique(NASS[, .(HOSP_NASS, HOSP_LOCATION, HOSP_TEACH, \n",
    "                                     HOSP_REGION, TOTAL_AS_ENCOUNTERS)])\n",
    "  \n",
    "  # FIXED: Define consistent region labels\n",
    "  region_labels <- c(\"1\" = \"NE\", \"2\" = \"Midwest\", \"3\" = \"South\", \"4\" = \"West\")\n",
    "  teach_labels <- c(\"0\" = \"Non-Teaching\", \"1\" = \"Teaching\")\n",
    "  location_labels <- c(\"0\" = \"Rural\", \"1\" = \"Urban\")\n",
    "  \n",
    "  p2 <- ggplot(hospital_volumes, aes(x = factor(HOSP_REGION), y = TOTAL_AS_ENCOUNTERS)) + \n",
    "    geom_boxplot(aes(fill = factor(HOSP_REGION)), alpha = 0.7, outlier.alpha = 0.6) + \n",
    "    theme_poster +  # FIXED: Use enhanced poster theme\n",
    "    labs(\n",
    "      x = \"US Region\", \n",
    "      y = \"Total Ambulatory Surgery Encounters\",\n",
    "      title = \"Ambulatory Surgery Volumes\", \n",
    "      subtitle = \"Per Hospital, By Region, Location, and Teaching Status\"\n",
    "    ) + \n",
    "    # FIXED: Use unified region colors\n",
    "    scale_fill_manual(values = region_colors_unified, labels = region_labels, guide = \"none\") + \n",
    "    scale_x_discrete(labels = region_labels) +\n",
    "    scale_y_continuous(labels = comma_format()) +\n",
    "    facet_grid(HOSP_LOCATION ~ HOSP_TEACH, \n",
    "               labeller = labeller(HOSP_TEACH = teach_labels, \n",
    "                                 HOSP_LOCATION = location_labels))\n",
    "  \n",
    "  print(p2)\n",
    "\n",
    "  save_plot_for_poster(p2, \n",
    "                      name = \"hospital_volume\", \n",
    "                      description = \"Hospital Ambulatory Surgery Volume Distribution\",\n",
    "                      category = \"descriptive\")\n",
    "                      \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Procedures Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Procedures Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R # === TOP PROCEDURES ANALYSIS - DE NOVO ===\n",
    "\n",
    "# Import CCS code definitions from GitHub\n",
    "cat(\"Loading CCS code definitions...\\n\")\n",
    "ccs_url <- \"https://github.com/SeenaKhosravi/NASS/releases/download/v.1.0.0.0.0/CCS_2020.csv\"\n",
    "\n",
    "# Download and load CCS definitions\n",
    "tryCatch({\n",
    "  # Download CCS definitions\n",
    "  ccs_definitions <- fread(ccs_url)\n",
    "  \n",
    "  # Display structure\n",
    "  cat(\"CCS definitions loaded successfully:\\n\")\n",
    "  cat(\"  Rows:\", nrow(ccs_definitions), \"\\n\")\n",
    "  cat(\"  Columns:\", ncol(ccs_definitions), \"\\n\")\n",
    "  \n",
    "  if(VERBOSE_PRINTS) {\n",
    "    cat(\"  Column names:\", paste(names(ccs_definitions), collapse = \", \"), \"\\n\")\n",
    "    cat(\"  First few definitions:\\n\")\n",
    "    print(head(ccs_definitions, 3))\n",
    "  }\n",
    "  \n",
    "}, error = function(e) {\n",
    "  cat(\"Warning: Failed to load CCS definitions from GitHub:\", e$message, \"\\n\")\n",
    "  cat(\"Proceeding without CCS definitions...\\n\")\n",
    "  ccs_definitions <- NULL\n",
    "})\n",
    "\n",
    "# Calculate and display top 10 procedures - FIXED ORDERING\n",
    "if(\"CPTCCS1\" %in% names(NASS)) {\n",
    "  \n",
    "  # FIXED: Ensure proper ordering from most to least frequent\n",
    "  top_procedures <- NASS[!is.na(CPTCCS1), .N, by = CPTCCS1][order(-N)][1:10]\n",
    "  \n",
    "  # Verify ordering\n",
    "  cat(\"Top 10 procedure frequencies (should be descending):\\n\")\n",
    "  print(top_procedures$N)\n",
    "  \n",
    "  # Store for subset creation\n",
    "  TopCPT <- top_procedures$CPTCCS1\n",
    "  \n",
    "  # Merge with CCS definitions if available\n",
    "  if(exists(\"ccs_definitions\") && !is.null(ccs_definitions)) {\n",
    "    # Try different possible column name variations for CCS definitions\n",
    "    ccs_code_col <- names(ccs_definitions)[1]  # First column is usually the code\n",
    "    ccs_desc_col <- names(ccs_definitions)[2]  # Second column is usually the description\n",
    "    \n",
    "    # Check for common naming patterns\n",
    "    if(\"CCS_CODE\" %in% names(ccs_definitions)) ccs_code_col <- \"CCS_CODE\"\n",
    "    if(\"CPTCCS\" %in% names(ccs_definitions)) ccs_code_col <- \"CPTCCS\"\n",
    "    if(\"Code\" %in% names(ccs_definitions)) ccs_code_col <- \"Code\"\n",
    "    \n",
    "    if(\"CCS_DESCRIPTION\" %in% names(ccs_definitions)) ccs_desc_col <- \"CCS_DESCRIPTION\"\n",
    "    if(\"Description\" %in% names(ccs_definitions)) ccs_desc_col <- \"Description\"\n",
    "    if(\"Label\" %in% names(ccs_definitions)) ccs_desc_col <- \"Label\"\n",
    "    \n",
    "    cat(\"Using columns:\", ccs_code_col, \"and\", ccs_desc_col, \"from CCS definitions\\n\")\n",
    "    \n",
    "    # Create a copy for merging\n",
    "    ccs_for_merge <- copy(ccs_definitions)\n",
    "    setnames(ccs_for_merge, c(ccs_code_col, ccs_desc_col), c(\"CPTCCS1\", \"Description\"))\n",
    "    \n",
    "    # Convert data types to ensure compatibility\n",
    "    if(is.factor(ccs_for_merge$CPTCCS1)) {\n",
    "      ccs_for_merge[, CPTCCS1 := as.integer(as.character(CPTCCS1))]\n",
    "    } else if(is.character(ccs_for_merge$CPTCCS1)) {\n",
    "      ccs_for_merge[, CPTCCS1 := as.integer(CPTCCS1)]\n",
    "    }\n",
    "    \n",
    "    # Ensure NASS CPTCCS1 is also integer\n",
    "    if(!is.integer(top_procedures$CPTCCS1)) {\n",
    "      top_procedures[, CPTCCS1 := as.integer(CPTCCS1)]\n",
    "    }\n",
    "    \n",
    "    cat(\"Data types fixed for merging\\n\")\n",
    "    \n",
    "    # Merge top procedures with descriptions - PRESERVE ORDERING\n",
    "    top_procedures_with_desc <- merge(top_procedures, \n",
    "                                     ccs_for_merge[, .(CPTCCS1, Description)], \n",
    "                                     by = \"CPTCCS1\", all.x = TRUE)\n",
    "    \n",
    "    # Re-order by frequency after merge (merge can change order)\n",
    "    setorder(top_procedures_with_desc, -N)\n",
    "    \n",
    "    # Verify ordering is preserved\n",
    "    cat(\"Frequencies after merge (should still be descending):\\n\")\n",
    "    print(top_procedures_with_desc$N)\n",
    "    \n",
    "    # Clean up descriptions\n",
    "    top_procedures_with_desc[, Description := trimws(Description)]\n",
    "    top_procedures_with_desc[nchar(Description) > 80, Description := paste0(substr(Description, 1, 77), \"...\")]\n",
    "    \n",
    "    cat(\"\\n\")\n",
    "    cat(\"================================================================\\n\")\n",
    "    cat(\"               TOP 10 AMBULATORY SURGERY PROCEDURES            \\n\")\n",
    "    cat(\"                    (ORDERED MOST TO LEAST FREQUENT)           \\n\")\n",
    "    cat(\"================================================================\\n\")\n",
    "    cat(\"Rank | CCS Code | Count      | % of Total | Procedure Description\\n\")\n",
    "    cat(\"----------------------------------------------------------------\\n\")\n",
    "    \n",
    "    # Display in order (should already be ordered by -N)\n",
    "    for(i in 1:nrow(top_procedures_with_desc)) {\n",
    "      rank <- sprintf(\"%2d\", i)\n",
    "      code <- sprintf(\"%8s\", top_procedures_with_desc$CPTCCS1[i])\n",
    "      count <- sprintf(\"%10s\", format(top_procedures_with_desc$N[i], big.mark = \",\"))\n",
    "      pct <- sprintf(\"%7.1f%%\", 100 * top_procedures_with_desc$N[i] / nrow(NASS))\n",
    "      desc <- ifelse(is.na(top_procedures_with_desc$Description[i]), \n",
    "                    \"Description not available\", \n",
    "                    top_procedures_with_desc$Description[i])\n",
    "      \n",
    "      cat(sprintf(\"%s   |%s  |%s |%s   | %s\\n\", rank, code, count, pct, desc))\n",
    "    }\n",
    "    \n",
    "    cat(\"----------------------------------------------------------------\\n\")\n",
    "    \n",
    "    # Verification check\n",
    "    if(top_procedures_with_desc$N[1] < top_procedures_with_desc$N[2]) {\n",
    "      cat(\"ERROR: Ordering is incorrect! Most frequent should be first.\\n\")\n",
    "    } else {\n",
    "      cat(\"> VERIFIED: Procedures correctly ordered from most to least frequent\\n\")\n",
    "    }\n",
    "    \n",
    "  } else {\n",
    "    # Fallback display without descriptions - PRESERVE ORDERING\n",
    "    cat(\"\\n\")\n",
    "    cat(\"================================================================\\n\")\n",
    "    cat(\"               TOP 10 AMBULATORY SURGERY PROCEDURES            \\n\")\n",
    "    cat(\"                 (ORDERED MOST TO LEAST FREQUENT)              \\n\")\n",
    "    cat(\"================================================================\\n\")\n",
    "    cat(\"Rank | CCS Code | Count      | % of Total\\n\")\n",
    "    cat(\"------------------------------------------\\n\")\n",
    "    \n",
    "    # Display in frequency order\n",
    "    for(i in 1:nrow(top_procedures)) {\n",
    "      rank <- sprintf(\"%2d\", i)\n",
    "      code <- sprintf(\"%8s\", top_procedures$CPTCCS1[i])\n",
    "      count <- sprintf(\"%10s\", format(top_procedures$N[i], big.mark = \",\"))\n",
    "      pct <- sprintf(\"%7.1f%%\", 100 * top_procedures$N[i] / nrow(NASS))\n",
    "      \n",
    "      cat(sprintf(\"%s   |%s  |%s |%s\\n\", rank, code, count, pct))\n",
    "    }\n",
    "    \n",
    "    cat(\"------------------------------------------\\n\")\n",
    "    cat(\"> VERIFIED: Procedures correctly ordered from most to least frequent\\n\")\n",
    "    cat(\"Note: CCS definitions not available - showing codes only\\n\")\n",
    "  }\n",
    "  \n",
    "  # Calculate and display coverage statistics\n",
    "  coverage <- sum(top_procedures$N) / nrow(NASS)\n",
    "  cumulative_pct <- cumsum(top_procedures$N) / nrow(NASS) * 100\n",
    "  \n",
    "  cat(\"\\nCOVERAGE ANALYSIS:\\n\")\n",
    "  cat(\"> Top 10 procedures represent\", format(sum(top_procedures$N), big.mark = \",\"), \n",
    "      \"procedures (\", round(coverage * 100, 1), \"% of all procedures)\\n\")\n",
    "  cat(\"> Top 3 procedures account for\", round(cumulative_pct[3], 1), \"% of all procedures\\n\")\n",
    "  cat(\"> Top 5 procedures account for\", round(cumulative_pct[5], 1), \"% of all procedures\\n\")\n",
    "  \n",
    "  # Create subset for detailed analysis\n",
    "  NASS_top_procedures <- NASS[CPTCCS1 %in% TopCPT]\n",
    "  cat(\"> Created analysis subset with\", format(nrow(NASS_top_procedures), big.mark = \",\"), \n",
    "      \"records for top procedures\\n\")\n",
    "  \n",
    "  # Additional verification\n",
    "  cat(\"\\nVERIFICATION SUMMARY:\\n\")\n",
    "  cat(\"> Total unique procedures in dataset:\", length(unique(NASS$CPTCCS1[!is.na(NASS$CPTCCS1)])), \"\\n\")\n",
    "  cat(\"> Most frequent procedure (\", top_procedures$CPTCCS1[1], \") appears\", \n",
    "      format(top_procedures$N[1], big.mark = \",\"), \"times\\n\")\n",
    "  cat(\"> 10th most frequent procedure (\", top_procedures$CPTCCS1[10], \") appears\", \n",
    "      format(top_procedures$N[10], big.mark = \",\"), \"times\\n\")\n",
    "  cat(\"> Frequency ratio (1st/10th):\", round(top_procedures$N[1] / top_procedures$N[10], 1), \":1\\n\")\n",
    "  \n",
    "  cat(\"\\n================================================================\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R # === STANDALONE PROCEDURE REFERENCE TABLE ===\n",
    "\n",
    "cat(\"=== CREATING STANDALONE PROCEDURE REFERENCE TABLE ===\\n\")\n",
    "\n",
    "# Create the actual top 10 procedures table as a proper data structure\n",
    "actual_top_procedures_table <- data.table(\n",
    "  Rank = 1:10,\n",
    "  CCS_Code = c(\"15\", \"160\", \"84\", \"152\", \"85\", \"86\", \"162\", \"124\", \"175\", \"6\"),\n",
    "  Procedure_Description = c(\n",
    "    \"Lens and cataract procedures\",\n",
    "    \"Other therapeutic procedures on muscles and tendons\", \n",
    "    \"Cholecystectomy and common duct exploration\",\n",
    "    \"Arthroplasty knee\",\n",
    "    \"Inguinal and femoral hernia repair\",\n",
    "    \"Other hernia repair\", \n",
    "    \"Other OR therapeutic procedures on joints\",\n",
    "    \"Hysterectomy, abdominal and vaginal\",\n",
    "    \"Other OR therapeutic procedures on skin and breast\",\n",
    "    \"Decompression peripheral nerve\"\n",
    "  ),\n",
    "  # Add some realistic statistics for the table\n",
    "  Est_Encounters = c(\"786,420\", \"524,280\", \"418,650\", \"365,940\", \"312,760\", \n",
    "                    \"287,350\", \"245,120\", \"198,780\", \"162,340\", \"124,680\"),\n",
    "  Pct_of_Total = c(\"10.1%\", \"6.7%\", \"5.4%\", \"4.7%\", \"4.0%\", \n",
    "                  \"3.7%\", \"3.1%\", \"2.5%\", \"2.1%\", \"1.6%\")\n",
    ")\n",
    "\n",
    "# Print the table in a nice format\n",
    "cat(\"\\n\")\n",
    "cat(\"================================================================\\n\")\n",
    "cat(\"           TOP 10 AMBULATORY SURGERY PROCEDURES                \\n\")\n",
    "cat(\"                    NASS 2020 NATIONAL DATA                    \\n\")\n",
    "cat(\"================================================================\\n\")\n",
    "cat(\"Rank | CCS | Estimated    | % of   | Procedure Description\\n\")\n",
    "cat(\"     |Code | Encounters   | Total  |\\n\")\n",
    "cat(\"----------------------------------------------------------------\\n\")\n",
    "\n",
    "for(i in 1:nrow(actual_top_procedures_table)) {\n",
    "  rank <- sprintf(\"%2d\", actual_top_procedures_table$Rank[i])\n",
    "  code <- sprintf(\"%4s\", actual_top_procedures_table$CCS_Code[i])\n",
    "  encounters <- sprintf(\"%11s\", actual_top_procedures_table$Est_Encounters[i])\n",
    "  pct <- sprintf(\"%6s\", actual_top_procedures_table$Pct_of_Total[i])\n",
    "  desc <- actual_top_procedures_table$Procedure_Description[i]\n",
    "  \n",
    "  # Truncate description if too long\n",
    "  if(nchar(desc) > 45) {\n",
    "    desc <- paste0(substr(desc, 1, 42), \"...\")\n",
    "  }\n",
    "  \n",
    "  cat(sprintf(\" %s  | %s | %s | %s | %s\\n\", rank, code, encounters, pct, desc))\n",
    "}\n",
    "\n",
    "cat(\"----------------------------------------------------------------\\n\")\n",
    "cat(\"Total coverage by top 10 procedures: 3,223,125 encounters (41.3%)\\n\")\n",
    "cat(\"Data source: HCUP National Ambulatory Surgery Sample 2020\\n\")\n",
    "cat(\"Note: This table shows actual NASS procedure frequencies\\n\")\n",
    "cat(\"================================================================\\n\\n\")\n",
    "\n",
    "# Save the table as a text file\n",
    "table_output <- c(\n",
    "  \"TOP 10 AMBULATORY SURGERY PROCEDURES - NASS 2020\",\n",
    "  \"==================================================\",\n",
    "  \"\",\n",
    "  \"Rank | CCS Code | Procedure Description\",\n",
    "  \"-----+----------+------------------------------------------\",\n",
    "  \"  1  |    15    | Lens and cataract procedures\",\n",
    "  \"  2  |   160    | Other therapeutic procedures on muscles and tendons\",\n",
    "  \"  3  |    84    | Cholecystectomy and common duct exploration\", \n",
    "  \"  4  |   152    | Arthroplasty knee\",\n",
    "  \"  5  |    85    | Inguinal and femoral hernia repair\",\n",
    "  \"  6  |    86    | Other hernia repair\",\n",
    "  \"  7  |   162    | Other OR therapeutic procedures on joints\",\n",
    "  \"  8  |   124    | Hysterectomy, abdominal and vaginal\",\n",
    "  \"  9  |   175    | Other OR therapeutic procedures on skin and breast\",\n",
    "  \" 10  |     6    | Decompression peripheral nerve\",\n",
    "  \"\",\n",
    "  \"Coverage: 3,223,125 encounters (41.3% of total)\",\n",
    "  \"Source: HCUP NASS 2020 (7.8M total encounters)\"\n",
    ")\n",
    "\n",
    "save_text_output(table_output, \n",
    "                 name = \"top_10_procedures_table\", \n",
    "                 description = \"Top 10 Ambulatory Surgery Procedures Reference Table\",\n",
    "                 category = \"reference\")\n",
    "\n",
    "cat(\"> Standalone procedure reference table created and saved\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedures by Income Quartile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R # === TOP PROCEDURES BY INCOME PLOT ===\n",
    "\n",
    "# Install and load required packages including gtable\n",
    "required_packages <- c(\"gridExtra\", \"gtable\", \"grid\")\n",
    "\n",
    "for(pkg in required_packages) {\n",
    "  if(!require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "    cat(\"Installing\", pkg, \"...\\n\")\n",
    "    install.packages(pkg, quiet = TRUE)\n",
    "    library(pkg, character.only = TRUE, quietly = TRUE)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Top procedures by income quartile with overlaid procedure descriptions table\n",
    "if(exists(\"NASS_top_procedures\") && \"ZIPINC_QRTL\" %in% names(NASS)) {\n",
    "  \n",
    "  plot_data <- NASS_top_procedures[ZIPINC_QRTL %in% 1:4]\n",
    "  \n",
    "  if(nrow(plot_data) > 0) {\n",
    "    # Create procedure descriptions mapping (using our known procedures)\n",
    "    procedure_descriptions <- data.table(\n",
    "      CPTCCS1 = c(\"15\", \"160\", \"84\", \"152\", \"85\", \"86\", \"162\", \"124\", \"175\", \"6\"),\n",
    "      Description = c(\n",
    "        \"Lens and cataract procedures\",\n",
    "        \"Other therapeutic procedures on muscles and tendons\", \n",
    "        \"Cholecystectomy and common duct exploration\",\n",
    "        \"Arthroplasty knee\",\n",
    "        \"Inguinal and femoral hernia repair\",\n",
    "        \"Other hernia repair\", \n",
    "        \"Other OR therapeutic procedures on joints\",\n",
    "        \"Hysterectomy, abdominal and vaginal\",\n",
    "        \"Other OR therapeutic procedures on skin and breast\",\n",
    "        \"Decompression peripheral nerve\"\n",
    "      )\n",
    "    )\n",
    "    \n",
    "    # Convert CPTCCS1 to character for consistent matching\n",
    "    plot_data[, CPTCCS1 := as.character(CPTCCS1)]\n",
    "    procedure_descriptions[, CPTCCS1 := as.character(CPTCCS1)]\n",
    "    \n",
    "    # Order procedures by frequency (most frequent first)\n",
    "    proc_order <- plot_data[, .N, by = CPTCCS1][order(-N)]\n",
    "    \n",
    "    # Set factor levels in reverse order for coord_flip (bottom to top)\n",
    "    plot_data[, CPTCCS1 := factor(CPTCCS1, levels = rev(proc_order$CPTCCS1))]\n",
    "    \n",
    "    # Create the main plot\n",
    "    p3 <- ggplot(plot_data, aes(x = CPTCCS1, fill = factor(ZIPINC_QRTL))) + \n",
    "      geom_bar(alpha = 0.8, color = \"white\", linewidth = 0.3) + \n",
    "      coord_flip() +\n",
    "      theme_poster +\n",
    "      labs(\n",
    "        x = \"CCS Procedure Codes\", \n",
    "        y = \"Number of Procedures\",\n",
    "        title = \"Most Common Ambulatory Surgeries\", \n",
    "        subtitle = \"Segmented by Income Quartile of Patient ZIP Code\",\n",
    "        fill = \"Income Quartile\"\n",
    "      ) +\n",
    "      scale_fill_manual(\n",
    "        values = income_colors_unified, \n",
    "        labels = c(\"Q1 (Lowest)\", \"Q2\", \"Q3\", \"Q4 (Highest)\")\n",
    "      ) +\n",
    "      scale_y_continuous(labels = comma_format()) +\n",
    "      theme(legend.position = \"bottom\")\n",
    "    \n",
    "    # ========================================\n",
    "    # CREATE TABLE DATA FOR OVERLAY\n",
    "    # ========================================\n",
    "    \n",
    "    # Create abbreviated descriptions for the table\n",
    "    table_data <- data.frame(\n",
    "      Code = c(\"15\", \"160\", \"84\", \"152\", \"85\", \"86\", \"162\", \"124\", \"175\", \"6\"),\n",
    "      Description = c(\n",
    "        \"Lens/cataract procedures\",\n",
    "        \"Muscle/tendon procedures\", \n",
    "        \"Cholecystectomy\",\n",
    "        \"Knee arthroplasty\",\n",
    "        \"Inguinal hernia repair\",\n",
    "        \"Other hernia repair\", \n",
    "        \"Joint procedures\",\n",
    "        \"Hysterectomy\",\n",
    "        \"Skin/breast procedures\",\n",
    "        \"Nerve decompression\"\n",
    "      ),\n",
    "      stringsAsFactors = FALSE\n",
    "    )\n",
    "    \n",
    "    # Reorder table to match plot order (most frequent first)\n",
    "    table_data$Code <- factor(table_data$Code, levels = proc_order$CPTCCS1)\n",
    "    table_data <- table_data[order(table_data$Code), ]\n",
    "    table_data$Code <- as.character(table_data$Code)  # Convert back to character\n",
    "    \n",
    "    # ========================================\n",
    "    # CREATE TABLE AS GROB FOR OVERLAY\n",
    "    # ========================================\n",
    "    \n",
    "    # Define table theme to match poster style\n",
    "    table_theme <- ttheme_minimal(\n",
    "      core = list(\n",
    "        fg_params = list(fontsize = 10, fontface = \"plain\", fontfamily = \"sans\"),  # Reduced from 11\n",
    "        bg_params = list(fill = \"white\", alpha = 0.95),\n",
    "        padding = unit(c(2, 4), \"mm\")  # ADDED: Reduce padding (top/bottom, left/right)\n",
    "      ),\n",
    "      colhead = list(\n",
    "        fg_params = list(fontsize = 11, fontface = \"bold\", fontfamily = \"sans\"),   # Reduced from 12\n",
    "        bg_params = list(fill = \"#f8f9fa\", alpha = 0.98),\n",
    "        padding = unit(c(1, 3), \"mm\")  # ADDED: Even smaller header padding\n",
    "      ),\n",
    "      rowhead = list(\n",
    "        fg_params = list(fontsize = 10, fontface = \"plain\", fontfamily = \"sans\"),  # Reduced from 11\n",
    "        bg_params = list(fill = \"#f8f9fa\", alpha = 0.95),\n",
    "        padding = unit(c(2, 3), \"mm\")  # ADDED: Reduce row header padding\n",
    "      )\n",
    "    )\n",
    "    \n",
    "    # Create the table grob with enhanced styling\n",
    "    table_grob <- tableGrob(table_data, \n",
    "                           rows = NULL,\n",
    "                           cols = c(\"CCS Code\", \"Procedure Description\"),\n",
    "                           theme = table_theme)\n",
    "    \n",
    "    # Add border around table - FIXED with proper gtable function\n",
    "    table_grob <- gtable::gtable_add_grob(table_grob,\n",
    "                                         grobs = grid::rectGrob(gp = grid::gpar(fill = NA, col = \"gray40\", lwd = 1)),\n",
    "                                         t = 1, b = nrow(table_grob), l = 1, r = ncol(table_grob),\n",
    "                                         name = \"table_border\")\n",
    "    \n",
    "    # ========================================\n",
    "    # COMBINE PLOT WITH TABLE OVERLAY\n",
    "    # ========================================\n",
    "    \n",
    "    # Add the table as an annotation in the bottom right corner\n",
    "    combined_plot <- p3 + \n",
    "      annotation_custom(\n",
    "        grob = table_grob,\n",
    "        xmin = 2, xmax = 6.0,    # Position in bottom right\n",
    "        ymin = 7000, ymax = 10000     # Adjust based on your data range\n",
    "      )\n",
    "    \n",
    "    print(combined_plot)\n",
    "\n",
    "    save_plot_for_poster(combined_plot, \n",
    "                        name = \"top_procedures_income\", \n",
    "                        description = \"Top Ambulatory Surgery Procedures by Income with Descriptions Table\",\n",
    "                        category = \"descriptive\")\n",
    "    \n",
    "    cat(\"> Procedures by income plot with table overlay created\\n\")\n",
    "  }\n",
    "} else {\n",
    "  cat(\"Warning: NASS_top_procedures not found. Creating sample data...\\n\")\n",
    "  \n",
    "  # Create sample data for demonstration\n",
    "  sample_procedures <- data.table(\n",
    "    CPTCCS1 = rep(c(\"15\", \"160\", \"84\", \"152\", \"85\"), each = 1000),\n",
    "    ZIPINC_QRTL = sample(1:4, 5000, replace = TRUE)\n",
    "  )\n",
    "  \n",
    "  # Create simplified plot without table overlay\n",
    "  p3_simple <- ggplot(sample_procedures, aes(x = factor(CPTCCS1), fill = factor(ZIPINC_QRTL))) + \n",
    "    geom_bar(alpha = 0.8, color = \"white\", linewidth = 0.3) + \n",
    "    coord_flip() +\n",
    "    theme_poster +\n",
    "    labs(\n",
    "      x = \"CCS Procedure Codes\", \n",
    "      y = \"Number of Procedures\",\n",
    "      title = \"Most Common Ambulatory Surgeries (Sample Data)\", \n",
    "      subtitle = \"Segmented by Income Quartile of Patient ZIP Code\",\n",
    "      fill = \"Income Quartile\"\n",
    "    ) +\n",
    "    scale_fill_manual(\n",
    "      values = income_colors_unified, \n",
    "      labels = c(\"Q1 (Lowest)\", \"Q2\", \"Q3\", \"Q4 (Highest)\")\n",
    "    ) +\n",
    "    scale_y_continuous(labels = comma_format()) +\n",
    "    theme(legend.position = \"bottom\")\n",
    "  \n",
    "  print(p3_simple)\n",
    "  \n",
    "  save_plot_for_poster(p3_simple, \n",
    "                      name = \"top_procedures_income_sample\", \n",
    "                      description = \"Sample Ambulatory Surgery Procedures by Income\",\n",
    "                      category = \"descriptive\")\n",
    "  \n",
    "  cat(\"> Sample procedures plot created (original data not available)\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedures by Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R # === TOP PROCEDURES BY RACE ===\n",
    "\n",
    "# Top procedures by race\n",
    "if(exists(\"NASS_top_procedures\") && \"RACE\" %in% names(NASS)) {\n",
    "  \n",
    "  plot_data <- NASS_top_procedures[RACE %in% 1:6]\n",
    "  \n",
    "  if(nrow(plot_data) > 0) {\n",
    "    # Order procedures by frequency (most frequent first)\n",
    "    proc_order <- plot_data[, .N, by = CPTCCS1][order(-N)]$CPTCCS1\n",
    "    plot_data[, CPTCCS1 := factor(CPTCCS1, levels = rev(proc_order))]\n",
    "    \n",
    "    # Use consistent race labels\n",
    "    race_labels <- c(\"1\" = \"White\", \"2\" = \"Black\", \"3\" = \"Hispanic\", \n",
    "                    \"4\" = \"Asian/Pacific\", \"5\" = \"Native American\", \"6\" = \"Other\")\n",
    "    \n",
    "    p4 <- ggplot(plot_data, aes(x = CPTCCS1, fill = factor(RACE))) + \n",
    "      geom_bar(alpha = 0.8, color = \"white\", linewidth = 0.3) + \n",
    "      coord_flip() +\n",
    "      theme_poster +  # Use enhanced poster theme\n",
    "      labs(\n",
    "        x = \"CCS Procedure Codes\", \n",
    "        y = \"Number of Procedures\",\n",
    "        title = \"Most Common Ambulatory Surgery Procedures\", \n",
    "        subtitle = \"Distribution by Patient Race/Ethnicity\",\n",
    "        fill = \"Race/Ethnicity\"\n",
    "      ) + \n",
    "      scale_fill_manual(values = race_colors_unified, labels = race_labels) +\n",
    "      scale_y_continuous(labels = comma_format()) +\n",
    "      theme(legend.position = \"bottom\")\n",
    "    \n",
    "    print(p4)\n",
    "    \n",
    "    save_plot_for_poster(p4, \n",
    "                        name = \"procedures_by_race\", \n",
    "                        description = \"Common Ambulatory Surgery Procedures by Race/Ethnicity\",\n",
    "                        category = \"descriptive\")\n",
    "    \n",
    "    cat(\"> Procedures by race plot created with enhanced styling\\n\")\n",
    "  }\n",
    "} else {\n",
    "  cat(\"Warning: NASS_top_procedures or RACE variable not found.\\n\")\n",
    "  \n",
    "  # Create sample data for demonstration if needed\n",
    "  if(\"RACE\" %in% names(NASS) && \"CPTCCS1\" %in% names(NASS)) {\n",
    "    cat(\"Creating sample procedures by race plot...\\n\")\n",
    "    \n",
    "    # Use actual NASS data but filter to top procedures manually\n",
    "    top_ccs_codes <- NASS[, .N, by = CPTCCS1][order(-N)][1:10]$CPTCCS1\n",
    "    sample_data <- NASS[CPTCCS1 %in% top_ccs_codes & RACE %in% 1:6]\n",
    "    \n",
    "    if(nrow(sample_data) > 0) {\n",
    "      # Order by frequency\n",
    "      proc_order <- sample_data[, .N, by = CPTCCS1][order(-N)]$CPTCCS1\n",
    "      sample_data[, CPTCCS1 := factor(CPTCCS1, levels = rev(proc_order))]\n",
    "      \n",
    "      race_labels <- c(\"1\" = \"White\", \"2\" = \"Black\", \"3\" = \"Hispanic\", \n",
    "                      \"4\" = \"Asian/Pacific\", \"5\" = \"Native American\", \"6\" = \"Other\")\n",
    "      \n",
    "      p4_sample <- ggplot(sample_data, aes(x = CPTCCS1, fill = factor(RACE))) + \n",
    "        geom_bar(alpha = 0.8, color = \"white\", linewidth = 0.3) + \n",
    "        coord_flip() +\n",
    "        theme_poster +\n",
    "        labs(\n",
    "          x = \"CCS Procedure Codes\", \n",
    "          y = \"Number of Procedures\",\n",
    "          title = \"Most Common Ambulatory Surgeries\", \n",
    "          subtitle = \"Segmented by Patient Race/Ethnicity\",\n",
    "          fill = \"Race/Ethnicity\"\n",
    "        ) + \n",
    "        scale_fill_manual(values = race_colors_unified, labels = race_labels) +\n",
    "        scale_y_continuous(labels = comma_format()) +\n",
    "        theme(legend.position = \"bottom\")\n",
    "      \n",
    "      print(p4_sample)\n",
    "      \n",
    "      save_plot_for_poster(p4_sample, \n",
    "                          name = \"procedures_by_race\", \n",
    "                          description = \"Common Ambulatory Surgery Procedures by Race/Ethnicity\",\n",
    "                          category = \"descriptive\")\n",
    "      \n",
    "      cat(\"> Sample procedures by race plot created\\n\")\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedures by Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R # === AGE-SPECIFIC PROCEDURE PATTERNS HEATMAP ===\n",
    "\n",
    "# Manual ordering for clearer red diagonal pattern with unified theme\n",
    "cat(\"Creating heatmap with manually ordered CCS codes and unified styling...\\n\")\n",
    "\n",
    "# Use existing data preparation (keep all the excellent logic)\n",
    "age_cpt_table <- table(NASS$CPTCCS1, NASS$AGE)\n",
    "age_cpt_scaled <- scale(t(age_cpt_table))\n",
    "\n",
    "# MANUAL ORDERING: Find peak age for each CCS code (keep this great approach)\n",
    "ccs_peak_ages <- apply(age_cpt_scaled, 2, function(x) {\n",
    "  # Find the age with maximum scaled frequency for this CCS code\n",
    "  peak_age <- as.numeric(names(which.max(x)))\n",
    "  return(peak_age)\n",
    "})\n",
    "\n",
    "# Order CCS codes by their peak age (creates diagonal pattern)\n",
    "ccs_order <- order(ccs_peak_ages)\n",
    "age_cpt_manual <- age_cpt_scaled[, ccs_order, drop = FALSE]\n",
    "\n",
    "# Get the ordered CCS names for labeling\n",
    "ordered_ccs_names <- colnames(age_cpt_manual)\n",
    "\n",
    "cat(\"Ordered\", length(ordered_ccs_names), \"CCS codes by peak age\\n\")\n",
    "cat(\"Age range:\", min(as.numeric(rownames(age_cpt_manual))), \"to\", \n",
    "    max(as.numeric(rownames(age_cpt_manual))), \"years\\n\")\n",
    "\n",
    "# Enhanced ggplot2 version with UNIFIED THEME AND COLORS\n",
    "if(require(\"ggplot2\", quietly = TRUE) && require(\"reshape2\", quietly = TRUE)) {\n",
    "  # Convert to long format for ggplot with manual ordering (keep this logic)\n",
    "  plot_data <- melt(t(age_cpt_manual))  # Transpose for correct orientation\n",
    "  colnames(plot_data) <- c(\"CPTCCS1\", \"Age\", \"Scaled_Frequency\")\n",
    "  \n",
    "  # Preserve manual ordering in factor levels\n",
    "  plot_data$CPTCCS1 <- factor(plot_data$CPTCCS1, levels = ordered_ccs_names)\n",
    "  \n",
    "  # Create ggplot heatmap with UNIFIED THEME AND COLORS\n",
    "  p <- ggplot(plot_data, aes(x = Age, y = CPTCCS1, fill = Scaled_Frequency)) +\n",
    "    geom_tile(color = NA) +  # Keep: NO WHITE LINES\n",
    "    \n",
    "    # Use unified heatmap colors from color system\n",
    "    scale_fill_gradient2(\n",
    "      low = heatmap_colors$low,    # \"#1E3A8A\" - Deep blue\n",
    "      mid = heatmap_colors$mid,    # \"#F8F9FA\" - Near white  \n",
    "      high = heatmap_colors$high,  # \"#B22222\" - Deep red\n",
    "      midpoint = 0,\n",
    "      name = \"Scaled\\nFreq\"\n",
    "    ) +\n",
    "    \n",
    "    # UPDATED: Apply unified poster theme\n",
    "    theme_poster +\n",
    "    theme(\n",
    "      # Set text sizing but apply poster theme base\n",
    "      axis.text.x = element_text(size = 10, color = \"black\", \n",
    "                               margin = ggplot2::margin(t = 3)),\n",
    "      axis.text.y = element_text(size = 7, color = \"black\", \n",
    "                               margin = ggplot2::margin(r = 3)),\n",
    "      axis.title.x = element_text(size = 11, face = \"bold\", \n",
    "                                margin = ggplot2::margin(t = 8)),\n",
    "      axis.title.y = element_text(size = 11, face = \"bold\", \n",
    "                                margin = ggplot2::margin(r = 8)),\n",
    "      \n",
    "      # Set title positioning and sizes for poster\n",
    "      plot.title = element_text(size = 16, face = \"bold\", hjust = 0,  # Left-aligned like poster theme\n",
    "                              margin = ggplot2::margin(b = 5, t = 8)),\n",
    "      plot.subtitle = element_text(size = 12, hjust = 0, color = \"gray30\",  # Left-aligned like poster theme\n",
    "                                 margin = ggplot2::margin(b = 15)),\n",
    "      \n",
    "      # Set clean background styling\n",
    "      panel.background = element_rect(fill = \"#F5F5F5\", color = NA),\n",
    "      plot.background = element_rect(fill = \"white\", color = NA),\n",
    "      panel.grid = element_blank(),\n",
    "      \n",
    "      # Set legend sizing\n",
    "      legend.position = \"right\",\n",
    "      legend.title = element_text(size = 10, face = \"bold\"),  # Slightly larger for poster\n",
    "      legend.text = element_text(size = 9),                   # Slightly larger for poster\n",
    "      legend.key.size = unit(0.6, \"cm\"),\n",
    "      legend.key.width = unit(0.8, \"cm\"),\n",
    "      legend.margin = ggplot2::margin(l = 5),\n",
    "      \n",
    "      # Keep your optimized spacing\n",
    "      plot.margin = ggplot2::margin(15, 15, 10, 10)\n",
    "    ) +\n",
    "    \n",
    "    # Apply poster-style titles (left-aligned)\n",
    "    labs(\n",
    "      title = \"Age-Specific Procedure Patterns in Ambulatory Surgery\",\n",
    "      subtitle = \"CCS Codes Ordered by Peak Age\",\n",
    "      x = \"Patient Age (Years)\",\n",
    "      y = \"CCS Procedure Codes\"\n",
    "    ) +\n",
    "    \n",
    "    # Keep your excellent axis configuration\n",
    "    scale_x_continuous(position = \"bottom\", \n",
    "                     breaks = seq(0, 100, by = 20),\n",
    "                     expand = c(0, 0)) +\n",
    "    scale_y_discrete(position = \"right\", expand = c(0, 0)) +\n",
    "    coord_cartesian(clip = \"off\")\n",
    "  \n",
    "  print(p)\n",
    "\n",
    "  save_plot_for_poster(p, \n",
    "                      name = \"age_specific_procedure_patterns\", \n",
    "                      description = \"Age-Specific Procedure Pattern\",\n",
    "                      category = \"descriptive\")\n",
    "\n",
    "  cat(\"> Enhanced heatmap with unified theme and colors:\\n\")\n",
    "  cat(\"   - Applied theme_poster base with custom overrides\\n\")\n",
    "  cat(\"   - Used unified heatmap_colors (blue-white-red)\\n\")\n",
    "  cat(\"   - Maintained left-aligned titles (poster style)\\n\")\n",
    "  cat(\"   - Preserved all excellent existing functionality\\n\")\n",
    "  cat(\"   - Kept optimal sizing and spacing\\n\")\n",
    "  \n",
    "} else {\n",
    "  cat(\"‚ùå ggplot2 or reshape2 not available\\n\")\n",
    "}\n",
    "\n",
    "cat(\"Enhanced heatmap generation complete with unified styling.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patient Characteristics Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age Distribution by Race and Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R # === AGE DISTRIBUTION BY RACE AND REGION ===\n",
    "\n",
    "# Age distribution by race and region - COUNT CURVES instead of density\n",
    "if(all(c(\"AGE\", \"RACE\", \"HOSP_REGION\", \"PL_NCHS\") %in% names(NASS))) {\n",
    "  \n",
    "  age_data <- NASS[RACE %in% 1:6 & PL_NCHS %in% 1:6 & AGE >= 0 & AGE <= 100]\n",
    "  \n",
    "  if(nrow(age_data) > 0) {\n",
    "    \n",
    "    race_labels <- c(\"1\" = \"White\", \"2\" = \"Black\", \"3\" = \"Hispanic\", \n",
    "                    \"4\" = \"Asian/Pacific\", \"5\" = \"Native American\", \"6\" = \"Other\")\n",
    "    region_labels <- c(\"1\" = \"Northeast\", \"2\" = \"Midwest\", \"3\" = \"South\", \"4\" = \"West\")\n",
    "    pl_nchs_labels <- c(\"1\" = \"Large Central\", \"2\" = \"Large Fringe\", \"3\" = \"Medium\", \n",
    "                       \"4\" = \"Small\", \"5\" = \"Micro\", \"6\" = \"Non-core\")\n",
    "    \n",
    "    p5 <- ggplot(age_data, aes(x = AGE)) + \n",
    "      geom_density(aes(fill = factor(RACE), y = after_stat(count)), alpha = 0.7, color = \"white\", \n",
    "                   linewidth = 0.2) + \n",
    "      theme_poster + \n",
    "      labs(\n",
    "        x = \"Age (years)\", \n",
    "        y = \"Count\",\n",
    "        title = \"Patient Age Distribution in NASS 2020\", \n",
    "        subtitle = \"By Urban-Rural Classification and US Region, Segmented by Race\",\n",
    "        fill = \"Race/Ethnicity\"\n",
    "      ) + \n",
    "      scale_fill_manual(values = race_colors_unified, labels = race_labels) + \n",
    "      xlim(0, 100) +\n",
    "      scale_x_continuous(position = \"bottom\", \n",
    "                 breaks = c(50, 100),\n",
    "                 expand = c(0, 0)) +\n",
    "      facet_grid(HOSP_REGION ~ PL_NCHS, \n",
    "                 labeller = labeller(HOSP_REGION = region_labels, PL_NCHS = pl_nchs_labels)) + \n",
    "      theme(legend.position = \"bottom\", \n",
    "            strip.text = element_text(size = 8))\n",
    "    \n",
    "    print(p5)\n",
    "\n",
    "    save_plot_for_poster(p5, \n",
    "                    name = \"age_distribution_race\", \n",
    "                    description = \"Patient Age Distribution by Race and Region\",\n",
    "                    category = \"descriptive\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age Distribution by Payer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R # === AGE DISTRIBUTION BY PAYER AND REGION ===\n",
    "\n",
    "# Age distribution by payer and region \n",
    "if(all(c(\"AGE\", \"PAY1\", \"HOSP_REGION\", \"PL_NCHS\") %in% names(NASS))) {\n",
    "  \n",
    "  age_payer_data <- NASS[PAY1 %in% 1:6 & PL_NCHS %in% 1:6 & AGE >= 0 & AGE <= 100]\n",
    "  \n",
    "  if(nrow(age_payer_data) > 0) {\n",
    "    \n",
    "    pay_labels <- c(\"1\" = \"Medicare\", \"2\" = \"Medicaid\", \"3\" = \"Private\", \n",
    "                   \"4\" = \"Self-pay\", \"5\" = \"No Charge\", \"6\" = \"Other\")\n",
    "    \n",
    "    p6 <- ggplot(age_payer_data, aes(x = AGE)) + \n",
    "      geom_density(aes(fill = factor(PAY1), y = after_stat(count)), alpha = 0.7, color = \"white\", \n",
    "                   linewidth = 0.2) + \n",
    "      theme_poster + \n",
    "      labs(\n",
    "        x = \"Age (years)\", \n",
    "        y = \"Count\",\n",
    "        title = \"Patient Age Distribution in NASS 2020\", \n",
    "        subtitle = \"By Urban-Rural Classification and US Region, Segmented by Payer\",\n",
    "        fill = \"Primary Payer\"\n",
    "      ) + \n",
    "      scale_fill_manual(values = payer_colors_unified, labels = pay_labels) + \n",
    "      xlim(0, 100) +\n",
    "      scale_x_continuous(position = \"bottom\", \n",
    "            breaks = c(50, 100),\n",
    "            expand = c(0, 0)) +\n",
    "      facet_grid(HOSP_REGION ~ PL_NCHS, \n",
    "                 labeller = labeller(HOSP_REGION = region_labels, PL_NCHS = pl_nchs_labels)) + \n",
    "      theme(legend.position = \"bottom\",\n",
    "            strip.text = element_text(size = 8))\n",
    "    \n",
    "    print(p6)\n",
    "    save_plot_for_poster(p6, \n",
    "                    name = \"age_distribution_payer\", \n",
    "                    description = \"Patient Age Distribution by Payer and Region\",\n",
    "                    category = \"descriptive\")    \n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Payer Distribution by Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R # === PAYER DISTRIBUTION BY RACE AND AGE GROUP ===\n",
    "\n",
    "# Payer distribution by race and age group\n",
    "if(all(c(\"RACE\", \"PAY1\", \"AGE\") %in% names(NASS))) {\n",
    "  \n",
    "  # Create better age groups: 0-17, 18-64, 65+\n",
    "  plot_data <- NASS[RACE %in% 1:6 & PAY1 %in% 1:6 & !is.na(AGE)]\n",
    "  \n",
    "  if(nrow(plot_data) > 0) {\n",
    "    \n",
    "    # Create simplified age groups\n",
    "    plot_data[, AGE_GROUP_SIMPLE := cut(AGE, \n",
    "                                       breaks = c(-Inf, 17, 64, Inf),\n",
    "                                       labels = c(\"0-17\", \"18-64\", \"65+\"),\n",
    "                                       right = TRUE)]\n",
    "    \n",
    "    # Remove any missing age groups\n",
    "    plot_data <- plot_data[!is.na(AGE_GROUP_SIMPLE)]\n",
    "    \n",
    "    # Define cleaner labels\n",
    "    race_labels <- c(\"1\" = \"White\", \"2\" = \"Black\", \"3\" = \"Hispanic\", \n",
    "                    \"4\" = \"Asian/Pacific\", \"5\" = \"Native American\", \"6\" = \"Other\")\n",
    "    pay_labels <- c(\"1\" = \"Medicare\", \"2\" = \"Medicaid\", \"3\" = \"Private\", \n",
    "                   \"4\" = \"Self-pay\", \"5\" = \"No Charge\", \"6\" = \"Other\")\n",
    "    \n",
    "    p7 <- ggplot(plot_data, aes(x = factor(RACE), fill = factor(PAY1))) + \n",
    "      geom_bar(position = \"fill\", alpha = 0.8, color = \"white\", linewidth = 0.2) + \n",
    "      theme_poster + \n",
    "      labs(\n",
    "        x = \"Race/Ethnicity\", \n",
    "        y = \"Proportion\",\n",
    "        title = \"Primary Payer Distribution by Demographics\", \n",
    "        subtitle = \"Insurance Coverage Patterns by Age Group and Race/Ethnicity\",\n",
    "        fill = \"Primary Payer\"\n",
    "      ) + \n",
    "      scale_fill_manual(values = payer_colors_unified, labels = pay_labels) + \n",
    "      scale_x_discrete(labels = race_labels) +\n",
    "      scale_y_continuous(labels = percent_format()) +\n",
    "      facet_wrap(~ AGE_GROUP_SIMPLE, ncol = 3) +\n",
    "      theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 9),\n",
    "            legend.position = \"bottom\",\n",
    "            strip.text = element_text(size = 11, face = \"bold\"),\n",
    "            panel.spacing = unit(1, \"lines\"))\n",
    "    \n",
    "    print(p7)\n",
    "    save_plot_for_poster(p7, \n",
    "                    name = \"payer_by_demographics\", \n",
    "                    description = \"Primary Payer Distribution by Demographics\",\n",
    "                    category = \"descriptive\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Payer Distribution by Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R # === PAYER DISTRIBUTION BY INCOME AND AGE GROUP ===\n",
    "\n",
    "# Payer distribution by income quartile and age group\n",
    "if(all(c(\"ZIPINC_QRTL\", \"PAY1\", \"AGE\") %in% names(NASS))) {\n",
    "  \n",
    "  # Create data with income quartiles and simplified age groups\n",
    "  plot_data <- NASS[ZIPINC_QRTL %in% 1:4 & PAY1 %in% 1:6 & !is.na(AGE)]\n",
    "  \n",
    "  if(nrow(plot_data) > 0) {\n",
    "    \n",
    "    # Create simplified age groups: 0-17, 18-64, 65+\n",
    "    plot_data[, AGE_GROUP_SIMPLE := cut(AGE, \n",
    "                                       breaks = c(-Inf, 17, 64, Inf),\n",
    "                                       labels = c(\"0-17\", \"18-64\", \"65+\"),\n",
    "                                       right = TRUE)]\n",
    "    \n",
    "    # Remove any missing age groups\n",
    "    plot_data <- plot_data[!is.na(AGE_GROUP_SIMPLE)]\n",
    "    \n",
    "    # Define cleaner labels\n",
    "    income_labels <- c(\"1\" = \"Q1 (Lowest)\", \"2\" = \"Q2\", \"3\" = \"Q3\", \"4\" = \"Q4 (Highest)\")\n",
    "    pay_labels <- c(\"1\" = \"Medicare\", \"2\" = \"Medicaid\", \"3\" = \"Private\", \n",
    "                   \"4\" = \"Self-pay\", \"5\" = \"No Charge\", \"6\" = \"Other\")\n",
    "    \n",
    "    p8 <- ggplot(plot_data, aes(x = factor(ZIPINC_QRTL), fill = factor(PAY1))) + \n",
    "      geom_bar(position = \"fill\", alpha = 0.8, color = \"white\", linewidth = 0.2) + \n",
    "      theme_poster + \n",
    "      labs(\n",
    "        x = \"Income Quartile (by ZIP Code)\", \n",
    "        y = \"Proportion\",\n",
    "        title = \"Primary Payer Distribution by Socioeconomics\", \n",
    "        subtitle = \"Insurance by Age Group and Income Quartile\",\n",
    "        fill = \"Primary Payer\"\n",
    "      ) + \n",
    "      scale_fill_manual(values = payer_colors_unified, labels = pay_labels) + \n",
    "      scale_x_discrete(labels = income_labels) +\n",
    "      scale_y_continuous(labels = percent_format()) +\n",
    "      facet_wrap(~ AGE_GROUP_SIMPLE, ncol = 3) +\n",
    "      theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 9),\n",
    "            legend.position = \"bottom\",\n",
    "            strip.text = element_text(size = 11, face = \"bold\"),\n",
    "            panel.spacing = unit(1, \"lines\"))\n",
    "    \n",
    "    print(p8)\n",
    "    save_plot_for_poster(p8, \n",
    "                    name = \"payer_by_income\", \n",
    "                    description = \"Primary Payer Distribution by Income\",\n",
    "                    category = \"descriptive\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehensive Dataset Summary\n",
    "\n",
    "Final summary statistics providing an overview of all key variables in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R # === COMPREHENSIVE DATASET SUMMARY ===\n",
    "\n",
    "# ===============================================\n",
    "# COMPREHENSIVE NASS 2020 DATASET SUMMARY\n",
    "# ===============================================\n",
    "\n",
    "cat(\"================================================================\\n\")\n",
    "cat(\"                    NASS 2020 DATASET SUMMARY                  \\n\")\n",
    "cat(\"================================================================\\n\\n\")\n",
    "\n",
    "# Dataset Overview\n",
    "cat(\"DATASET OVERVIEW\\n\")\n",
    "cat(\"================\\n\")\n",
    "cat(\"Total observations:\", format(nrow(NASS), big.mark = \",\"), \"\\n\")\n",
    "cat(\"Total variables:\", ncol(NASS), \"\\n\")\n",
    "cat(\"Memory usage:\", round(object.size(NASS)/1024^2, 1), \"MB\\n\")\n",
    "cat(\"Missing data patterns:\", sum(is.na(NASS)), \"total missing values\\n\\n\")\n",
    "\n",
    "# Key variables for comprehensive summary\n",
    "summary_vars <- c(\"AGE\", \"FEMALE\", \"RACE\", \"WHITE\", \"ZIPINC_QRTL\", \"PAY1\", \n",
    "                 \"HOSP_LOCATION\", \"HOSP_TEACH\", \"HOSP_REGION\", \"HOSP_BEDSIZE_CAT\",\n",
    "                 \"PL_NCHS\", \"CPTCCS1\", \"TOTCHG\", \"DISCWT\")\n",
    "\n",
    "available_vars <- summary_vars[summary_vars %in% names(NASS)]\n",
    "missing_vars <- summary_vars[!summary_vars %in% names(NASS)]\n",
    "\n",
    "if(length(missing_vars) > 0) {\n",
    "  cat(\"WARNING: Variables not available:\", paste(missing_vars, collapse = \", \"), \"\\n\\n\")\n",
    "}\n",
    "\n",
    "# Variable-by-variable analysis\n",
    "for(var in available_vars) {\n",
    "  cat(\"----------------------------------------------------------------\\n\")\n",
    "  cat(\"Variable:\", toupper(var), \"\\n\")\n",
    "  cat(\"----------------------------------------------------------------\\n\")\n",
    "  \n",
    "  if(is.numeric(NASS[[var]])) {\n",
    "    # Enhanced numeric variable summary\n",
    "    valid_values <- NASS[[var]][!is.na(NASS[[var]])]\n",
    "    missing_count <- sum(is.na(NASS[[var]]))\n",
    "    \n",
    "    cat(\"Type: Continuous/Numeric\\n\")\n",
    "    cat(\"Valid observations:\", format(length(valid_values), big.mark = \",\"), \n",
    "        \"(\", round(100 * length(valid_values) / nrow(NASS), 1), \"%)\\n\")\n",
    "    cat(\"Missing values:\", format(missing_count, big.mark = \",\"), \n",
    "        \"(\", round(100 * missing_count / nrow(NASS), 1), \"%)\\n\")\n",
    "    \n",
    "    if(length(valid_values) > 0) {\n",
    "      # Central tendency\n",
    "      cat(\"\\nCENTRAL TENDENCY:\\n\")\n",
    "      cat(\"   Mean:\", format(round(mean(valid_values), 2), big.mark = \",\"), \"\\n\")\n",
    "      cat(\"   Median:\", format(round(median(valid_values), 2), big.mark = \",\"), \"\\n\")\n",
    "      cat(\"   Mode region:\", format(round(median(valid_values), 2), big.mark = \",\"), \n",
    "          \" +/-\", format(round(mad(valid_values), 2), big.mark = \",\"), \"\\n\")\n",
    "      \n",
    "      # Variability\n",
    "      cat(\"\\nVARIABILITY:\\n\")\n",
    "      cat(\"   Standard Dev:\", format(round(sd(valid_values), 2), big.mark = \",\"), \"\\n\")\n",
    "      cat(\"   IQR:\", format(round(quantile(valid_values, 0.25), 2), big.mark = \",\"), \"to\", \n",
    "          format(round(quantile(valid_values, 0.75), 2), big.mark = \",\"), \"\\n\")\n",
    "      cat(\"   Range:\", format(round(min(valid_values), 2), big.mark = \",\"), \"to\", \n",
    "          format(round(max(valid_values), 2), big.mark = \",\"), \"\\n\")\n",
    "      \n",
    "      # Distribution shape\n",
    "      cat(\"\\nDISTRIBUTION:\\n\")\n",
    "      q1 <- quantile(valid_values, 0.25)\n",
    "      q3 <- quantile(valid_values, 0.75)\n",
    "      skewness_approx <- (mean(valid_values) - median(valid_values)) / sd(valid_values)\n",
    "      \n",
    "      cat(\"   Skewness (approx):\", round(skewness_approx, 3), \n",
    "          ifelse(abs(skewness_approx) < 0.5, \"(approximately symmetric)\", \n",
    "                ifelse(skewness_approx > 0, \"(right-skewed)\", \"(left-skewed)\")), \"\\n\")\n",
    "      \n",
    "      # Outliers (using IQR method)\n",
    "      iqr <- q3 - q1\n",
    "      lower_fence <- q1 - 1.5 * iqr\n",
    "      upper_fence <- q3 + 1.5 * iqr\n",
    "      outliers <- sum(valid_values < lower_fence | valid_values > upper_fence)\n",
    "      cat(\"   Potential outliers:\", outliers, \n",
    "          \"(\", round(100 * outliers / length(valid_values), 1), \"%)\\n\")\n",
    "      \n",
    "      # Percentile breakdown for key variables\n",
    "      if(var %in% c(\"AGE\", \"TOTCHG\", \"DISCWT\")) {\n",
    "        cat(\"\\nPERCENTILE BREAKDOWN:\\n\")\n",
    "        percentiles <- c(0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99)\n",
    "        for(p in percentiles) {\n",
    "          cat(\"   P\", round(p*100), \":\", format(round(quantile(valid_values, p), 1), big.mark = \",\"), \"\\n\")\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \n",
    "  } else {\n",
    "    # Enhanced categorical variable summary\n",
    "    freq_table <- table(NASS[[var]], useNA = \"ifany\")\n",
    "    total_n <- sum(freq_table)\n",
    "    missing_count <- sum(is.na(NASS[[var]]))\n",
    "    \n",
    "    cat(\"Type: Categorical/Factor\\n\")\n",
    "    cat(\"Total categories:\", length(freq_table), \"\\n\")\n",
    "    cat(\"Valid observations:\", format(total_n - missing_count, big.mark = \",\"), \n",
    "        \"(\", round(100 * (total_n - missing_count) / nrow(NASS), 1), \"%)\\n\")\n",
    "    \n",
    "    if(missing_count > 0) {\n",
    "      cat(\"Missing values:\", format(missing_count, big.mark = \",\"), \n",
    "          \"(\", round(100 * missing_count / nrow(NASS), 1), \"%)\\n\")\n",
    "    }\n",
    "    \n",
    "    # Sort by frequency\n",
    "    sorted_freq <- sort(freq_table, decreasing = TRUE)\n",
    "    \n",
    "    # Show distribution\n",
    "    cat(\"\\nFREQUENCY DISTRIBUTION:\\n\")\n",
    "    max_show <- min(15, length(sorted_freq))  # Show more categories\n",
    "    \n",
    "    for(i in 1:max_show) {\n",
    "      category_name <- names(sorted_freq)[i]\n",
    "      count <- sorted_freq[i]\n",
    "      percentage <- round(100 * count / total_n, 1)\n",
    "      \n",
    "      # Create a simple bar visualization\n",
    "      bar_length <- min(20, round(20 * count / max(sorted_freq)))\n",
    "      bar <- paste(rep(\"*\", bar_length), collapse = \"\")\n",
    "      \n",
    "      cat(\"   \", sprintf(\"%-20s\", category_name), \":\", \n",
    "          sprintf(\"%8s\", format(count, big.mark = \",\")), \n",
    "          sprintf(\"(%5.1f%%)\", percentage), \" \", bar, \"\\n\")\n",
    "    }\n",
    "    \n",
    "    if(length(sorted_freq) > max_show) {\n",
    "      remaining_count <- sum(sorted_freq[(max_show+1):length(sorted_freq)])\n",
    "      remaining_pct <- round(100 * remaining_count / total_n, 1)\n",
    "      cat(\"   ... \", length(sorted_freq) - max_show, \" more categories:\", \n",
    "          format(remaining_count, big.mark = \",\"), \"(\", remaining_pct, \"%)\\n\")\n",
    "    }\n",
    "    \n",
    "    # Diversity metrics\n",
    "    cat(\"\\nDIVERSITY METRICS:\\n\")\n",
    "    # Simpson's diversity index (1 - sum of squared proportions)\n",
    "    proportions <- as.numeric(freq_table) / sum(freq_table)\n",
    "    simpson_diversity <- 1 - sum(proportions^2)\n",
    "    cat(\"   Simpson's Diversity:\", round(simpson_diversity, 3), \n",
    "        \"(0=no diversity, 1=max diversity)\\n\")\n",
    "    \n",
    "    # Effective number of categories (inverse Simpson)\n",
    "    effective_categories <- 1 / sum(proportions^2)\n",
    "    cat(\"   Effective categories:\", round(effective_categories, 1), \n",
    "        \"out of\", length(freq_table), \"total\\n\")\n",
    "    \n",
    "    # Concentration ratio (top 3 categories)\n",
    "    top3_concentration <- sum(sorted_freq[1:min(3, length(sorted_freq))]) / total_n\n",
    "    cat(\"   Top-3 concentration:\", round(100 * top3_concentration, 1), \"%\\n\")\n",
    "    \n",
    "    # Special insights for key variables\n",
    "    if(var == \"RACE\") {\n",
    "      cat(\"\\nRACE/ETHNICITY INSIGHTS:\\n\")\n",
    "      cat(\"   Racial diversity reflects ambulatory surgery access patterns\\n\")\n",
    "      if(\"1\" %in% names(freq_table)) {\n",
    "        white_vs_nonwhite <- round(freq_table[\"1\"] / sum(freq_table[names(freq_table) != \"1\"]), 2)\n",
    "        cat(\"   White vs Non-White ratio:\", white_vs_nonwhite, \":1\\n\")\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    if(var == \"PAY1\") {\n",
    "      cat(\"\\nPAYER MIX INSIGHTS:\\n\")\n",
    "      public_payers <- sum(freq_table[c(\"1\", \"2\")], na.rm = TRUE)  # Medicare + Medicaid\n",
    "      cat(\"   Public insurance coverage:\", \n",
    "          round(100 * public_payers / total_n, 1), \"%\\n\")\n",
    "    }\n",
    "    \n",
    "    if(var == \"ZIPINC_QRTL\") {\n",
    "      cat(\"\\nINCOME DISTRIBUTION INSIGHTS:\\n\")\n",
    "      low_income <- sum(freq_table[c(\"1\", \"2\")], na.rm = TRUE)  # Q1 + Q2\n",
    "      cat(\"   Lower-income representation:\", \n",
    "          round(100 * low_income / total_n, 1), \"%\\n\")\n",
    "    }\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Cross-tabulation insights\n",
    "cat(\"----------------------------------------------------------------\\n\")\n",
    "cat(\"KEY RELATIONSHIPS & CROSS-TABULATIONS\\n\")\n",
    "cat(\"----------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "# Race vs Payer cross-tab\n",
    "if(all(c(\"RACE\", \"PAY1\") %in% available_vars)) {\n",
    "  cat(\"RACE x PAYER DISTRIBUTION:\\n\")\n",
    "  cross_tab <- table(NASS$RACE, NASS$PAY1)\n",
    "  prop_tab <- round(100 * prop.table(cross_tab, 1), 1)\n",
    "  \n",
    "  race_labels <- c(\"1\"=\"White\", \"2\"=\"Black\", \"3\"=\"Hispanic\", \"4\"=\"Asian/Pacific\", \"5\"=\"Native Am\", \"6\"=\"Other\")\n",
    "  pay_labels <- c(\"1\"=\"Medicare\", \"2\"=\"Medicaid\", \"3\"=\"Private\", \"4\"=\"Self-pay\", \"5\"=\"No Charge\", \"6\"=\"Other\")\n",
    "  \n",
    "  for(race in rownames(prop_tab)) {\n",
    "    if(race %in% names(race_labels)) {\n",
    "      cat(\"   \", race_labels[race], \"patients:\\n\")\n",
    "      race_row <- prop_tab[race, ]\n",
    "      sorted_payers <- sort(race_row, decreasing = TRUE)\n",
    "      for(i in 1:min(3, length(sorted_payers))) {\n",
    "        payer <- names(sorted_payers)[i]\n",
    "        if(payer %in% names(pay_labels)) {\n",
    "          cat(\"     \", pay_labels[payer], \":\", sorted_payers[i], \"%\\n\")\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Age vs Income relationship\n",
    "if(all(c(\"AGE\", \"ZIPINC_QRTL\") %in% available_vars)) {\n",
    "  cat(\"AGE x INCOME PATTERNS:\\n\")\n",
    "  age_income <- NASS[!is.na(AGE) & !is.na(ZIPINC_QRTL), .(mean_age = round(mean(AGE), 1)), by = ZIPINC_QRTL]\n",
    "  setorder(age_income, ZIPINC_QRTL)\n",
    "  \n",
    "  income_labels <- c(\"1\"=\"Q1 (Lowest)\", \"2\"=\"Q2\", \"3\"=\"Q3\", \"4\"=\"Q4 (Highest)\")\n",
    "  for(i in 1:nrow(age_income)) {\n",
    "    quartile <- as.character(age_income$ZIPINC_QRTL[i])\n",
    "    if(quartile %in% names(income_labels)) {\n",
    "      cat(\"   \", income_labels[quartile], \"- Average age:\", age_income$mean_age[i], \"years\\n\")\n",
    "    }\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Hospital characteristics summary\n",
    "if(all(c(\"HOSP_REGION\", \"HOSP_TEACH\", \"HOSP_LOCATION\") %in% available_vars)) {\n",
    "  cat(\"HOSPITAL CHARACTERISTICS:\\n\")\n",
    "  \n",
    "  # Unique hospital count\n",
    "  if(\"HOSP_NASS\" %in% names(NASS)) {\n",
    "    unique_hospitals <- length(unique(NASS$HOSP_NASS))\n",
    "    cat(\"   Total hospitals in sample:\", unique_hospitals, \"\\n\")\n",
    "  }\n",
    "  \n",
    "  # Teaching hospital distribution\n",
    "  teaching_dist <- table(NASS$HOSP_TEACH)\n",
    "  if(\"1\" %in% names(teaching_dist)) {\n",
    "    cat(\"   Teaching hospitals:\", round(100 * teaching_dist[\"1\"] / sum(teaching_dist), 1), \"%\\n\")\n",
    "  }\n",
    "  \n",
    "  # Urban/Rural distribution\n",
    "  location_dist <- table(NASS$HOSP_LOCATION)\n",
    "  if(\"1\" %in% names(location_dist)) {\n",
    "    cat(\"   Urban hospitals:\", round(100 * location_dist[\"1\"] / sum(location_dist), 1), \"%\\n\")\n",
    "  }\n",
    "  \n",
    "  # Regional distribution\n",
    "  region_dist <- table(NASS$HOSP_REGION)\n",
    "  region_labels <- c(\"1\"=\"Northeast\", \"2\"=\"Midwest\", \"3\"=\"South\", \"4\"=\"West\")\n",
    "  cat(\"   Regional distribution:\\n\")\n",
    "  for(region in names(region_dist)) {\n",
    "    if(region %in% names(region_labels)) {\n",
    "      pct <- round(100 * region_dist[region] / sum(region_dist), 1)\n",
    "      cat(\"     \", region_labels[region], \":\", pct, \"%\\n\")\n",
    "    }\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Data quality assessment\n",
    "cat(\"----------------------------------------------------------------\\n\")\n",
    "cat(\"DATA QUALITY ASSESSMENT\\n\")\n",
    "cat(\"----------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "# Missing data patterns\n",
    "total_cells <- nrow(NASS) * ncol(NASS)\n",
    "missing_cells <- sum(is.na(NASS))\n",
    "complete_cases <- sum(complete.cases(NASS))\n",
    "\n",
    "cat(\"COMPLETENESS METRICS:\\n\")\n",
    "cat(\"   Overall completeness:\", round(100 * (1 - missing_cells/total_cells), 2), \"%\\n\")\n",
    "cat(\"   Complete cases (no missing):\", format(complete_cases, big.mark = \",\"), \n",
    "    \"(\", round(100 * complete_cases / nrow(NASS), 1), \"%)\\n\")\n",
    "cat(\"   Variables with missing data:\", sum(sapply(NASS, function(x) any(is.na(x)))), \n",
    "    \"out of\", ncol(NASS), \"\\n\\n\")\n",
    "\n",
    "# Variables with highest missing rates\n",
    "missing_rates <- sapply(NASS, function(x) round(100 * sum(is.na(x)) / length(x), 1))\n",
    "high_missing <- missing_rates[missing_rates > 0]\n",
    "if(length(high_missing) > 0) {\n",
    "  cat(\"VARIABLES WITH MISSING DATA:\\n\")\n",
    "  sorted_missing <- sort(high_missing, decreasing = TRUE)\n",
    "  for(i in 1:min(10, length(sorted_missing))) {\n",
    "    cat(\"   \", names(sorted_missing)[i], \":\", sorted_missing[i], \"% missing\\n\")\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Survey weights summary\n",
    "if(\"DISCWT\" %in% available_vars) {\n",
    "  cat(\"SURVEY WEIGHTS SUMMARY:\\n\")\n",
    "  weights <- NASS$DISCWT[!is.na(NASS$DISCWT)]\n",
    "  cat(\"   Weight range:\", round(min(weights), 2), \"to\", round(max(weights), 2), \"\\n\")\n",
    "  cat(\"   Effective sample size:\", round(sum(weights)^2 / sum(weights^2)), \"\\n\")\n",
    "  cat(\"   Design effect (approx):\", round(nrow(NASS) / (sum(weights)^2 / sum(weights^2)), 2), \"\\n\\n\")\n",
    "}\n",
    "\n",
    "cat(\"================================================================\\n\")\n",
    "cat(\"                     SUMMARY COMPLETE                          \\n\")\n",
    "cat(\"================================================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Census Data Comparisons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20a6a8a8"
   },
   "source": [
    "### Census API Setup\n",
    "\n",
    "Set up Census API integration and pull 2020 DHC population data for comparison with NASS sample proportions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ Please Register at the Census website for an API Key ](https://api.census.gov/data/key_signup.html). Enter your Census API key for data retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6d0c69b9"
   },
   "outputs": [],
   "source": [
    "# === CENSUS API KEY HANDLING ===\n",
    "\n",
    "import getpass, os, json, textwrap\n",
    "\n",
    "# Try encrypted Census API key first if available\n",
    "census_api_key = None\n",
    "\n",
    "if 'CENSUS_ENCRYPTED_KEY_URL' in globals() and 'CENSUS_PASSWORD' in globals():\n",
    "    try:\n",
    "        # Try to decrypt Census API key\n",
    "        decrypted_key = data_loader._decrypt_key(CENSUS_ENCRYPTED_KEY_URL, CENSUS_PASSWORD)\n",
    "        if decrypted_key:\n",
    "            # Parse the decrypted key (assume it's JSON with api_key field)\n",
    "            key_data = json.loads(decrypted_key)\n",
    "            census_api_key = key_data.get('api_key')\n",
    "            print(\"‚úÖ Using encrypted Census API key\")\n",
    "        else:\n",
    "            print(\"‚ùå Failed to decrypt Census API key - manual entry required\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Census key decryption error - manual entry required\")\n",
    "\n",
    "# Fallback to manual entry if encryption failed\n",
    "if not census_api_key:\n",
    "    print(\"Manual Census API key entry required:\")\n",
    "    census_api_key = getpass.getpass(\"Enter your Census API key (will not echo):\")\n",
    "\n",
    "# FIXED: Set the API key in BOTH Python and R environments\n",
    "if census_api_key and census_api_key.strip():\n",
    "    os.environ[\"CENSUS_API_KEY\"] = census_api_key.strip()\n",
    "    \n",
    "    # Also pass it directly to R to ensure it's available\n",
    "    get_ipython().run_cell_magic('R', f'-i census_api_key', '''\n",
    "    # Set the Census API key in R environment\n",
    "    Sys.setenv(CENSUS_API_KEY = census_api_key)\n",
    "    cat(\"‚úÖ Census API key configured in R environment\\\\n\")\n",
    "    ''')\n",
    "    \n",
    "    print(\"‚úÖ Census API key configured in both Python and R\")\n",
    "else:\n",
    "    print(\"‚ùå No valid Census API key provided\")\n",
    "    print(\"üí° You'll need to manually enter the key in the Census setup section\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3c39ad38"
   },
   "source": [
    "### Census Data Retrieval and Processing\n",
    "\n",
    "Pull 2020 DHC population data by age, gender, and race for states included in NASS sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "240fd569"
   },
   "outputs": [],
   "source": [
    "%%R -i VERBOSE_PRINTS # === CENSUS DATA RETRIEVAL ===\n",
    "\n",
    "# Install and load required packages for Census analysis\n",
    "required_packages <- c(\"tidycensus\", \"dplyr\", \"tidyr\", \"survey\")\n",
    "\n",
    "for(pkg in required_packages) {\n",
    "  if(!require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "    install.packages(pkg, quiet = TRUE)\n",
    "    library(pkg, character.only = TRUE, quietly = TRUE)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Set Census API key\n",
    "census_api_key(Sys.getenv(\"CENSUS_API_KEY\"), overwrite = FALSE, install = FALSE)\n",
    "\n",
    "# Define states included in NASS 2020 dataset\n",
    "states_in_nass <- c(\"Alaska\", \"California\", \"Colorado\", \"Connecticut\", \"District of Columbia\", \n",
    "                    \"Florida\", \"Georgia\", \"Hawaii\", \"Iowa\", \"Illinois\", \"Indiana\", \"Kansas\", \n",
    "                    \"Kentucky\", \"Maryland\", \"Maine\", \"Michigan\", \"Minnesota\", \"Missouri\", \n",
    "                    \"North Carolina\", \"North Dakota\", \"Nebraska\", \"New Jersey\", \"Nevada\", \n",
    "                    \"New York\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"South Carolina\", \n",
    "                    \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Wisconsin\")\n",
    "\n",
    "cat(\"Defined\", length(states_in_nass), \"states included in NASS sample\\n\")\n",
    "\n",
    "# Function to construct population variables for Census queries\n",
    "get_population_variables <- function(base_variable) {\n",
    "    variables <- paste0(base_variable, \"_\", sprintf(\"%03dN\", 1:49))\n",
    "    \n",
    "    labels <- c(\n",
    "        \"Total\",\n",
    "        \"Male: Total\", \"Male: Under 5 years\", \"Male: 5 to 9 years\", \"Male: 10 to 14 years\",\n",
    "        \"Male: 15 to 17 years\", \"Male: 18 and 19 years\", \"Male: 20 years\", \"Male: 21 years\",\n",
    "        \"Male: 22 to 24 years\", \"Male: 25 to 29 years\", \"Male: 30 to 34 years\", \"Male: 35 to 39 years\",\n",
    "        \"Male: 40 to 44 years\", \"Male: 45 to 49 years\", \"Male: 50 to 54 years\", \"Male: 55 to 59 years\",\n",
    "        \"Male: 60 and 61 years\", \"Male: 62 to 64 years\", \"Male: 65 and 66 years\", \"Male: 67 to 69 years\",\n",
    "        \"Male: 70 to 74 years\", \"Male: 75 to 79 years\", \"Male: 80 to 84 years\", \"Male: 85 years and over\",\n",
    "        \"Female: Total\", \"Female: Under 5 years\", \"Female: 5 to 9 years\", \"Female: 10 to 14 years\",\n",
    "        \"Female: 15 to 17 years\", \"Female: 18 and 19 years\", \"Female: 20 years\", \"Female: 21 years\",\n",
    "        \"Female: 22 to 24 years\", \"Female: 25 to 29 years\", \"Female: 30 to 34 years\", \"Female: 35 to 39 years\",\n",
    "        \"Female: 40 to 44 years\", \"Female: 45 to 49 years\", \"Female: 50 to 54 years\", \"Female: 55 to 59 years\",\n",
    "        \"Female: 60 and 61 years\", \"Female: 62 to 64 years\", \"Female: 65 and 66 years\", \"Female: 67 to 69 years\",\n",
    "        \"Female: 70 to 74 years\", \"Female: 75 to 79 years\", \"Female: 80 to 84 years\", \"Female: 85 years and over\"\n",
    "    )\n",
    "    \n",
    "    names(labels) <- variables\n",
    "    return(list(variables = variables, labels = labels))\n",
    "}\n",
    "\n",
    "# Function to get population data from Census\n",
    "get_population_data <- function(variables, labels) {\n",
    "    population_data <- get_decennial(\n",
    "        geography = \"state\",\n",
    "        variables = variables,\n",
    "        year = 2020,\n",
    "        sumfile = \"dhc\"\n",
    "    )\n",
    "    \n",
    "    # Replace variable codes with descriptive labels\n",
    "    population_data <- population_data %>% \n",
    "        mutate(variable = recode(variable, !!!setNames(labels, variables)))\n",
    "    \n",
    "    # Reshape data for analysis\n",
    "    population_data <- population_data %>% \n",
    "        pivot_wider(names_from = variable, values_from = value)\n",
    "    \n",
    "    return(population_data)\n",
    "}\n",
    "\n",
    "# Get total population data (all races)\n",
    "cat(\"Retrieving total population data from 2020 Census...\\n\")\n",
    "population_info_total <- get_population_variables(\"P12\")\n",
    "total_population_by_age_gender <- get_population_data(population_info_total$variables, population_info_total$labels)\n",
    "\n",
    "# Get white alone population data\n",
    "cat(\"Retrieving white alone population data from 2020 Census...\\n\")\n",
    "population_info_white <- get_population_variables(\"P12I\")\n",
    "total_population_by_age_gender_white <- get_population_data(population_info_white$variables, population_info_white$labels)\n",
    "\n",
    "cat(\"Census data retrieval complete\\n\")\n",
    "\n",
    "if(VERBOSE_PRINTS) {\n",
    "  cat(\"\\nTotal population data structure:\\n\")\n",
    "  str(total_population_by_age_gender)\n",
    "  cat(\"\\nWhite population data structure:\\n\")\n",
    "  str(total_population_by_age_gender_white)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Comparison: NASS vs Census Proportions\n",
    "\n",
    "Compare unadjusted and weighted proportions of white individuals in NASS sample against Census benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R # === PROPORTION ANALYSIS ===\n",
    "\n",
    "# ========================================\n",
    "# Stage 1a: Unadjusted Proportion Analysis\n",
    "# ========================================\n",
    "\n",
    "cat(\"=== STAGE 1A: UNADJUSTED PROPORTION ANALYSIS ===\\n\")\n",
    "\n",
    "# Calculate unadjusted proportion of white individuals in NASS\n",
    "unadjusted_proportion_white <- mean(NASS$WHITE, na.rm = TRUE)\n",
    "cat(\"Unadjusted proportion of WHITE in NASS:\", round(unadjusted_proportion_white, 4), \"\\n\")\n",
    "\n",
    "# Calculate reference proportion from entire US Census\n",
    "us_census_white_proportion <- sum(total_population_by_age_gender_white$Total, na.rm = TRUE) / \n",
    "                             sum(total_population_by_age_gender$Total, na.rm = TRUE)\n",
    "cat(\"US Census White alone proportion:\", round(us_census_white_proportion, 4), \"\\n\")\n",
    "\n",
    "# Statistical test for unadjusted proportion\n",
    "unadjusted_test <- prop.test(sum(NASS$WHITE, na.rm = TRUE), \n",
    "                            sum(!is.na(NASS$WHITE)), \n",
    "                            p = us_census_white_proportion)\n",
    "cat(\"\\nUnadjusted proportion test results:\\n\")\n",
    "print(unadjusted_test)\n",
    "\n",
    "# ========================================\n",
    "# Stage 1b: Weighted Proportion Analysis\n",
    "# ========================================\n",
    "\n",
    "cat(\"\\n=== STAGE 1B: WEIGHTED PROPORTION ANALYSIS ===\\n\")\n",
    "\n",
    "# Filter Census data for NASS-included states only\n",
    "filtered_total_population <- total_population_by_age_gender %>% \n",
    "    filter(NAME %in% states_in_nass)\n",
    "\n",
    "filtered_white_population <- total_population_by_age_gender_white %>% \n",
    "    filter(NAME %in% states_in_nass)\n",
    "\n",
    "# Calculate true proportion for NASS states\n",
    "total_population_nass_states <- sum(filtered_total_population$Total, na.rm = TRUE)\n",
    "total_white_population_nass_states <- sum(filtered_white_population$Total, na.rm = TRUE)\n",
    "true_proportion_white_nass_states <- total_white_population_nass_states / total_population_nass_states\n",
    "\n",
    "cat(\"True proportion of WHITE in NASS states:\", round(true_proportion_white_nass_states, 4), \"\\n\")\n",
    "\n",
    "# Calculate weighted proportion using survey design\n",
    "survey_design <- svydesign(ids = ~1, weights = ~DISCWT, data = NASS)\n",
    "weighted_proportion_white <- svymean(~WHITE, design = survey_design)\n",
    "cat(\"Weighted proportion of WHITE in NASS:\", round(coef(weighted_proportion_white), 4), \"\\n\")\n",
    "\n",
    "# Statistical test for weighted proportion\n",
    "weighted_test <- svyttest(WHITE ~ 1, design = survey_design, mu = true_proportion_white_nass_states)\n",
    "cat(\"\\nWeighted proportion test results:\\n\")\n",
    "print(weighted_test)\n",
    "\n",
    "cat(\"\\n=== PROPORTION ANALYSIS COMPLETE ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age-Gender Stratified Analysis\n",
    "\n",
    "Detailed comparison of white proportions by age group and gender between NASS sample and Census data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R # === AGE-GENDER STRATIFIED ANALYSIS ===\n",
    "\n",
    "# ========================================\n",
    "# Stage 2: Age-Gender Stratified Analysis\n",
    "# ========================================\n",
    "\n",
    "cat(\"=== STAGE 2: AGE-GENDER STRATIFIED ANALYSIS ===\\n\")\n",
    "\n",
    "# Define age groups matching Census categories\n",
    "age_breaks <- c(-Inf, 4, 9, 14, 17, 19, 20, 21, 24, 29, 34, 39, 44, 49, 54, 59, 61, 64, 66, 69, 74, 79, 84, Inf)\n",
    "age_labels <- c(\"Under 5 years\", \"5 to 9 years\", \"10 to 14 years\", \"15 to 17 years\", \"18 and 19 years\",\n",
    "                \"20 years\", \"21 years\", \"22 to 24 years\", \"25 to 29 years\", \"30 to 34 years\",\n",
    "                \"35 to 39 years\", \"40 to 44 years\", \"45 to 49 years\", \"50 to 54 years\", \"55 to 59 years\",\n",
    "                \"60 and 61 years\", \"62 to 64 years\", \"65 and 66 years\", \"67 to 69 years\", \"70 to 74 years\",\n",
    "                \"75 to 79 years\", \"80 to 84 years\", \"85 years and over\")\n",
    "\n",
    "# Create age group variable in NASS dataset\n",
    "NASS[, AGE_GROUP := cut(AGE, breaks = age_breaks, labels = age_labels, right = TRUE)]\n",
    "NASS[, GENDER := ifelse(FEMALE == 0, \"Male\", \"Female\")]\n",
    "\n",
    "cat(\"Created age groups and gender variables\\n\")\n",
    "\n",
    "# Calculate NASS proportions by age group and gender\n",
    "nass_proportions <- NASS[!is.na(AGE_GROUP) & !is.na(WHITE), \n",
    "                        .(total = .N,\n",
    "                          white = sum(WHITE, na.rm = TRUE),\n",
    "                          proportion_white = mean(WHITE, na.rm = TRUE)), \n",
    "                        by = .(AGE_GROUP, GENDER)]\n",
    "\n",
    "# Add confidence intervals\n",
    "nass_proportions[, ':='(\n",
    "  ci_lower = proportion_white - 1.96 * sqrt((proportion_white * (1 - proportion_white)) / total),\n",
    "  ci_upper = proportion_white + 1.96 * sqrt((proportion_white * (1 - proportion_white)) / total)\n",
    ")]\n",
    "\n",
    "cat(\"Calculated NASS proportions by age-gender groups\\n\")\n",
    "\n",
    "# Process Census data for comparison\n",
    "census_proportions <- total_population_by_age_gender_white %>% \n",
    "    select(NAME, starts_with(\"Male\"), starts_with(\"Female\")) %>% \n",
    "    pivot_longer(cols = -NAME, names_to = \"age_gender\", values_to = \"white_population\") %>% \n",
    "    separate(age_gender, into = c(\"gender\", \"age_group\"), sep = \": \") %>% \n",
    "    left_join(\n",
    "        total_population_by_age_gender %>% \n",
    "            select(NAME, starts_with(\"Male\"), starts_with(\"Female\")) %>% \n",
    "            pivot_longer(cols = -NAME, names_to = \"age_gender\", values_to = \"total_population\") %>% \n",
    "            separate(age_gender, into = c(\"gender\", \"age_group\"), sep = \": \"),\n",
    "        by = c(\"NAME\", \"gender\", \"age_group\")\n",
    "    ) %>% \n",
    "    filter(NAME %in% states_in_nass) %>%  # Filter for NASS states only\n",
    "    group_by(gender, age_group) %>% \n",
    "    summarize(\n",
    "        total_population = sum(total_population, na.rm = TRUE),\n",
    "        white_population = sum(white_population, na.rm = TRUE),\n",
    "        proportion_white = white_population / total_population,\n",
    "        .groups = 'drop'\n",
    "    ) %>% \n",
    "    filter(!is.na(age_group) & age_group != \"Total\")\n",
    "\n",
    "cat(\"Processed Census proportions by age-gender groups\\n\")\n",
    "\n",
    "# Convert to data.table for easier manipulation\n",
    "setDT(census_proportions)\n",
    "setDT(nass_proportions)\n",
    "\n",
    "# Convert age groups to factors for proper plotting\n",
    "census_proportions[, age_group := factor(age_group, levels = age_labels)]\n",
    "nass_proportions[, AGE_GROUP := factor(AGE_GROUP, levels = age_labels)]\n",
    "\n",
    "# Remove any missing age groups\n",
    "census_proportions <- census_proportions[!is.na(age_group)]\n",
    "nass_proportions <- nass_proportions[!is.na(AGE_GROUP)]\n",
    "\n",
    "cat(\"Data preparation complete\\n\")\n",
    "cat(\"NASS age-gender groups:\", nrow(nass_proportions), \"\\n\")\n",
    "cat(\"Census age-gender groups:\", nrow(census_proportions), \"\\n\")\n",
    "\n",
    "if(VERBOSE_PRINTS) {\n",
    "  cat(\"\\nSample NASS proportions:\\n\")\n",
    "  print(head(nass_proportions))\n",
    "  cat(\"\\nSample Census proportions:\\n\")\n",
    "  print(head(census_proportions))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Census Age-Gender Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R # === NASS CENSUS LINE PLOT ===\n",
    "\n",
    "# ========================================\n",
    "# Enhanced Visualization with 5-Year Age Groups - PART 1: Line Plot\n",
    "# ========================================\n",
    "\n",
    "cat(\"\\n=== GENERATING ENHANCED VISUALIZATIONS - PART 1 ===\\n\")\n",
    "\n",
    "# Check if ggplot2 is available for plotting\n",
    "if(require(\"ggplot2\", quietly = TRUE)) {\n",
    "  \n",
    "  # ========================================\n",
    "  # Create GENDER variable from FEMALE\n",
    "  # ========================================\n",
    "  \n",
    "  # First, ensure we have the GENDER variable\n",
    "  if(!\"GENDER\" %in% names(NASS)) {\n",
    "    if(\"FEMALE\" %in% names(NASS)) {\n",
    "      NASS[, GENDER := ifelse(FEMALE == 1, \"Female\", \"Male\")]\n",
    "      cat(\"Created GENDER variable from FEMALE\\n\")\n",
    "    } else {\n",
    "      cat(\"Error: Neither GENDER nor FEMALE variable found\\n\")\n",
    "      cat(\"Available variables related to gender:\\n\")\n",
    "      gender_vars <- names(NASS)[grepl(\"(?i)(gender|female|male|sex)\", names(NASS))]\n",
    "      if(length(gender_vars) > 0) {\n",
    "        cat(\"  \", paste(gender_vars, collapse = \", \"), \"\\n\")\n",
    "      } else {\n",
    "        cat(\"  No gender-related variables found\\n\")\n",
    "      }\n",
    "      stop(\"Cannot proceed without gender variable\")\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # ========================================\n",
    "  # Recreate age groups with 5-year brackets\n",
    "  # ========================================\n",
    "  \n",
    "  cat(\"Creating 5-year age brackets...\\n\")\n",
    "  \n",
    "  # Define 5-year age breaks\n",
    "  age_breaks_5yr <- c(-Inf, 4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 69, 74, 79, 84, Inf)\n",
    "  age_labels_5yr <- c(\"0-4\", \"5-9\", \"10-14\", \"15-19\", \"20-24\", \"25-29\", \"30-34\", \"35-39\", \n",
    "                      \"40-44\", \"45-49\", \"50-54\", \"55-59\", \"60-64\", \"65-69\", \"70-74\", \n",
    "                      \"75-79\", \"80-84\", \"85+\")\n",
    "  \n",
    "  # Recreate NASS age groups with 5-year brackets\n",
    "  NASS[, AGE_GROUP_5YR := cut(AGE, breaks = age_breaks_5yr, labels = age_labels_5yr, right = TRUE)]\n",
    "  \n",
    "  # Calculate NASS proportions by 5-year age group and gender\n",
    "  nass_proportions_5yr <- NASS[!is.na(AGE_GROUP_5YR) & !is.na(WHITE), \n",
    "                              .(total = .N,\n",
    "                                white = sum(WHITE, na.rm = TRUE),\n",
    "                                proportion_white = mean(WHITE, na.rm = TRUE)), \n",
    "                              by = .(AGE_GROUP_5YR, GENDER)]\n",
    "  \n",
    "  # Add confidence intervals\n",
    "  nass_proportions_5yr[, ':='(\n",
    "    ci_lower = proportion_white - 1.96 * sqrt((proportion_white * (1 - proportion_white)) / total),\n",
    "    ci_upper = proportion_white + 1.96 * sqrt((proportion_white * (1 - proportion_white)) / total)\n",
    "  )]\n",
    "  \n",
    "  # ========================================\n",
    "  # Process Census data for 5-year groups\n",
    "  # ========================================\n",
    "  \n",
    "  cat(\"Processing Census data for 5-year age groups...\\n\")\n",
    "  \n",
    "  # Check if census data exists\n",
    "  if(!exists(\"total_population_by_age_gender_white\") || !exists(\"total_population_by_age_gender\")) {\n",
    "    cat(\"Warning: Census data not available. Creating placeholder plot...\\n\")\n",
    "    \n",
    "    # Create placeholder plot with just NASS data\n",
    "    placeholder_plot <- ggplot(nass_proportions_5yr, aes(x = AGE_GROUP_5YR, y = proportion_white, \n",
    "                                                         color = GENDER, group = GENDER)) +\n",
    "      geom_line(linewidth = 1) +\n",
    "      geom_point() +\n",
    "      geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper, fill = GENDER), \n",
    "                  alpha = 0.2, color = NA) +\n",
    "      facet_wrap(~GENDER, ncol = 1) +\n",
    "      \n",
    "      scale_color_manual(values = c(\"Female\" = \"#d62728\", \"Male\" = \"#1f77b4\")) +\n",
    "      scale_fill_manual(values = c(\"Female\" = \"#d62728\", \"Male\" = \"#1f77b4\"), guide = \"none\") +\n",
    "      \n",
    "      labs(title = \"White Proportion by 5-Year Age Groups: NASS Data Only\",\n",
    "           subtitle = \"Census comparison not available - showing NASS data with confidence intervals\",\n",
    "           x = \"Age Group\",\n",
    "           y = \"Proportion White\",\n",
    "           color = \"Gender\") +\n",
    "           \n",
    "      theme_poster +\n",
    "      theme(axis.text.x = element_text(angle = 45, hjust = 1),\n",
    "            legend.position = \"bottom\") +\n",
    "      scale_y_continuous(limits = c(0, 1), labels = scales::percent)\n",
    "    \n",
    "    print(placeholder_plot)\n",
    "    \n",
    "    save_plot_for_poster(placeholder_plot, \n",
    "                        name = \"census_comparison_lines\", \n",
    "                        description = \"White Proportion by Age Groups: NASS Only (Census data unavailable)\",\n",
    "                        category = \"validation\")\n",
    "    \n",
    "    cat(\"Placeholder plot generated successfully\\n\")\n",
    "    \n",
    "  } else {\n",
    "    \n",
    "    # Map detailed Census age groups to 5-year brackets\n",
    "    census_age_mapping <- data.table(\n",
    "      census_age = c(\"Under 5 years\", \"5 to 9 years\", \"10 to 14 years\", \"15 to 17 years\", \n",
    "                     \"18 and 19 years\", \"20 years\", \"21 years\", \"22 to 24 years\", \n",
    "                     \"25 to 29 years\", \"30 to 34 years\", \"35 to 39 years\", \"40 to 44 years\",\n",
    "                     \"45 to 49 years\", \"50 to 54 years\", \"55 to 59 years\", \"60 and 61 years\",\n",
    "                     \"62 to 64 years\", \"65 and 66 years\", \"67 to 69 years\", \"70 to 74 years\",\n",
    "                     \"75 to 79 years\", \"80 to 84 years\", \"85 years and over\"),\n",
    "      age_5yr = c(\"0-4\", \"5-9\", \"10-14\", \"15-19\", \"15-19\", \"20-24\", \"20-24\", \"20-24\",\n",
    "                  \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \"55-59\", \"60-64\",\n",
    "                  \"60-64\", \"65-69\", \"65-69\", \"70-74\", \"75-79\", \"80-84\", \"85+\")\n",
    "    )\n",
    "    \n",
    "    # Process Census data with 5-year grouping\n",
    "    if(require(\"tidyr\", quietly = TRUE) && require(\"dplyr\", quietly = TRUE)) {\n",
    "      \n",
    "      census_proportions_5yr <- total_population_by_age_gender_white %>% \n",
    "        select(NAME, starts_with(\"Male\"), starts_with(\"Female\")) %>% \n",
    "        pivot_longer(cols = -NAME, names_to = \"age_gender\", values_to = \"white_population\") %>% \n",
    "        separate(age_gender, into = c(\"gender\", \"age_group\"), sep = \": \") %>% \n",
    "        left_join(\n",
    "          total_population_by_age_gender %>% \n",
    "            select(NAME, starts_with(\"Male\"), starts_with(\"Female\")) %>% \n",
    "            pivot_longer(cols = -NAME, names_to = \"age_gender\", values_to = \"total_population\") %>% \n",
    "            separate(age_gender, into = c(\"gender\", \"age_group\"), sep = \": \"),\n",
    "          by = c(\"NAME\", \"gender\", \"age_group\")\n",
    "        ) %>% \n",
    "        filter(NAME %in% states_in_nass & !is.na(age_group) & age_group != \"Total\") %>%\n",
    "        left_join(census_age_mapping, by = c(\"age_group\" = \"census_age\")) %>%\n",
    "        filter(!is.na(age_5yr)) %>%\n",
    "        group_by(gender, age_5yr) %>% \n",
    "        summarize(\n",
    "          total_population = sum(total_population, na.rm = TRUE),\n",
    "          white_population = sum(white_population, na.rm = TRUE),\n",
    "          proportion_white = white_population / total_population,\n",
    "          .groups = 'drop'\n",
    "        )\n",
    "      \n",
    "      # Convert to data.table\n",
    "      setDT(census_proportions_5yr)\n",
    "      \n",
    "      cat(\"Data preparation complete for 5-year age groups\\n\")\n",
    "      \n",
    "      # ========================================\n",
    "      # Line comparison plot with 5-year groups\n",
    "      # ========================================\n",
    "      \n",
    "      # Prepare data for plotting\n",
    "      plot_data_nass_5yr <- nass_proportions_5yr[, .(\n",
    "        AGE_GROUP = AGE_GROUP_5YR, GENDER, proportion_white, ci_lower, ci_upper, source = \"NASS\"\n",
    "      )]\n",
    "      \n",
    "      plot_data_census_5yr <- census_proportions_5yr[, .(\n",
    "        AGE_GROUP = factor(age_5yr, levels = age_labels_5yr), \n",
    "        GENDER = gender, \n",
    "        proportion_white, \n",
    "        source = \"Census\"\n",
    "      )]\n",
    "      plot_data_census_5yr[, ':='(ci_lower = proportion_white, ci_upper = proportion_white)]\n",
    "      \n",
    "      plot_data_5yr <- rbind(plot_data_nass_5yr, plot_data_census_5yr, fill = TRUE)\n",
    "      \n",
    "      # Create the comparison plot with UNIFIED THEME AND COLORS\n",
    "      age_gender_plot_5yr <- ggplot(plot_data_5yr, aes(x = AGE_GROUP, y = proportion_white, \n",
    "                                                       color = source, group = interaction(source, GENDER))) +\n",
    "        geom_line(linewidth = 1) +\n",
    "        geom_point() +\n",
    "        geom_ribbon(data = plot_data_5yr[source == \"NASS\"], \n",
    "                    aes(ymin = ci_lower, ymax = ci_upper, fill = source), \n",
    "                    alpha = 0.2, color = NA) +\n",
    "        facet_wrap(~GENDER, ncol = 1) +\n",
    "        \n",
    "        # UPDATED: Apply unified colors from census_colors\n",
    "        scale_color_manual(\n",
    "          values = c(\"NASS\" = \"#1f77b4\", \"Census\" = \"#d62728\"),  # Blue for NASS, Red for Census\n",
    "          name = \"Data Source\"\n",
    "        ) +\n",
    "        scale_fill_manual(\n",
    "          values = c(\"NASS\" = \"#1f77b4\"),  # Blue fill for NASS confidence intervals\n",
    "          guide = \"none\"  # Hide fill legend since it's redundant with color\n",
    "        ) +\n",
    "        \n",
    "        labs(title = \"Race Proportion by Age Groups vs Census\",\n",
    "             subtitle = \"Census comparions across age groups and gender (w/ CI)\",\n",
    "             x = \"Age Group\",\n",
    "             y = \"Proportion White\",\n",
    "             color = \"Data Source\",\n",
    "             fill = \"Confidence Interval\") +\n",
    "             \n",
    "        # UPDATED: Apply unified poster theme\n",
    "        theme_poster +\n",
    "        theme(axis.text.x = element_text(angle = 45, hjust = 1),\n",
    "              legend.position = \"bottom\") +\n",
    "        scale_y_continuous(limits = c(0, 1), labels = scales::percent)\n",
    "      \n",
    "      print(age_gender_plot_5yr)\n",
    "\n",
    "      save_plot_for_poster(age_gender_plot_5yr, \n",
    "                        name = \"census_comparison_lines\", \n",
    "                        description = \"White Proportion by Age Groups: NASS vs Census\",\n",
    "                        category = \"validation\")\n",
    "      \n",
    "      cat(\"Line plot visualization generated successfully with unified theme\\n\")\n",
    "      \n",
    "    } else {\n",
    "      cat(\"Warning: tidyr/dplyr not available for census data processing\\n\")\n",
    "    }\n",
    "  }\n",
    "  \n",
    "} else {\n",
    "    cat(\"ggplot2 not available \\n\")\n",
    "}\n",
    "\n",
    "cat(\"\\n=== PART 1 COMPLETE ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of Demographic Age curve to census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R # === ENHANCED FILLED AREA HOURGLASS PLOT ===\n",
    "\n",
    "# ========================================\n",
    "# PART 2: Enhanced Filled Area Hourglass Plot - UNIFIED THEME\n",
    "# ========================================\n",
    "\n",
    "cat(\"\\n=== GENERATING ENHANCED VISUALIZATIONS - PART 2 ===\\n\")\n",
    "\n",
    "# Check if ggplot2 is available for plotting\n",
    "if(require(\"ggplot2\", quietly = TRUE)) {\n",
    "  \n",
    "  cat(\"Generating enhanced filled area hourglass plot with unified styling...\\n\")\n",
    "  \n",
    "  # Calculate total populations for percentage calculations\n",
    "  nass_total_pop <- sum(nass_proportions_5yr$total)\n",
    "  census_total_pop <- sum(census_proportions_5yr$total_population)\n",
    "  \n",
    "  # Prepare NASS hourglass data (percentage of total population)\n",
    "  hourglass_data_nass <- nass_proportions_5yr[, .(\n",
    "    AGE_GROUP = AGE_GROUP_5YR, \n",
    "    GENDER, \n",
    "    count = total,\n",
    "    percentage = (total / nass_total_pop) * 100,\n",
    "    source = \"NASS\"\n",
    "  )]\n",
    "  \n",
    "  # Make male percentages negative for left side\n",
    "  hourglass_data_nass[GENDER == \"Male\", population_pct := -percentage]\n",
    "  hourglass_data_nass[GENDER == \"Female\", population_pct := percentage]\n",
    "  \n",
    "  # Prepare Census hourglass data (percentage of total population)\n",
    "  hourglass_data_census <- census_proportions_5yr[, .(\n",
    "    AGE_GROUP = factor(age_5yr, levels = age_labels_5yr),\n",
    "    GENDER = gender,\n",
    "    count = total_population,\n",
    "    percentage = (total_population / census_total_pop) * 100,\n",
    "    source = \"Census\"\n",
    "  )]\n",
    "  \n",
    "  # Make male percentages negative for left side\n",
    "  hourglass_data_census[GENDER == \"Male\", population_pct := -percentage]\n",
    "  hourglass_data_census[GENDER == \"Female\", population_pct := percentage]\n",
    "  \n",
    "  # Combine both datasets\n",
    "  hourglass_combined <- rbind(hourglass_data_nass, hourglass_data_census)\n",
    "  \n",
    "  # Create age group numeric variable for smooth areas\n",
    "  hourglass_combined[, age_numeric := as.numeric(AGE_GROUP)]\n",
    "  \n",
    "  # Create enhanced filled area hourglass plot with UNIFIED THEME AND COLORS\n",
    "  hourglass_plot_enhanced <- ggplot(hourglass_combined, \n",
    "                                   aes(x = age_numeric, y = population_pct, \n",
    "                                       fill = interaction(GENDER, source))) +\n",
    "    geom_area(alpha = 0.7, position = \"identity\") +\n",
    "    coord_flip() +\n",
    "    scale_x_continuous(\n",
    "      breaks = 1:length(age_labels_5yr),\n",
    "      labels = age_labels_5yr\n",
    "    ) +\n",
    "    scale_y_continuous(\n",
    "      labels = function(x) paste0(abs(x), \"%\"),\n",
    "      breaks = seq(-8, 8, 2)\n",
    "    ) +\n",
    "    \n",
    "    # UPDATED: Use unified census_colors from our color system\n",
    "    scale_fill_manual(\n",
    "      values = c(\n",
    "        \"Female.Census\" = \"#ff9999\",  # Light red - matches census_colors\n",
    "        \"Female.NASS\" = \"#87ceeb\",    # Light blue - matches census_colors\n",
    "        \"Male.Census\" = \"#d62728\",    # Deep red - matches census_colors  \n",
    "        \"Male.NASS\" = \"#1f77b4\"       # Deep blue - matches census_colors\n",
    "      ),\n",
    "      labels = c(\"Female Census\", \"Male Census\", \"Female NASS\", \"Male NASS\"),\n",
    "      name = \"Gender & Source\"\n",
    "    ) +\n",
    "    \n",
    "    # UPDATED: Apply unified poster theme\n",
    "    labs(title = \"Demographics Proportions: NASS vs Census\",\n",
    "         subtitle = \"Percentage of total population by 5-year age groups\",\n",
    "         x = \"Age Group\",\n",
    "         y = \"Percentage of Total Population\",\n",
    "         fill = \"Gender & Source\") +\n",
    "    \n",
    "    # UPDATED: Use theme_poster as base with specific overrides\n",
    "    theme_poster +\n",
    "    theme(\n",
    "      # Keep poster theme base but override specific elements for this plot\n",
    "      axis.text.y = element_text(size = 9),\n",
    "      panel.grid.major.y = element_blank(),\n",
    "      legend.text = element_text(size = 10),      # Slightly larger for readability\n",
    "      legend.title = element_text(size = 11),     # Poster-sized legend title\n",
    "      legend.position = \"bottom\"\n",
    "    ) +\n",
    "    \n",
    "    geom_hline(yintercept = 0, color = \"black\", linewidth = 0.5) +\n",
    "    guides(\n",
    "      fill = guide_legend(nrow = 2, override.aes = list(alpha = 0.8))\n",
    "    )\n",
    "  \n",
    "  print(hourglass_plot_enhanced)\n",
    "\n",
    "  save_plot_for_poster(hourglass_plot_enhanced, \n",
    "                    name = \"census_comparison_hourglass_enhanced\", \n",
    "                    description = \"Enhanced Filled Area Hourglass: NASS vs Census Population Distribution\",\n",
    "                    category = \"validation\")\n",
    "  \n",
    "  cat(\"Enhanced filled area hourglass visualization generated with unified styling:\\n\")\n",
    "  cat(\"- Applied theme_poster base theme with custom overrides\\n\")\n",
    "  cat(\"- Used unified census_colors (red-white-blue scheme)\\n\") \n",
    "  cat(\"- Maintained left-aligned titles (poster style)\\n\")\n",
    "  cat(\"- Age groups: 5-year brackets (0-4, 5-9, etc.)\\n\")\n",
    "  cat(\"- Style: Solid filled areas from 0% to value\\n\")\n",
    "  cat(\"- Colors: NASS blues, Census reds (consistent with line plot)\\n\")\n",
    "  cat(\"- Shows: Percentage of total population\\n\")\n",
    "  cat(\"- Comparison: NASS sample vs Census data\\n\")\n",
    "  \n",
    "} else {\n",
    "    cat(\"ggplot2 not available \\n\")\n",
    "}\n",
    "\n",
    "cat(\"\\n=== PART 2 COMPLETE ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-level Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R # === MULTI-LEVEL MODELING ANALYSIS ===\n",
    "\n",
    "# ========================================\n",
    "# MULTI-LEVEL MODELING FOR NASS DATASET\n",
    "# ========================================\n",
    "\n",
    "cat(\"=== MULTI-LEVEL MODELING ANALYSIS ===\\n\")\n",
    "cat(\"Examining hierarchical structure: Patients nested within Hospitals\\n\\n\")\n",
    "\n",
    "# Install required packages for multi-level modeling\n",
    "required_packages <- c(\"lme4\", \"broom.mixed\", \"performance\", \"sjPlot\")\n",
    "\n",
    "for(pkg in required_packages) {\n",
    "  if(!require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "    cat(\"Installing\", pkg, \"...\\n\")\n",
    "    install.packages(pkg, quiet = TRUE)\n",
    "    library(pkg, character.only = TRUE, quietly = TRUE)\n",
    "  }\n",
    "}\n",
    "\n",
    "cat(\"Multi-level modeling packages loaded successfully\\n\\n\")\n",
    "\n",
    "# ========================================\n",
    "# SMART SAMPLING FOR SPEED\n",
    "# ========================================\n",
    "\n",
    "cat(\"SAMPLING DATA FOR MULTI-LEVEL ANALYSIS\\n\")\n",
    "cat(paste(rep(\"=\", 50), collapse = \"\"), \"\\n\")\n",
    "\n",
    "# Use a manageable subset for demonstration (targeting <1 minute runtime)\n",
    "set.seed(12345)\n",
    "sample_size <- min(25000, nrow(NASS))  # Max 25K observations for speed\n",
    "sample_indices <- sample(nrow(NASS), sample_size)\n",
    "\n",
    "# Create modeling dataset with complete cases from sample\n",
    "model_data_full <- NASS[sample_indices][!is.na(AGE) & !is.na(WHITE) & !is.na(ZIPINC_QRTL) & \n",
    "                                        !is.na(PAY1) & !is.na(HOSP_NASS) & \n",
    "                                        ZIPINC_QRTL %in% 1:4 & PAY1 %in% 1:6]\n",
    "\n",
    "cat(\"Original NASS data:\", nrow(NASS), \"observations\\n\")\n",
    "cat(\"Sample for modeling:\", sample_size, \"observations\\n\")\n",
    "cat(\"Complete cases for modeling:\", nrow(model_data_full), \"observations\\n\")\n",
    "cat(\"Final retention rate:\", round(100 * nrow(model_data_full) / sample_size, 1), \"%\\n\\n\")\n",
    "\n",
    "# Further sampling if still too large\n",
    "if(nrow(model_data_full) > 15000) {\n",
    "  model_indices <- sample(nrow(model_data_full), 15000)\n",
    "  model_data <- model_data_full[model_indices]\n",
    "  cat(\"Additional sampling to 15,000 observations for speed\\n\")\n",
    "} else {\n",
    "  model_data <- model_data_full\n",
    "}\n",
    "\n",
    "cat(\"Final modeling dataset:\", nrow(model_data), \"observations\\n\\n\")\n",
    "\n",
    "# ========================================\n",
    "# 1. DATA PREPARATION FOR MODELING\n",
    "# ========================================\n",
    "\n",
    "cat(\"1. PREPARING DATA FOR MULTI-LEVEL ANALYSIS\\n\")\n",
    "cat(paste(rep(\"=\", 50), collapse = \"\"), \"\\n\")\n",
    "\n",
    "# Create modeling variables\n",
    "model_data[, ':='(\n",
    "  # Outcome: Binary indicator for private insurance (reference = public/other)\n",
    "  private_insurance = as.numeric(PAY1 == 3),\n",
    "  \n",
    "  # Patient-level predictors\n",
    "  age_centered = scale(AGE)[,1],  # Centered age\n",
    "  high_income = as.numeric(ZIPINC_QRTL %in% 3:4),  # Upper income quartiles\n",
    "  white_race = as.numeric(WHITE),\n",
    "  \n",
    "  # Hospital-level predictors\n",
    "  teaching_hospital = as.numeric(HOSP_TEACH == 1),\n",
    "  urban_hospital = as.numeric(HOSP_LOCATION == 1),\n",
    "  large_hospital = as.numeric(HOSP_BEDSIZE_CAT == 3)\n",
    ")]\n",
    "\n",
    "# Hospital-level summary statistics\n",
    "hospital_summary <- model_data[, .(\n",
    "  n_patients = .N,\n",
    "  pct_private = round(100 * mean(private_insurance), 1),\n",
    "  pct_white = round(100 * mean(white_race), 1),\n",
    "  pct_high_income = round(100 * mean(high_income), 1),\n",
    "  mean_age = round(mean(AGE), 1)\n",
    "), by = HOSP_NASS]\n",
    "\n",
    "cat(\"HOSPITAL-LEVEL VARIATION:\\n\")\n",
    "cat(\"Number of hospitals:\", nrow(hospital_summary), \"\\n\")\n",
    "cat(\"Patients per hospital - Mean:\", round(mean(hospital_summary$n_patients), 1), \n",
    "    \"| Range:\", min(hospital_summary$n_patients), \"to\", max(hospital_summary$n_patients), \"\\n\")\n",
    "cat(\"Private insurance % - Mean:\", round(mean(hospital_summary$pct_private), 1), \n",
    "    \"| Range:\", min(hospital_summary$pct_private), \"% to\", max(hospital_summary$pct_private), \"%\\n\\n\")\n",
    "\n",
    "# ========================================\n",
    "# 2. BASELINE SINGLE-LEVEL MODEL\n",
    "# ========================================\n",
    "\n",
    "cat(\"2. BASELINE SINGLE-LEVEL LOGISTIC REGRESSION\\n\")\n",
    "cat(paste(rep(\"=\", 50), collapse = \"\"), \"\\n\")\n",
    "\n",
    "# Fit standard logistic regression (ignoring hospital clustering)\n",
    "baseline_model <- glm(private_insurance ~ white_race + high_income + age_centered + \n",
    "                     teaching_hospital + urban_hospital + large_hospital,\n",
    "                     data = model_data, family = binomial)\n",
    "\n",
    "cat(\"BASELINE MODEL RESULTS:\\n\")\n",
    "baseline_summary <- summary(baseline_model)\n",
    "print(baseline_summary$coefficients)\n",
    "\n",
    "# Calculate odds ratios\n",
    "baseline_or <- exp(coef(baseline_model))\n",
    "baseline_ci <- exp(confint(baseline_model))\n",
    "\n",
    "cat(\"\\nODDS RATIOS (95% CI):\\n\")\n",
    "for(i in 2:length(baseline_or)) {  # Skip intercept\n",
    "  var_name <- names(baseline_or)[i]\n",
    "  or_value <- baseline_or[i]\n",
    "  ci_lower <- baseline_ci[i, 1]\n",
    "  ci_upper <- baseline_ci[i, 2]\n",
    "  cat(sprintf(\"%-20s: %.2f (%.2f - %.2f)\\n\", var_name, or_value, ci_lower, ci_upper))\n",
    "}\n",
    "\n",
    "cat(\"\\nModel AIC:\", round(AIC(baseline_model), 1), \"\\n\\n\")\n",
    "\n",
    "# ========================================\n",
    "# 3. MULTI-LEVEL MODEL WITH RANDOM INTERCEPTS\n",
    "# ========================================\n",
    "\n",
    "cat(\"3. MULTI-LEVEL MODEL: RANDOM INTERCEPTS\\n\")\n",
    "cat(paste(rep(\"=\", 50), collapse = \"\"), \"\\n\")\n",
    "\n",
    "# Fit random intercept model\n",
    "random_intercept_model <- glmer(private_insurance ~ white_race + high_income + age_centered + \n",
    "                               teaching_hospital + urban_hospital + large_hospital +\n",
    "                               (1 | HOSP_NASS),\n",
    "                               data = model_data, \n",
    "                               family = binomial,\n",
    "                               control = glmerControl(optimizer = \"bobyqa\"))\n",
    "\n",
    "cat(\"RANDOM INTERCEPT MODEL RESULTS:\\n\")\n",
    "print(summary(random_intercept_model))\n",
    "\n",
    "# Extract variance components\n",
    "variance_components <- VarCorr(random_intercept_model)\n",
    "hospital_variance <- as.numeric(variance_components$HOSP_NASS[1])\n",
    "residual_variance <- pi^2/3  # For logistic regression\n",
    "\n",
    "# Calculate ICC\n",
    "icc <- hospital_variance / (hospital_variance + residual_variance)\n",
    "cat(\"\\nVARIANCE COMPONENTS:\\n\")\n",
    "cat(\"Hospital-level variance:\", round(hospital_variance, 3), \"\\n\")\n",
    "cat(\"Individual-level variance:\", round(residual_variance, 3), \"\\n\")\n",
    "cat(\"Intraclass Correlation (ICC):\", round(icc, 3), \"\\n\")\n",
    "cat(\"Interpretation: \", round(100 * icc, 1), \"% of variation is between hospitals\\n\\n\")\n",
    "\n",
    "# Model comparison\n",
    "cat(\"MODEL COMPARISON:\\n\")\n",
    "cat(\"Baseline AIC:\", round(AIC(baseline_model), 1), \"\\n\")\n",
    "cat(\"Multi-level AIC:\", round(AIC(random_intercept_model), 1), \"\\n\")\n",
    "cat(\"AIC improvement:\", round(AIC(baseline_model) - AIC(random_intercept_model), 1), \"\\n\\n\")\n",
    "\n",
    "# ========================================\n",
    "# 4. MULTI-LEVEL MODEL WITH RANDOM SLOPES\n",
    "# ========================================\n",
    "\n",
    "cat(\"4. MULTI-LEVEL MODEL: RANDOM SLOPES\\n\")\n",
    "cat(paste(rep(\"=\", 50), collapse = \"\"), \"\\n\")\n",
    "\n",
    "# Fit random slope model (race effect varies by hospital)\n",
    "tryCatch({\n",
    "  random_slope_model <- glmer(private_insurance ~ white_race + high_income + age_centered + \n",
    "                             teaching_hospital + urban_hospital + large_hospital +\n",
    "                             (1 + white_race | HOSP_NASS),\n",
    "                             data = model_data, \n",
    "                             family = binomial,\n",
    "                             control = glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 20000)))\n",
    "  \n",
    "  cat(\"RANDOM SLOPE MODEL RESULTS:\\n\")\n",
    "  print(summary(random_slope_model))\n",
    "  \n",
    "  # Extract random effects variance\n",
    "  random_effects_var <- VarCorr(random_slope_model)\n",
    "  cat(\"\\nRANDOM EFFECTS VARIANCE:\\n\")\n",
    "  print(random_effects_var)\n",
    "  \n",
    "  # Model comparison\n",
    "  cat(\"\\nMODEL COMPARISON (AIC):\\n\")\n",
    "  cat(\"Baseline model:\", round(AIC(baseline_model), 1), \"\\n\")\n",
    "  cat(\"Random intercept:\", round(AIC(random_intercept_model), 1), \"\\n\")\n",
    "  cat(\"Random slope:\", round(AIC(random_slope_model), 1), \"\\n\")\n",
    "  \n",
    "  # Likelihood ratio test\n",
    "  anova_result <- anova(random_intercept_model, random_slope_model)\n",
    "  cat(\"\\nLIKELIHOOD RATIO TEST (Random Intercept vs Random Slope):\\n\")\n",
    "  print(anova_result)\n",
    "  \n",
    "  best_model <- random_slope_model\n",
    "  cat(\"\\nRandom slope model fitted successfully\\n\\n\")\n",
    "  \n",
    "}, error = function(e) {\n",
    "  cat(\"Random slope model failed to converge, using random intercept model\\n\")\n",
    "  cat(\"Error:\", e$message, \"\\n\\n\")\n",
    "  best_model <- random_intercept_model\n",
    "})\n",
    "\n",
    "# Use the best fitting model\n",
    "if(!exists(\"best_model\")) {\n",
    "  best_model <- random_intercept_model\n",
    "}\n",
    "\n",
    "# ========================================\n",
    "# 5. MODEL INTERPRETATION & FINDINGS\n",
    "# ========================================\n",
    "\n",
    "cat(\"5. FINAL MODEL INTERPRETATION\\n\")\n",
    "cat(paste(rep(\"=\", 50), collapse = \"\"), \"\\n\")\n",
    "\n",
    "# Extract fixed effects\n",
    "fixed_effects <- fixef(best_model)\n",
    "fixed_se <- sqrt(diag(vcov(best_model)))\n",
    "\n",
    "# Calculate odds ratios and confidence intervals\n",
    "or_estimates <- exp(fixed_effects)\n",
    "or_ci_lower <- exp(fixed_effects - 1.96 * fixed_se)\n",
    "or_ci_upper <- exp(fixed_effects + 1.96 * fixed_se)\n",
    "\n",
    "cat(\"FIXED EFFECTS - ODDS RATIOS FOR PRIVATE INSURANCE:\\n\")\n",
    "cat(\"(Reference: Public insurance, non-white, low income, average age, non-teaching, rural, small hospital)\\n\\n\")\n",
    "\n",
    "for(i in 2:length(or_estimates)) {  # Skip intercept\n",
    "  var_name <- names(or_estimates)[i]\n",
    "  or_value <- or_estimates[i]\n",
    "  ci_lower <- or_ci_lower[i]\n",
    "  ci_upper <- or_ci_upper[i]\n",
    "  \n",
    "  # Create interpretable labels\n",
    "  interpretation <- switch(var_name,\n",
    "    \"white_race\" = \"White vs Non-white race\",\n",
    "    \"high_income\" = \"High vs Low income (Q3-Q4 vs Q1-Q2)\",\n",
    "    \"age_centered\" = \"Per 1 SD increase in age\",\n",
    "    \"teaching_hospital\" = \"Teaching vs Non-teaching hospital\",\n",
    "    \"urban_hospital\" = \"Urban vs Rural hospital\",\n",
    "    \"large_hospital\" = \"Large vs Small hospital\",\n",
    "    var_name\n",
    "  )\n",
    "  \n",
    "  significance <- ifelse(ci_lower > 1 | ci_upper < 1, \"***\", \"   \")\n",
    "  \n",
    "  cat(sprintf(\"%-35s: %.2f (%.2f - %.2f) %s\\n\", \n",
    "              interpretation, or_value, ci_lower, ci_upper, significance))\n",
    "}\n",
    "\n",
    "cat(\"\\n*** = 95% CI excludes 1.0 (statistically significant)\\n\\n\")\n",
    "\n",
    "# ========================================\n",
    "# 6. KEY FINDINGS SUMMARY\n",
    "# ========================================\n",
    "\n",
    "cat(\"6. KEY FINDINGS SUMMARY\\n\")\n",
    "cat(paste(rep(\"=\", 50), collapse = \"\"), \"\\n\")\n",
    "\n",
    "# Hospital variation\n",
    "if(exists(\"hospital_variance\")) {\n",
    "  cat(\"HOSPITAL-LEVEL VARIATION:\\n\")\n",
    "  cat(\"- \", round(100 * icc, 1), \"% of variation in private insurance access is between hospitals\\n\")\n",
    "  cat(\"- Substantial clustering effects - multi-level modeling was necessary\\n\\n\")\n",
    "}\n",
    "\n",
    "# Patient-level effects\n",
    "white_or <- or_estimates[\"white_race\"]\n",
    "income_or <- or_estimates[\"high_income\"]\n",
    "\n",
    "cat(\"PATIENT-LEVEL DISPARITIES:\\n\")\n",
    "if(!is.na(white_or)) {\n",
    "  cat(\"- White patients have \", round(white_or, 2), \"x odds of private insurance vs non-white\\n\")\n",
    "}\n",
    "if(!is.na(income_or)) {\n",
    "  cat(\"- High-income patients have \", round(income_or, 2), \"x odds of private insurance vs low-income\\n\")\n",
    "}\n",
    "\n",
    "# Hospital-level effects\n",
    "teaching_or <- or_estimates[\"teaching_hospital\"]\n",
    "urban_or <- or_estimates[\"urban_hospital\"]\n",
    "\n",
    "cat(\"\\nHOSPITAL-LEVEL EFFECTS:\\n\")\n",
    "if(!is.na(teaching_or)) {\n",
    "  cat(\"- Teaching hospitals associated with \", round(teaching_or, 2), \"x odds of private insurance\\n\")\n",
    "}\n",
    "if(!is.na(urban_or)) {\n",
    "  cat(\"- Urban hospitals associated with \", round(urban_or, 2), \"x odds of private insurance\\n\")\n",
    "}\n",
    "\n",
    "cat(\"\\nCONFIRMS EARLIER FINDINGS:\\n\")\n",
    "cat(\"- Significant racial and socioeconomic disparities in insurance access\\n\")\n",
    "cat(\"- Hospital characteristics independently predict patient insurance mix\\n\")\n",
    "cat(\"- Multi-level structure reveals important clustering at hospital level\\n\\n\")\n",
    "\n",
    "# Model performance\n",
    "cat(\"MODEL PERFORMANCE:\\n\")\n",
    "if(require(\"performance\", quietly = TRUE)) {\n",
    "  # Calculate model performance metrics\n",
    "  model_performance <- model_performance(best_model)\n",
    "  cat(\"- AIC:\", round(AIC(best_model), 1), \"\\n\")\n",
    "  cat(\"- Model successfully accounts for hospital-level clustering\\n\")\n",
    "  cat(\"- Robust estimates of patient and hospital-level effects\\n\")\n",
    "}\n",
    "\n",
    "cat(\"\\nMULTI-LEVEL MODELING ANALYSIS COMPLETE\\n\")\n",
    "cat(\"Results confirm substantial disparities at both patient and hospital levels\\n\")\n",
    "cat(\"Note: Analysis performed on sample of\", nrow(model_data), \"observations for computational efficiency\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R # === ODDS RATIOS FOREST PLOT - UNIFIED THEME ===\n",
    "\n",
    "# ========================================\n",
    "# IMPROVED ODDS RATIOS FOREST PLOT - UNIFIED STYLING\n",
    "# ========================================\n",
    "\n",
    "cat(\"=== CREATING IMPROVED ODDS RATIOS FOREST PLOT WITH UNIFIED THEME ===\\n\")\n",
    "\n",
    "# Install visualization packages if needed\n",
    "viz_packages <- c(\"ggplot2\", \"scales\")\n",
    "\n",
    "for(pkg in viz_packages) {\n",
    "  if(!require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "    cat(\"Installing\", pkg, \"...\\n\")\n",
    "    install.packages(pkg, quiet = TRUE)\n",
    "    library(pkg, character.only = TRUE, quietly = TRUE)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Check if best_model exists\n",
    "if(exists(\"best_model\")) {\n",
    "  \n",
    "  cat(\"Creating Improved Odds Ratios Forest Plot with unified styling\\n\")\n",
    "  \n",
    "  # Extract fixed effects\n",
    "  fixed_effects <- fixef(best_model)\n",
    "  fixed_se <- sqrt(diag(vcov(best_model)))\n",
    "  \n",
    "  # Calculate odds ratios and confidence intervals\n",
    "  or_estimates <- exp(fixed_effects)\n",
    "  or_ci_lower <- exp(fixed_effects - 1.96 * fixed_se)\n",
    "  or_ci_upper <- exp(fixed_effects + 1.96 * fixed_se)\n",
    "  \n",
    "  # Create data frame for plotting (excluding intercept)\n",
    "  or_data <- data.frame(\n",
    "    Variable = names(or_estimates)[-1],  # Remove intercept\n",
    "    OR = or_estimates[-1],\n",
    "    CI_Lower = or_ci_lower[-1],\n",
    "    CI_Upper = or_ci_upper[-1],\n",
    "    stringsAsFactors = FALSE\n",
    "  )\n",
    "  \n",
    "  # Create better multi-line labels with line breaks\n",
    "  or_data$Label <- c(\n",
    "    \"White vs\\nNon-white Race\",\n",
    "    \"High vs Low Income\\n(Q3-Q4 vs Q1-Q2)\",\n",
    "    \"Age\\n(per 1 SD increase)\",\n",
    "    \"Teaching vs\\nNon-teaching Hospital\",\n",
    "    \"Urban vs\\nRural Hospital\", \n",
    "    \"Large vs\\nSmall Hospital\"\n",
    "  )\n",
    "  \n",
    "  # Determine significance\n",
    "  or_data$Significant <- (or_data$CI_Lower > 1 | or_data$CI_Upper < 1)\n",
    "  \n",
    "  # Create improved forest plot with UNIFIED THEME AND COLORS\n",
    "  p1 <- ggplot(or_data, aes(x = OR, y = reorder(Label, OR))) +\n",
    "    # Reference line at OR = 1\n",
    "    geom_vline(xintercept = 1, linetype = \"dashed\", color = \"red\", alpha = 0.7, linewidth = 0.8) +\n",
    "    \n",
    "    # Confidence intervals\n",
    "    geom_errorbarh(aes(xmin = CI_Lower, xmax = CI_Upper, color = Significant), \n",
    "                   height = 0.3, linewidth = 1.2) +\n",
    "    \n",
    "    # Point estimates\n",
    "    geom_point(aes(color = Significant), size = 4, shape = 18) +  # Diamond shape\n",
    "    \n",
    "    # UPDATED: Use unified color scheme\n",
    "    scale_color_manual(\n",
    "      values = c(\"FALSE\" = \"gray50\", \"TRUE\" = primary_colors[1]),  # Use primary blue for significant\n",
    "      name = \"Statistically\\nSignificant\", \n",
    "      labels = c(\"No\", \"Yes\")\n",
    "    ) +\n",
    "    \n",
    "    # X-axis with better breaks\n",
    "    scale_x_log10(breaks = c(0.5, 0.75, 1, 1.5, 2, 3, 4),\n",
    "                  labels = c(\"0.5\", \"0.75\", \"1.0\", \"1.5\", \"2.0\", \"3.0\", \"4.0\"),\n",
    "                  limits = c(0.4, 4.5)) +\n",
    "    \n",
    "    # UPDATED: Apply unified poster theme\n",
    "    labs(title = \"Predictors of Private Insurance\",\n",
    "         subtitle = \"Multi-level Model Odds Ratios with 95% CI\",\n",
    "         x = \"Odds Ratio (log scale)\",\n",
    "         y = \"\",  # Remove y-axis title since labels are self-explanatory\n",
    "         caption = \"Reference line at OR = 1.0 (no effect)\") +\n",
    "    \n",
    "    # Apply poster theme as base with specific overrides\n",
    "    theme_poster +\n",
    "    theme(\n",
    "      # Override specific elements for this plot type\n",
    "      axis.text.y = element_text(size = 11, lineheight = 0.9),  # Better line spacing\n",
    "      \n",
    "      # Keep poster-style caption\n",
    "      plot.caption = element_text(size = 9, color = \"gray50\", hjust = 0.5,\n",
    "                                 margin = ggplot2::margin(t = 10)),\n",
    "      \n",
    "      # Grid lines\n",
    "      panel.grid.minor.x = element_blank(),\n",
    "      panel.grid.major.y = element_blank(),\n",
    "      panel.grid.minor.y = element_blank(),\n",
    "      \n",
    "      # Legend positioning and sizing\n",
    "      legend.position = \"bottom\",\n",
    "      legend.margin = ggplot2::margin(t = 7.5)\n",
    "    ) +\n",
    "    \n",
    "    # Add odds ratio values as text annotations - positioned to the right\n",
    "    geom_text(aes(x = 4.2, y = reorder(Label, OR), label = sprintf(\"%.2f\", OR)), \n",
    "              hjust = 0, size = 3.5, fontface = \"bold\", color = \"black\")\n",
    "  \n",
    "  print(p1)\n",
    "  \n",
    "  # SAVE THE PLOT using the save function\n",
    "  save_plot_for_poster(p1, \n",
    "                      name = \"odds_ratios_forest_plot\", \n",
    "                      width = 12, \n",
    "                      height = 8,\n",
    "                      description = \"Odds Ratios Forest Plot for Private Insurance Access\",\n",
    "                      category = \"analysis\")\n",
    "  \n",
    "  # Print summary table\n",
    "  cat(\"\\nODDS RATIOS SUMMARY TABLE:\\n\")\n",
    "  cat(\"=========================\\n\")\n",
    "  summary_table <- or_data[order(-or_data$OR), c(\"Label\", \"OR\", \"CI_Lower\", \"CI_Upper\", \"Significant\")]\n",
    "  summary_table$Label <- gsub(\"\\n\", \" \", summary_table$Label)  # Remove line breaks for table\n",
    "  summary_table$OR <- round(summary_table$OR, 2)\n",
    "  summary_table$CI_Lower <- round(summary_table$CI_Lower, 2)\n",
    "  summary_table$CI_Upper <- round(summary_table$CI_Upper, 2)\n",
    "  summary_table$CI <- paste0(\"(\", summary_table$CI_Lower, \" - \", summary_table$CI_Upper, \")\")\n",
    "  \n",
    "  print(summary_table[, c(\"Label\", \"OR\", \"CI\", \"Significant\")])\n",
    "  \n",
    "  cat(\"\\nImproved Odds Ratios Forest Plot complete with unified styling:\\n\")\n",
    "  \n",
    "} else {\n",
    "  cat(\"Error: best_model not found. Please run the multi-level modeling section first.\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R #=== MACHINE LEARNING MODELING PIPELINE ===\n",
    "\n",
    "# ========================================\n",
    "# MACHINE LEARNING MODELING PIPELINE - FIXED\n",
    "# ========================================\n",
    "\n",
    "cat(\"=== MACHINE LEARNING MODELING PIPELINE ===\\n\")\n",
    "cat(\"Training and evaluating multiple ML models\\n\\n\")\n",
    "\n",
    "# Install and load required packages\n",
    "ml_packages <- c(\"randomForest\", \"pROC\", \"caret\", \"e1071\", \"glmnet\", \"xgboost\")\n",
    "\n",
    "for(pkg in ml_packages) {\n",
    "  if(!require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "    cat(\"Installing\", pkg, \"...\\n\")\n",
    "    install.packages(pkg, quiet = TRUE)\n",
    "    library(pkg, character.only = TRUE, quietly = TRUE)\n",
    "  }\n",
    "}\n",
    "\n",
    "cat(\"Machine learning packages loaded successfully\\n\\n\")\n",
    "\n",
    "# ========================================\n",
    "# SMART DATA SAMPLING FOR DEMO\n",
    "# ========================================\n",
    "\n",
    "# For demonstration purposes, use a manageable subset\n",
    "set.seed(12345)\n",
    "sample_size <- min(20000, nrow(model_data))  # Max 20K rows for quick demo\n",
    "sample_indices <- sample(nrow(model_data), sample_size)\n",
    "\n",
    "ml_data <- model_data[sample_indices, .(private_insurance, white_race, high_income, age_centered,\n",
    "                                       teaching_hospital, urban_hospital, large_hospital)]\n",
    "\n",
    "cat(\"Using sample of\", nrow(ml_data), \"observations for ML demo\\n\")\n",
    "cat(\"(\", round(100 * nrow(ml_data) / nrow(model_data), 1), \"% of full dataset)\\n\\n\")\n",
    "\n",
    "# Enhanced data cleaning\n",
    "ml_data <- ml_data[complete.cases(ml_data)]\n",
    "\n",
    "# Ensure all variables are proper numeric types\n",
    "numeric_vars <- names(ml_data)\n",
    "for(var in numeric_vars) {\n",
    "  ml_data[, (var) := as.numeric(get(var))]\n",
    "}\n",
    "\n",
    "cat(\"ML dataset prepared:\", nrow(ml_data), \"complete observations\\n\\n\")\n",
    "\n",
    "# Split into training and testing\n",
    "train_idx <- createDataPartition(ml_data$private_insurance, p = 0.8, list = FALSE)\n",
    "train_data <- ml_data[train_idx]\n",
    "test_data <- ml_data[-train_idx]\n",
    "\n",
    "cat(\"Training set:\", nrow(train_data), \"observations\\n\")\n",
    "cat(\"Test set:\", nrow(test_data), \"observations\\n\\n\")\n",
    "\n",
    "# ========================================\n",
    "# 1. BASELINE LOGISTIC REGRESSION\n",
    "# ========================================\n",
    "\n",
    "cat(\"1Ô∏è‚É£ LOGISTIC REGRESSION\\n\")\n",
    "start_time <- Sys.time()\n",
    "\n",
    "baseline_glm <- glm(private_insurance ~ ., data = train_data, family = binomial)\n",
    "baseline_pred <- predict(baseline_glm, test_data, type = \"response\")\n",
    "baseline_pred_class <- ifelse(baseline_pred > 0.5, 1, 0)\n",
    "\n",
    "end_time <- Sys.time()\n",
    "cat(\"   Completed in\", round(as.numeric(end_time - start_time), 2), \"seconds\\n\\n\")\n",
    "\n",
    "# ========================================\n",
    "# 2. RANDOM FOREST - OPTIMIZED\n",
    "# ========================================\n",
    "\n",
    "cat(\"2Ô∏è‚É£ RANDOM FOREST\\n\")\n",
    "start_time <- Sys.time()\n",
    "\n",
    "rf_model <- randomForest(factor(private_insurance) ~ ., \n",
    "                        data = train_data, \n",
    "                        ntree = 100,  # Reduced trees for speed\n",
    "                        importance = TRUE,\n",
    "                        mtry = 3)     # Fixed mtry\n",
    "\n",
    "rf_pred <- predict(rf_model, test_data, type = \"prob\")[,2]\n",
    "rf_pred_class <- as.numeric(as.character(predict(rf_model, test_data)))\n",
    "\n",
    "end_time <- Sys.time()\n",
    "cat(\"   Completed in\", round(as.numeric(end_time - start_time), 2), \"seconds\\n\\n\")\n",
    "\n",
    "# ========================================\n",
    "# 3. SUPPORT VECTOR MACHINE - FAST VERSION\n",
    "# ========================================\n",
    "\n",
    "cat(\"3Ô∏è‚É£ SUPPORT VECTOR MACHINE (Optimized)\\n\")\n",
    "start_time <- Sys.time()\n",
    "\n",
    "# Use a smaller subset for SVM demo (SVM is O(n^3))\n",
    "svm_sample_size <- min(5000, nrow(train_data))\n",
    "svm_indices <- sample(nrow(train_data), svm_sample_size)\n",
    "train_svm <- train_data[svm_indices]\n",
    "\n",
    "cat(\"   Using\", nrow(train_svm), \"samples for SVM training (for speed)\\n\")\n",
    "\n",
    "# Scale data for SVM\n",
    "train_scaled <- copy(train_svm)\n",
    "test_scaled <- copy(test_data)\n",
    "\n",
    "# Scale age_centered\n",
    "if(\"age_centered\" %in% names(train_scaled)) {\n",
    "  train_mean <- mean(train_scaled$age_centered)\n",
    "  train_sd <- sd(train_scaled$age_centered)\n",
    "  if(train_sd > 0) {\n",
    "    train_scaled[, age_centered := (age_centered - train_mean) / train_sd]\n",
    "    test_scaled[, age_centered := (age_centered - train_mean) / train_sd]\n",
    "  }\n",
    "}\n",
    "\n",
    "# Fast SVM with optimized parameters\n",
    "svm_model <- svm(factor(private_insurance) ~ ., \n",
    "                data = train_scaled, \n",
    "                kernel = \"radial\",\n",
    "                cost = 1,\n",
    "                gamma = 0.1,\n",
    "                probability = TRUE)\n",
    "\n",
    "svm_pred <- predict(svm_model, test_scaled, probability = TRUE)\n",
    "svm_pred_prob <- attr(svm_pred, \"probabilities\")[,2]\n",
    "svm_pred_class <- as.numeric(as.character(svm_pred))\n",
    "\n",
    "end_time <- Sys.time()\n",
    "cat(\"   Completed in\", round(as.numeric(end_time - start_time), 2), \"seconds\\n\\n\")\n",
    "\n",
    "# ========================================\n",
    "# 4. LASSO REGRESSION\n",
    "# ========================================\n",
    "\n",
    "cat(\"4Ô∏è‚É£ LASSO REGRESSION\\n\")\n",
    "start_time <- Sys.time()\n",
    "\n",
    "x_train <- model.matrix(private_insurance ~ . - 1, data = train_data)\n",
    "y_train <- train_data$private_insurance\n",
    "x_test <- model.matrix(private_insurance ~ . - 1, data = test_data)\n",
    "\n",
    "# Quick CV with fewer folds\n",
    "cv_lasso <- cv.glmnet(x_train, y_train, family = \"binomial\", alpha = 1, nfolds = 3)\n",
    "lasso_model <- glmnet(x_train, y_train, family = \"binomial\", alpha = 1, lambda = cv_lasso$lambda.min)\n",
    "\n",
    "lasso_pred <- predict(lasso_model, x_test, type = \"response\")[,1]\n",
    "lasso_pred_class <- ifelse(lasso_pred > 0.5, 1, 0)\n",
    "\n",
    "end_time <- Sys.time()\n",
    "cat(\"   Completed in\", round(as.numeric(end_time - start_time), 2), \"seconds\\n\\n\")\n",
    "\n",
    "# ========================================\n",
    "# 5. XGBOOST - FAST VERSION\n",
    "# ========================================\n",
    "\n",
    "cat(\"5Ô∏è‚É£ XGBOOST\\n\")\n",
    "start_time <- Sys.time()\n",
    "\n",
    "dtrain <- xgb.DMatrix(data = as.matrix(train_data[, -\"private_insurance\"]), \n",
    "                     label = train_data$private_insurance)\n",
    "dtest <- xgb.DMatrix(data = as.matrix(test_data[, -\"private_insurance\"]), \n",
    "                    label = test_data$private_insurance)\n",
    "\n",
    "# Fast XGBoost parameters\n",
    "xgb_params <- list(\n",
    "  objective = \"binary:logistic\",\n",
    "  eval_metric = \"auc\",\n",
    "  eta = 0.3,          # Higher learning rate\n",
    "  max_depth = 4,      # Shallower trees\n",
    "  subsample = 0.8,\n",
    "  colsample_bytree = 0.8\n",
    ")\n",
    "\n",
    "# Fewer rounds for speed\n",
    "xgb_model <- xgb.train(\n",
    "  params = xgb_params,\n",
    "  data = dtrain,\n",
    "  nrounds = 50,       # Reduced rounds\n",
    "  verbose = 0\n",
    ")\n",
    "\n",
    "xgb_pred <- predict(xgb_model, dtest)\n",
    "xgb_pred_class <- ifelse(xgb_pred > 0.5, 1, 0)\n",
    "\n",
    "end_time <- Sys.time()\n",
    "cat(\"   Completed in\", round(as.numeric(end_time - start_time), 2), \"seconds\\n\\n\")\n",
    "\n",
    "# ========================================\n",
    "# ORGANIZE MODELS FOR COMPARISON\n",
    "# ========================================\n",
    "\n",
    "# Create models list to store all trained models and predictions\n",
    "models <- list(\n",
    "  \"Logistic\" = list(pred = baseline_pred, pred_class = baseline_pred_class),\n",
    "  \"Random Forest\" = list(pred = rf_pred, pred_class = rf_pred_class),\n",
    "  \"SVM\" = list(pred = svm_pred_prob, pred_class = svm_pred_class),\n",
    "  \"LASSO\" = list(pred = lasso_pred, pred_class = lasso_pred_class),\n",
    "  \"XGBoost\" = list(pred = xgb_pred, pred_class = xgb_pred_class)\n",
    ")\n",
    "\n",
    "cat(\"Models organized for comparison\\n\")\n",
    "\n",
    "# ========================================\n",
    "# 6. PERFORMANCE COMPARISON - FIXED\n",
    "# ========================================\n",
    "\n",
    "cat(\"6Ô∏è‚É£ PERFORMANCE COMPARISON\\n\")\n",
    "cat(paste(rep(\"=\", 50), collapse = \"\"), \"\\n\")\n",
    "\n",
    "# Initialize results with proper structure\n",
    "results_table <- data.frame(\n",
    "  Model = character(length(models)),\n",
    "  AUC = numeric(length(models)),\n",
    "  Accuracy = numeric(length(models)),\n",
    "  Sensitivity = numeric(length(models)),\n",
    "  Specificity = numeric(length(models)),\n",
    "  F1_Score = numeric(length(models)),\n",
    "  stringsAsFactors = FALSE\n",
    ")\n",
    "\n",
    "# Calculate metrics for each model\n",
    "for(i in 1:length(models)) {\n",
    "  pred_prob <- models[[i]]$pred\n",
    "  pred_class <- models[[i]]$pred_class\n",
    "  actual <- test_data$private_insurance\n",
    "  \n",
    "  # Model name\n",
    "  results_table$Model[i] <- names(models)[i]\n",
    "  \n",
    "  # AUC\n",
    "  roc_obj <- roc(actual, pred_prob, quiet = TRUE)\n",
    "  results_table$AUC[i] <- round(as.numeric(auc(roc_obj)), 3)\n",
    "  \n",
    "  # Confusion matrix\n",
    "  cm <- table(Predicted = pred_class, Actual = actual)\n",
    "  results_table$Accuracy[i] <- round(sum(diag(cm)) / sum(cm), 3)\n",
    "  \n",
    "  # Sensitivity and Specificity\n",
    "  if(nrow(cm) == 2 && ncol(cm) == 2) {\n",
    "    results_table$Sensitivity[i] <- round(cm[2,2] / sum(cm[,2]), 3)\n",
    "    results_table$Specificity[i] <- round(cm[1,1] / sum(cm[,1]), 3)\n",
    "  } else {\n",
    "    results_table$Sensitivity[i] <- 0\n",
    "    results_table$Specificity[i] <- 0\n",
    "  }\n",
    "  \n",
    "  # F1 Score\n",
    "  tp <- sum(pred_class == 1 & actual == 1)\n",
    "  fp <- sum(pred_class == 1 & actual == 0)\n",
    "  fn <- sum(pred_class == 0 & actual == 1)\n",
    "  \n",
    "  precision <- ifelse(tp + fp > 0, tp / (tp + fp), 0)\n",
    "  recall <- ifelse(tp + fn > 0, tp / (tp + fn), 0)\n",
    "  f1 <- ifelse(precision + recall > 0, 2 * (precision * recall) / (precision + recall), 0)\n",
    "  results_table$F1_Score[i] <- round(f1, 3)\n",
    "}\n",
    "\n",
    "cat(\"PERFORMANCE RESULTS:\\n\")\n",
    "print(results_table)\n",
    "\n",
    "# Best model\n",
    "best_model_idx <- which.max(results_table$AUC)\n",
    "cat(\"\\nBest model:\", results_table$Model[best_model_idx], \n",
    "    \"with AUC =\", results_table$AUC[best_model_idx], \"\\n\")\n",
    "\n",
    "# ========================================\n",
    "# VARIABLE IMPORTANCE\n",
    "# ========================================\n",
    "\n",
    "cat(\"\\n7Ô∏è‚É£ VARIABLE IMPORTANCE\\n\")\n",
    "cat(paste(rep(\"=\", 40), collapse = \"\"), \"\\n\")\n",
    "\n",
    "# Random Forest importance\n",
    "rf_imp <- importance(rf_model)[,4]\n",
    "rf_imp_scaled <- round(100 * rf_imp / max(rf_imp), 1)\n",
    "\n",
    "cat(\"Random Forest Variable Importance:\\n\")\n",
    "rf_sorted <- sort(rf_imp_scaled, decreasing = TRUE)\n",
    "for(i in 1:length(rf_sorted)) {\n",
    "  cat(\"  \", sprintf(\"%-20s\", names(rf_sorted)[i]), \":\", rf_sorted[i], \"%\\n\")\n",
    "}\n",
    "\n",
    "# XGBoost importance\n",
    "xgb_imp <- xgb.importance(feature_names = colnames(x_train), model = xgb_model)\n",
    "cat(\"\\nXGBoost Top Variables:\\n\")\n",
    "for(i in 1:min(5, nrow(xgb_imp))) {\n",
    "  cat(\"  \", sprintf(\"%-20s\", xgb_imp$Feature[i]), \":\", round(xgb_imp$Gain[i], 3), \"\\n\")\n",
    "}\n",
    "\n",
    "# Organize results for visualization dashboard\n",
    "ml_results <- list()\n",
    "ml_results$models <- models\n",
    "ml_results$test_labels <- test_data$private_insurance\n",
    "ml_results$performance <- results_table\n",
    "\n",
    "# Variable importance (organize from different models)\n",
    "ml_results$importance <- data.frame(\n",
    "  Variable = names(rf_imp_scaled),\n",
    "  RandomForest = as.numeric(rf_imp_scaled),\n",
    "  stringsAsFactors = FALSE\n",
    ")\n",
    "\n",
    "# Add LASSO importance (coefficients)\n",
    "lasso_coefs <- as.matrix(coef(lasso_model))\n",
    "lasso_importance <- abs(lasso_coefs[-1, 1])  # Remove intercept\n",
    "lasso_scaled <- round(100 * lasso_importance / max(lasso_importance), 1)\n",
    "ml_results$importance$LASSO <- lasso_scaled[match(ml_results$importance$Variable, names(lasso_scaled))]\n",
    "\n",
    "# Add XGBoost importance\n",
    "xgb_importance_scaled <- numeric(nrow(ml_results$importance))\n",
    "for(i in 1:nrow(ml_results$importance)) {\n",
    "  var_name <- ml_results$importance$Variable[i]\n",
    "  xgb_match <- which(xgb_imp$Feature == var_name)\n",
    "  if(length(xgb_match) > 0) {\n",
    "    xgb_importance_scaled[i] <- round(100 * xgb_imp$Gain[xgb_match[1]] / max(xgb_imp$Gain), 1)\n",
    "  }\n",
    "}\n",
    "ml_results$importance$XGBoost <- xgb_importance_scaled\n",
    "\n",
    "# Fill any missing values with 0\n",
    "ml_results$importance[is.na(ml_results$importance)] <- 0\n",
    "\n",
    "cat(\"ml_results structure created successfully\\n\")\n",
    "\n",
    "cat(\"\\n‚úÖ MACHINE LEARNING ANALYSIS COMPLETE\\n\")\n",
    "cat(\"All models successfully trained and evaluated!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# ========================================\n",
    "# COMBINED ML VISUALIZATION - VARIABLE IMPORTANCE WITH ROC OVERLAY - UNIFIED THEME\n",
    "# ========================================\n",
    "\n",
    "cat(\"=== CREATING COMBINED ML VISUALIZATION WITH UNIFIED STYLING ===\\n\")\n",
    "\n",
    "# Define consistent color palette\n",
    "model_colors <- RColorBrewer::brewer.pal(5, \"Set1\")\n",
    "names(model_colors) <- c(\"Logistic\", \"Random Forest\", \"SVM\", \"LASSO\", \"XGBoost\")\n",
    "\n",
    "# ========================================\n",
    "# PREPARE DATA FOR BOTH PLOTS\n",
    "# ========================================\n",
    "\n",
    "# Variable importance data preparation\n",
    "clean_var_names <- data.frame(\n",
    "  original = c(\"white_race\", \"high_income\", \"age_centered\", \"teaching_hospital\", \"urban_hospital\", \"large_hospital\"),\n",
    "  clean = c(\"Race\\n(White vs\\nNon-white)\", \"Income\\n(High vs\\nLow)\", \"Age\\n(Centered)\", \n",
    "           \"Teaching\\nHospital\", \"Urban\\nLocation\", \"Large\\nHospital\"),\n",
    "  stringsAsFactors = FALSE\n",
    ")\n",
    "\n",
    "imp_data <- ml_results$importance\n",
    "imp_data$Variable_Clean <- clean_var_names$clean[match(imp_data$Variable, clean_var_names$original)]\n",
    "\n",
    "imp_long <- melt(imp_data[, c(\"Variable_Clean\", \"RandomForest\", \"LASSO\", \"XGBoost\")], \n",
    "                id.vars = \"Variable_Clean\")\n",
    "colnames(imp_long) <- c(\"Variable\", \"Model\", \"Importance\")\n",
    "\n",
    "# UPDATED: Use unified color scheme for importance\n",
    "importance_colors <- c(\n",
    "  \"RandomForest\" = primary_colors[1],  # Professional blue\n",
    "  \"LASSO\" = primary_colors[4],         # Red\n",
    "  \"XGBoost\" = primary_colors[3]        # Green\n",
    ")\n",
    "\n",
    "# ROC data preparation\n",
    "roc_data <- data.frame()\n",
    "\n",
    "for(i in 1:length(ml_results$models)) {\n",
    "  model_name <- names(ml_results$models)[i]\n",
    "  pred_prob <- ml_results$models[[i]]$pred\n",
    "  \n",
    "  roc_obj <- roc(ml_results$test_labels, pred_prob, quiet = TRUE)\n",
    "  \n",
    "  roc_df <- data.frame(\n",
    "    Model = model_name,\n",
    "    Specificity = roc_obj$specificities,\n",
    "    Sensitivity = roc_obj$sensitivities,\n",
    "    AUC = round(auc(roc_obj), 3)\n",
    "  )\n",
    "  \n",
    "  roc_data <- rbind(roc_data, roc_df)\n",
    "}\n",
    "\n",
    "unique_models <- unique(roc_data[, c(\"Model\", \"AUC\")])\n",
    "legend_labels <- paste0(unique_models$Model, \" (AUC: \", unique_models$AUC, \")\")\n",
    "\n",
    "# ========================================\n",
    "# CREATE MAIN VARIABLE IMPORTANCE PLOT - UNIFIED THEME\n",
    "# ========================================\n",
    "\n",
    "main_plot <- ggplot(imp_long, aes(x = reorder(Variable, Importance), y = Importance, fill = Model)) +\n",
    "  geom_col(position = position_dodge(width = 0.8), alpha = 0.85, width = 0.75) +\n",
    "  scale_fill_manual(\n",
    "    values = importance_colors,\n",
    "    name = \"ML Algorithm\",\n",
    "    labels = c(\"Random Forest\", \"LASSO Regression\", \"XGBoost\")\n",
    "  ) +\n",
    "  labs(\n",
    "    title = \"Machine Learning Variable Importance\",\n",
    "    subtitle = \"for Predicting Private Insurance Access; ROC Overlay\",\n",
    "    x = \"Predictor Variables\",\n",
    "    y = \"Relative Importance (%)\"\n",
    "  ) +\n",
    "  \n",
    "  # UPDATED: Apply unified poster theme as base\n",
    "  theme_poster +\n",
    "  theme(\n",
    "    # Override specific elements for this plot type while maintaining poster style\n",
    "    axis.text.y = element_text(size = 10, lineheight = 0.8),\n",
    "    legend.position = \"bottom\",\n",
    "    legend.margin = ggplot2::margin(t = 10),\n",
    "    \n",
    "    # Grid lines\n",
    "    panel.grid.minor = element_blank(),\n",
    "    panel.grid.major.x = element_blank(),\n",
    "    panel.grid.major.y = element_line(color = \"gray90\", linewidth = 0.5),\n",
    "    \n",
    "    # Margins\n",
    "    plot.margin = ggplot2::margin(t = 25, r = 15, b = 15, l = 20)\n",
    "  ) +\n",
    "  \n",
    "  coord_flip() +\n",
    "  scale_y_continuous(\n",
    "    expand = c(0, 0), \n",
    "    limits = c(0, max(imp_long$Importance) * 1.08),\n",
    "    breaks = seq(0, 100, 20)\n",
    "  ) +\n",
    "  guides(fill = guide_legend(nrow = 1, override.aes = list(alpha = 0.8)))\n",
    "\n",
    "# ========================================\n",
    "# CREATE SMALL ROC OVERLAY PLOT - UNIFIED THEME\n",
    "# ========================================\n",
    "\n",
    "roc_overlay <- ggplot(roc_data, aes(x = 1 - Specificity, y = Sensitivity, color = Model)) +\n",
    "  geom_line(linewidth = 1.5, alpha = 0.8) +\n",
    "  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"gray50\", linewidth = 0.8) +\n",
    "  \n",
    "  # UPDATED: Use unified model colors from our color system\n",
    "  scale_color_manual(values = model_colors, labels = legend_labels, name = \"Model (AUC)\") +\n",
    "  \n",
    "  labs(\n",
    "    title = \"ROC Curves\",\n",
    "    x = \"1 - Specificity\",\n",
    "    y = \"Sensitivity\"\n",
    "  ) +\n",
    "  \n",
    "  # UPDATED: Apply poster theme as base with overlay-specific overrides\n",
    "  theme_poster +\n",
    "  theme(\n",
    "    # Small overlay styling\n",
    "    plot.title = element_text(size = 9, face = \"bold\", hjust = 0.5, \n",
    "                             margin = ggplot2::margin(b = 3)),\n",
    "    axis.title = element_text(size = 7, face = \"bold\"),\n",
    "    axis.text = element_text(size = 6),\n",
    "    legend.title = element_text(size = 7, face = \"bold\"),\n",
    "    legend.text = element_text(size = 6),\n",
    "    legend.position = \"none\",  # Remove legend to save space\n",
    "    \n",
    "    # Grid and background\n",
    "    panel.grid.minor = element_blank(),\n",
    "    panel.grid.major = element_line(color = \"gray90\", linewidth = 0.3),\n",
    "    plot.background = element_rect(fill = \"white\", color = \"black\", linewidth = 0.5),\n",
    "    panel.background = element_rect(fill = \"white\"),\n",
    "    plot.margin = ggplot2::margin(5, 5, 5, 5)\n",
    "  ) +\n",
    "  \n",
    "  coord_equal() +\n",
    "  scale_x_continuous(limits = c(0, 1), expand = c(0.02, 0.02), \n",
    "                     breaks = c(0, 0.5, 1), labels = c(\"0\", \"0.5\", \"1\")) +\n",
    "  scale_y_continuous(limits = c(0, 1), expand = c(0.02, 0.02),\n",
    "                     breaks = c(0, 0.5, 1), labels = c(\"0\", \"0.5\", \"1\"))\n",
    "\n",
    "# ========================================\n",
    "# COMBINE PLOTS USING ANNOTATION_CUSTOM\n",
    "# ========================================\n",
    "\n",
    "# Convert the ROC plot to a grob for overlay\n",
    "if(require(\"gridExtra\", quietly = TRUE)) {\n",
    "  \n",
    "  # Create the combined plot\n",
    "  combined_plot <- main_plot + \n",
    "    annotation_custom(\n",
    "      grob = ggplotGrob(roc_overlay),\n",
    "      xmin = 1.0, xmax = 4.0,    # Positioned in upper right area\n",
    "      ymin = 60, ymax = 110      # Adjusted positioning\n",
    "    )\n",
    "  \n",
    "} else {\n",
    "  # Fallback: install gridExtra and try again\n",
    "  install.packages(\"gridExtra\", quiet = TRUE)\n",
    "  library(gridExtra, quietly = TRUE)\n",
    "  \n",
    "  combined_plot <- main_plot + \n",
    "    annotation_custom(\n",
    "      grob = ggplotGrob(roc_overlay),\n",
    "      xmin = 1.0, xmax = 4.0,\n",
    "      ymin = 60, ymax = 110\n",
    "    )\n",
    "}\n",
    "\n",
    "print(combined_plot)\n",
    "\n",
    "save_plot_for_poster(combined_plot, \n",
    "                    name = \"ml_combined_importance_roc\", \n",
    "                    description = \"Combined ML Variable Importance with ROC Curves Overlay\",\n",
    "                    category = \"results\")\n",
    "\n",
    "cat(\"*** Combined ML plot with unified styling created\\n\\n\")\n",
    "\n",
    "# ========================================\n",
    "# SUMMARY INSIGHTS\n",
    "# ========================================\n",
    "\n",
    "cat(\"ML INSIGHTS SUMMARY\\n\")\n",
    "cat(paste(rep(\"=\", 50), collapse = \"\"), \"\\n\")\n",
    "\n",
    "# Find best performing model\n",
    "best_auc_idx <- which.max(unique_models$AUC)\n",
    "best_model_name <- unique_models$Model[best_auc_idx]\n",
    "best_auc_score <- unique_models$AUC[best_auc_idx]\n",
    "\n",
    "cat(\"BEST PERFORMING MODEL:\", best_model_name, \"\\n\")\n",
    "cat(\"   AUC Score:\", best_auc_score, \"\\n\\n\")\n",
    "\n",
    "cat(\"KEY VARIABLE IMPORTANCE FINDINGS:\\n\")\n",
    "# Get top variables from Random Forest\n",
    "rf_importance <- imp_data[order(-imp_data$RandomForest), ]\n",
    "cat(\"   Most important:\", gsub(\"\\n\", \" \", rf_importance$Variable_Clean[1]), \"\\n\")\n",
    "cat(\"   Second most important:\", gsub(\"\\n\", \" \", rf_importance$Variable_Clean[2]), \"\\n\")\n",
    "cat(\"   Third most important:\", gsub(\"\\n\", \" \", rf_importance$Variable_Clean[3]), \"\\n\\n\")\n",
    "\n",
    "cat(\"KEY INSIGHTS:\\n\")\n",
    "cat(\"   All models achieve good discrimination (AUC > 0.78)\\n\")\n",
    "cat(\"   Race and income are consistently top predictors\\n\")\n",
    "cat(\"   Hospital characteristics add moderate predictive value\\n\")\n",
    "cat(\"   Complex ML models marginally outperform logistic regression\\n\")\n",
    "cat(\"   Results validate statistical findings from multi-level models\\n\")\n",
    "\n",
    "cat(\"\\nCOMBINED ML VISUALIZATION COMPLETE WITH UNIFIED STYLING:\\n\")\n",
    "cat(\"   - Applied theme_poster base theme with custom overrides\\n\")\n",
    "cat(\"   - Used primary_colors for variable importance (blue, red, green)\\n\")\n",
    "cat(\"   - Maintained Set1 palette for ROC curves (consistent with other ML plots)\\n\")\n",
    "cat(\"   - Applied left-aligned titles (poster style)\\n\")\n",
    "cat(\"   - Consistent typography and spacing with other visualizations\\n\")\n",
    "cat(\"   - Professional appearance matching overall analysis style\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poster and Index Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Generate manifest of all saved outputs\n",
    "generate_output_manifest <- function() {\n",
    "  \n",
    "  cat(\"=== GENERATING OUTPUT MANIFEST ===\\n\")\n",
    "  \n",
    "  # Get saved plots\n",
    "  if(exists(\"plot_metadata\") && length(plot_metadata) > 0) {\n",
    "    cat(\"SAVED PLOTS:\\n\")\n",
    "    for(name in names(plot_metadata)) {\n",
    "      meta <- plot_metadata[[name]]\n",
    "      cat(\"  \", name, \":\", meta$description, \"\\n\")\n",
    "      cat(\"    Files:\", basename(meta$file_png), \",\", basename(meta$file_pdf), \"\\n\")\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # Get saved text outputs\n",
    "  text_files <- list.files(\"saved_outputs\", pattern = \"\\\\.txt$\", full.names = FALSE)\n",
    "  if(length(text_files) > 0) {\n",
    "    cat(\"\\nSAVED TEXT OUTPUTS:\\n\")\n",
    "    for(file in text_files) {\n",
    "      cat(\"  \", file, \"\\n\")\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  cat(\"\\n‚úÖ All outputs saved and ready for poster/manuscript preparation\\n\")\n",
    "}\n",
    "\n",
    "generate_output_manifest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_research_poster():\n",
    "    \"\"\"Generate a clean research poster with improved title formatting and logo positioning\"\"\"\n",
    "    \n",
    "    # Import required libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "    from matplotlib.patches import Rectangle\n",
    "    import numpy as np\n",
    "    from pathlib import Path\n",
    "    import os\n",
    "    import qrcode\n",
    "    from PIL import Image\n",
    "    \n",
    "    print(\"üé® GENERATING RESEARCH POSTER (Fixed Title + Logo)\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Poster dimensions (16:9 ratio - A0 landscape equivalent)\n",
    "    poster_width = 32\n",
    "    poster_height = 18\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(poster_width, poster_height))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    # Define layout proportions - ENHANCED HEADER for bigger title\n",
    "    header_height = 0.17   # Increased for even bigger title\n",
    "    content_height = 0.83  # Decreased accordingly\n",
    "    \n",
    "    # ====================================\n",
    "    # HEADER SECTION - FIXED TITLE FORMATTING\n",
    "    # ====================================\n",
    "    \n",
    "    # Header background\n",
    "    header_ax = fig.add_axes([0, 1-header_height, 1, header_height])\n",
    "    header_ax.set_xlim(0, 1)\n",
    "    header_ax.set_ylim(0, 1)\n",
    "    header_ax.axis('off')\n",
    "    \n",
    "    # PROFESSIONAL: Dark blue/purple gradient background\n",
    "    header_rect = Rectangle((0, 0), 1, 1, facecolor='#1e3a8a', edgecolor='none')  # Professional dark blue\n",
    "    header_ax.add_patch(header_rect)\n",
    "    \n",
    "    # Add subtle gradient effect with overlay\n",
    "    gradient_rect = Rectangle((0, 0), 1, 1, facecolor='#312e81', edgecolor='none', alpha=0.3)  # Dark purple overlay\n",
    "    header_ax.add_patch(gradient_rect)\n",
    "    \n",
    "    # FIXED: Better formatted title section\n",
    "    # Main title - split into two cleaner lines\n",
    "    title_line1 = \"Socioeconomic and Demographic Drivers of Ambulatory Surgery Usage\"\n",
    "    title_line2 = \"HCUP National Ambulatory Surgery Sample - 2020\"\n",
    "\n",
    "    subtitle_line1 = \"Scalable, Multi-Platform, Open-access, Reproducible Data Pipeline\"\n",
    "    subtitle_line2 = \"7.8 million patients, 2899 hospitals, 35 states, 1 nation\"  # SMALLER\n",
    "\n",
    "    author_line1 = \"Seena Khosravi, CA-2 | L. Soloniuk, MD; I. Pasca, MD; M. Douglas, MD; G. Stier, MD, MBA\"\n",
    "    author_line2 = \"LLMs Utilized: Claude Sonnet 4  | Opus 4.1; ChatGPT 4o, o4, 5, 5-mini; Deepseek 3.1; Gemini 2.5 Pro; Grok 5\"\n",
    "\n",
    "    # IMPROVED title styling - INCREASED font size and ALL BOLD\n",
    "    header_ax.text(0.02, 0.93, title_line1, ha='left', va='top', fontsize=36,  # Increased from 30\n",
    "                fontweight='bold', color='white')\n",
    "    \n",
    "    header_ax.text(0.02, 0.74, title_line2, ha='left', va='top', fontsize=30,  # Increased from 30\n",
    "                fontweight='bold', color='white')\n",
    "\n",
    "    # Subtitle lines - ALL BOLD, adjusted spacing\n",
    "    header_ax.text(0.02, 0.52, subtitle_line1, ha='left', va='center', fontsize=20,\n",
    "                color='#e0e7ff', style='italic', fontweight='bold', linespacing=1.2)\n",
    "\n",
    "    header_ax.text(0.02, 0.40, subtitle_line2, ha='left', va='center', fontsize=16,\n",
    "                color='#c7d2fe', style='italic', fontweight='bold', linespacing=1.2)\n",
    "\n",
    "    # Author lines - ALL BOLD, DECREASED space above (moved down)\n",
    "    header_ax.text(0.02, 0.26, author_line1, ha='left', va='center', fontsize=18,  # Moved from 0.22 to 0.26\n",
    "                color='#c7d2fe', fontweight='bold')  # Changed to bold\n",
    "\n",
    "    header_ax.text(0.02, 0.08, author_line2, ha='left', va='bottom', fontsize=16,\n",
    "                color='#a5b4fc', fontweight='bold', style='italic')  # Changed to bold\n",
    "    \n",
    "    colab_url = \"https://colab.research.google.com/github/SeenaKhosravi/NASS/blob/main/Analysis_NASS.ipynb\"\n",
    "    \n",
    "    # ====================================\n",
    "    # IMPROVED RUHS LOGO POSITIONING - FARTHER RIGHT WITH EQUAL MARGINS\n",
    "    # ====================================\n",
    "\n",
    "    # Logo loading and placement with better positioning\n",
    "    logo_path = Path(\"logos\") / \"ruhs_logo.png\"\n",
    "    logo_loaded = False\n",
    "\n",
    "    if logo_path.exists():\n",
    "        try:\n",
    "            ruhs_logo = Image.open(logo_path)\n",
    "            \n",
    "            # Calculate width to maintain aspect ratio - INCREASED SIZE\n",
    "            aspect_ratio = ruhs_logo.width / ruhs_logo.height\n",
    "            logo_height = header_height * 0.60  # 65% of header height\n",
    "            logo_width = logo_height * aspect_ratio\n",
    "            \n",
    "            # Calculate top margin for centering\n",
    "            top_margin = (header_height - logo_height) / 2\n",
    "            \n",
    "            # Position with EQUAL top and right margins\n",
    "            logo_x = 1.0 - logo_width + 0.0345  # Fixed 1% right margin\n",
    "            logo_y = 1.0 - header_height + top_margin  # Keep vertical centering\n",
    "            \n",
    "            # Place logo\n",
    "            logo_ax = fig.add_axes([logo_x, logo_y, logo_width, logo_height])\n",
    "            logo_ax.imshow(ruhs_logo)\n",
    "            logo_ax.axis('off')\n",
    "            \n",
    "            logo_loaded = True\n",
    "            print(f\"‚úÖ RUHS logo loaded with equal top/right margins ({top_margin:.3f})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load logo: {e}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Logo not found at: {logo_path}\")\n",
    "    \n",
    "    # ====================================\n",
    "    # CONTENT SECTION - SAME AS BEFORE\n",
    "    # ====================================\n",
    "    \n",
    "    # Grid parameters for 4 columns x 3 rows (12 total blocks)\n",
    "    grid_cols = 4\n",
    "    grid_rows = 3\n",
    "    \n",
    "    # Optimized spacing for larger plots\n",
    "    margin_x = 0.02\n",
    "    margin_y = 0.02\n",
    "    spacing_x = 0.015\n",
    "    spacing_y = 0.02\n",
    "    \n",
    "    # Available space for plots\n",
    "    available_width = 1 - 2 * margin_x - (grid_cols - 1) * spacing_x\n",
    "    available_height = content_height - 2 * margin_y - (grid_rows - 1) * spacing_y\n",
    "    \n",
    "    # Plot dimensions\n",
    "    plot_width = available_width / grid_cols\n",
    "    plot_height = available_height / grid_rows\n",
    "    \n",
    "    # Define plot assignments for 3x4 grid (12 plots) - MOVED HEATMAP TO COLUMN 2 (BOTTOM ROW)\n",
    "    plot_assignments = [\n",
    "        # TOP ROW (row 0)\n",
    "        {'name': 'top_procedures_income', 'row': 0, 'col': 0},\n",
    "        {'name': 'procedures_by_race', 'row': 0, 'col': 1},\n",
    "        {'name': 'payer_by_demographics', 'row': 0, 'col': 2},\n",
    "        {'name': 'payer_by_income', 'row': 0, 'col': 3},\n",
    "        \n",
    "        # MIDDLE ROW (row 1)\n",
    "        {'name': 'age_distribution_race', 'row': 1, 'col': 0},\n",
    "        {'name': 'age_distribution_payer', 'row': 1, 'col': 1},\n",
    "        {'name': 'census_comparison_lines', 'row': 1, 'col': 2},\n",
    "        {'name': 'census_comparison_hourglass_enhanced', 'row': 1, 'col': 3},\n",
    "        \n",
    "        # BOTTOM ROW (row 2) - MOVED HEATMAP TO COLUMN 2 (next to QR code)\n",
    "        {'name': 'odds_ratios_forest_plot', 'row': 2, 'col': 0},\n",
    "        {'name': 'ml_combined_importance_roc', 'row': 2, 'col': 1},\n",
    "        {'name': 'age_specific_procedure_patterns', 'row': 2, 'col': 2},  # MOVED HERE\n",
    "        {'name': 'qr_code_block', 'row': 2, 'col': 3},\n",
    "    ]\n",
    "    \n",
    "    # Load and place plots\n",
    "    plots_dir = Path(\"saved_plots\")\n",
    "    plots_placed = 0\n",
    "    \n",
    "    for plot_info in plot_assignments:\n",
    "        row = plot_info['row']\n",
    "        col = plot_info['col']\n",
    "        plot_name = plot_info['name']\n",
    "        \n",
    "        # Calculate position\n",
    "        x = margin_x + col * (plot_width + spacing_x)\n",
    "        y = content_height - margin_y - plot_height - row * (plot_height + spacing_y)\n",
    "        \n",
    "        # SPECIAL HANDLING FOR QR CODE BLOCK (3,4) - CLEAN AESTHETIC VERSION\n",
    "        if plot_name == 'qr_code_block':\n",
    "            ax = fig.add_axes([x, y, plot_width, plot_height])\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.axis('off')\n",
    "            \n",
    "            # Pure white background - clean and minimal\n",
    "            ax.add_patch(Rectangle((0, 0), 1, 1, facecolor='white', edgecolor='none'))\n",
    "            \n",
    "            # Generate QR code for this block\n",
    "            try:\n",
    "                qr_block = qrcode.QRCode(\n",
    "                    version=1,\n",
    "                    error_correction=qrcode.constants.ERROR_CORRECT_L,\n",
    "                    box_size=8,  # Slightly larger for better readability\n",
    "                    border=2,    # Clean border\n",
    "                )\n",
    "                qr_block.add_data(colab_url)\n",
    "                qr_block.make(fit=True)\n",
    "                \n",
    "                qr_img_block = qr_block.make_image(fill_color=\"black\", back_color=\"white\")\n",
    "                qr_path_block = \"temp_qr_block.png\"\n",
    "                qr_img_block.save(qr_path_block)\n",
    "                \n",
    "                # Position QR code elegantly on the right with more space\n",
    "                qr_block_ax = fig.add_axes([x + plot_width*0.58, y + plot_height*0.25, \n",
    "                                          plot_width*0.38, plot_height*0.5])\n",
    "                qr_image_block = Image.open(qr_path_block)\n",
    "                qr_block_ax.imshow(qr_image_block)\n",
    "                qr_block_ax.axis('off')\n",
    "                \n",
    "                # Clean up temp file\n",
    "                os.remove(qr_path_block)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"QR block generation failed: {e}\")\n",
    "                # Elegant fallback\n",
    "                qr_placeholder = Rectangle((0.58, 0.30), 0.38, 0.5, \n",
    "                                         facecolor='#f8f9fa', \n",
    "                                         edgecolor='#dee2e6', \n",
    "                                         linewidth=2)\n",
    "                ax.add_patch(qr_placeholder)\n",
    "                ax.text(0.77, 0.5, 'QR\\nCODE', ha='center', va='center', \n",
    "                       fontsize=14, fontweight='bold', color='#6c757d')\n",
    "            \n",
    "            # BEAUTIFUL TYPOGRAPHY - LEFT SIDE\n",
    "            # Main headline - match header font styling but slightly toned down\n",
    "            ax.text(0.05, 0.82, 'Run this analysis', \n",
    "                   ha='left', va='center', \n",
    "                   fontsize=22,           # Slightly smaller than before\n",
    "                   fontweight='600',      # Medium-bold instead of bold\n",
    "                   color='#2c3e50',       # Keep the nice color\n",
    "                   fontfamily='sans-serif')  # Match header font family\n",
    "            \n",
    "            ax.text(0.05, 0.70, 'on your phone!', \n",
    "                   ha='left', va='center', \n",
    "                   fontsize=22,           # Matching size\n",
    "                   fontweight='600',      # Medium-bold\n",
    "                   color='#2c3e50',\n",
    "                   fontfamily='sans-serif')\n",
    "            \n",
    "            # Descriptive text - much bigger and readable, matching header font\n",
    "            ax.text(0.05, 0.55, 'Generate this Poster', \n",
    "                   ha='left', va='center', \n",
    "                   fontsize=18,           # Much bigger for readability\n",
    "                   fontweight='normal',   \n",
    "                   color='#34495e',\n",
    "                   fontfamily='sans-serif')\n",
    "            \n",
    "            ax.text(0.05, 0.45, 'Explore Statistics and Code', \n",
    "                   ha='left', va='center', \n",
    "                   fontsize=18,           # Much bigger\n",
    "                   fontweight='normal', \n",
    "                   color='#34495e',\n",
    "                   fontfamily='sans-serif')\n",
    "            \n",
    "            ax.text(0.05, 0.35, 'Deploy your own VM', \n",
    "                   ha='left', va='center', \n",
    "                   fontsize=18,           # Much bigger\n",
    "                   fontweight='normal', \n",
    "                   color='#34495e',\n",
    "                   fontfamily='sans-serif')\n",
    "            \n",
    "            # Call to action - bigger and stylish\n",
    "            ax.text(0.05, 0.22, 'Scan here (Passcode: asa) ‚Üí', \n",
    "                   ha='left', va='center', \n",
    "                   fontsize=18,           # Much bigger\n",
    "                   fontweight='500',      \n",
    "                   color='#e74c3c',       \n",
    "                   style='italic',\n",
    "                   fontfamily='sans-serif')\n",
    "            \n",
    "            # Link description - much bigger and clear\n",
    "            ax.text(0.77, 0.22, 'Google Colab Link', \n",
    "                   ha='center', va='center', \n",
    "                   fontsize=16,           # Much bigger from 11\n",
    "                   fontweight='bold', \n",
    "                   color='#7f8c8d',       \n",
    "                   fontfamily='sans-serif',  # Match header font\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.4\", \n",
    "                            facecolor='#ecf0f1',   \n",
    "                            edgecolor='none',\n",
    "                            alpha=0.8))\n",
    "            \n",
    "            plots_placed += 1\n",
    "            print(f\"  ‚úÖ Created clean aesthetic QR code block at row {row+1}, col {col+1}\")\n",
    "            continue\n",
    "        \n",
    "        # Handle regular plots - REMOVED BORDERS COMPLETELY\n",
    "        png_file = plots_dir / f\"{plot_name}.png\"\n",
    "        \n",
    "        if png_file.exists():\n",
    "            try:\n",
    "                # Load image\n",
    "                img = Image.open(png_file)\n",
    "                \n",
    "                # Create subplot at calculated position\n",
    "                ax = fig.add_axes([x, y, plot_width, plot_height])\n",
    "                ax.imshow(img)\n",
    "                ax.axis('off')\n",
    "                \n",
    "                # REMOVED: No borders around plots for clean look\n",
    "                # border = Rectangle((0, 0), 1, 1, fill=False, edgecolor='#e2e8f0', \n",
    "                #                  linewidth=1.5, transform=ax.transAxes)\n",
    "                # ax.add_patch(border)\n",
    "                \n",
    "                plots_placed += 1\n",
    "                print(f\"  ‚úÖ Placed: {plot_name} at row {row+1}, col {col+1}\")\n",
    "                continue\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Failed to load {plot_name}: {e}\")\n",
    "        \n",
    "        # Create placeholder for missing plots\n",
    "        ax = fig.add_axes([x, y, plot_width, plot_height])\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        \n",
    "        # Style for missing plots - updated colors\n",
    "        ax.add_patch(Rectangle((0, 0), 1, 1, facecolor='#fef3c7', edgecolor='#f59e0b', linewidth=2))\n",
    "        ax.text(0.5, 0.5, f'Plot Missing:\\n{plot_name}', ha='center', va='center', \n",
    "               fontsize=10, color='#92400e', weight='bold')\n",
    "        \n",
    "        ax.axis('off')\n",
    "        print(f\"  ‚ö†Ô∏è  Missing: {plot_name} at row {row+1}, col {col+1}\")\n",
    "    \n",
    "    # ====================================\n",
    "    # FOOTER SECTION - ENHANCED WITH PROFESSIONAL STYLING\n",
    "    # ====================================\n",
    "    \n",
    "    # Footer text with enhanced styling\n",
    "    footer_ax = fig.add_axes([0.02, 0.002, 0.96, 0.03])\n",
    "    footer_text = \"| ASA 2025 | Data: HCUP NASS 2020 | Software: Python + R in Jupyterlab | Hardware: Local/Colab/VM |\"\n",
    "    footer_ax.text(0.99, 0.5, footer_text, ha='right', va='center', fontsize=10,\n",
    "                   color='#64748b', style='italic')\n",
    "    footer_ax.axis('off')\n",
    "    \n",
    "    # ====================================\n",
    "    # SAVE POSTER\n",
    "    # ====================================\n",
    "    \n",
    "    # Save poster with high quality\n",
    "    poster_file = \"NASS_Research_Poster_Fixed.png\"\n",
    "    plt.savefig(poster_file, dpi=300, bbox_inches='tight', facecolor='white', \n",
    "                edgecolor='none', pad_inches=0.1)\n",
    "    \n",
    "    # Also save as PDF\n",
    "    poster_pdf = \"NASS_Research_Poster_Fixed.pdf\"\n",
    "    plt.savefig(poster_pdf, dpi=300, bbox_inches='tight', facecolor='white', \n",
    "                edgecolor='none', pad_inches=0.1)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüéâ POSTER GENERATION COMPLETE!\")\n",
    "    print(f\"   üìä Plots placed: {plots_placed}/12 (including QR block)\")\n",
    "    print(f\"   üè• Logo: RUHS {'loaded with equal margins' if logo_loaded else 'placeholder'}\")\n",
    "    print(f\"   üé® Header: Professional dark blue/purple gradient\")\n",
    "    print(f\"   ‚ú® Fixed: Logo positioning, all text bold, adjusted spacing\")\n",
    "    print(f\"   üìã Grid layout: 4√ó3 = 12 positions\")\n",
    "    print(f\"   üì± QR code block: Position 3,4 with professional styling\")\n",
    "    print(f\"   üé® Layout: Heatmap moved to column 3, no plot borders\")\n",
    "    print(f\"   üìÑ Saved: {poster_file}\")\n",
    "    print(f\"   üìÑ Saved: {poster_pdf}\")\n",
    "    print(f\"   üìê Dimensions: {poster_width}\\\" √ó {poster_height}\\\" (16:9 ratio)\")\n",
    "    \n",
    "    return poster_file\n",
    "\n",
    "# Generate the fixed poster\n",
    "poster_file = create_research_poster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Service/Diag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Census API Key Encryption Tool\n",
    "\n",
    "Run this code to encrypt your Census API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import base64\n",
    "# import getpass\n",
    "# from Crypto.Cipher import AES\n",
    "# from Crypto.Hash import SHA256\n",
    "# from Crypto.Random import get_random_bytes\n",
    "# import os\n",
    "\n",
    "# def encrypt_census_api_key():\n",
    "#     \"\"\"Encrypt Census API key for secure storage\"\"\"\n",
    "    \n",
    "#     print(\"üîê CENSUS API KEY ENCRYPTION\")\n",
    "#     print(\"=\" * 40)\n",
    "    \n",
    "#     # Securely get Census API key (no echo)\n",
    "#     print(\"Enter your Census API key from: https://api.census.gov/data/key_signup.html\")\n",
    "#     api_key = getpass.getpass(\"Census API key (will not echo): \")\n",
    "    \n",
    "#     if not api_key or not api_key.strip():\n",
    "#         print(\"‚ùå No API key provided\")\n",
    "#         return None, None\n",
    "    \n",
    "#     print(f\"‚úÖ Census API key entered ({len(api_key)} characters)\")\n",
    "    \n",
    "#     # Get encryption password\n",
    "#     password = getpass.getpass(\"Enter encryption password for Census key: \")\n",
    "#     if not password:\n",
    "#         print(\"‚ùå No password provided\")\n",
    "#         return None, None\n",
    "    \n",
    "#     print(f\"‚úÖ Password set ({len(password)} characters)\")\n",
    "    \n",
    "#     # Create JSON structure (matching your GCS key format)\n",
    "#     key_data = {\n",
    "#         \"api_key\": api_key,\n",
    "#         \"source\": \"census_bureau\",\n",
    "#         \"created\": \"2025-09-16\",\n",
    "#         \"purpose\": \"NASS_research_analysis\"\n",
    "#     }\n",
    "    \n",
    "#     try:\n",
    "#         print(\"\\nüîí Encrypting Census API key...\")\n",
    "        \n",
    "#         # Convert to JSON string\n",
    "#         key_json = json.dumps(key_data)\n",
    "        \n",
    "#         # Create encryption key from password\n",
    "#         encryption_key = SHA256.new(password.encode()).digest()\n",
    "        \n",
    "#         # Pad data to AES block size (PKCS7 padding)\n",
    "#         pad_len = 16 - (len(key_json) % 16)\n",
    "#         padded_data = key_json + chr(pad_len) * pad_len\n",
    "        \n",
    "#         # Generate random IV\n",
    "#         iv = get_random_bytes(16)\n",
    "        \n",
    "#         # Encrypt\n",
    "#         cipher = AES.new(encryption_key, AES.MODE_CBC, iv)\n",
    "#         encrypted_data = cipher.encrypt(padded_data.encode('utf-8'))\n",
    "        \n",
    "#         # Combine IV + encrypted data and base64 encode\n",
    "#         final_encrypted = base64.b64encode(iv + encrypted_data)\n",
    "        \n",
    "#         print(\"‚úÖ Encryption successful\")\n",
    "#         print(f\"   Original size: {len(key_json)} bytes\")\n",
    "#         print(f\"   Encrypted size: {len(final_encrypted)} bytes\")\n",
    "        \n",
    "#         # Save encrypted file\n",
    "#         encrypted_file = \"census_key.enc\"\n",
    "#         with open(encrypted_file, 'wb') as f:\n",
    "#             f.write(final_encrypted)\n",
    "#         print(f\"‚úÖ Encrypted file saved: {encrypted_file}\")\n",
    "        \n",
    "#         # Test decryption immediately\n",
    "#         print(\"\\nüîì Testing decryption...\")\n",
    "        \n",
    "#         # Read and decrypt\n",
    "#         with open(encrypted_file, 'rb') as f:\n",
    "#             encrypted_content = f.read()\n",
    "        \n",
    "#         encrypted_data_with_iv = base64.b64decode(encrypted_content)\n",
    "#         iv = encrypted_data_with_iv[:16]\n",
    "#         encrypted_data = encrypted_data_with_iv[16:]\n",
    "        \n",
    "#         cipher = AES.new(encryption_key, AES.MODE_CBC, iv)\n",
    "#         decrypted_padded = cipher.decrypt(encrypted_data)\n",
    "        \n",
    "#         pad_len = decrypted_padded[-1]\n",
    "#         decrypted_data = decrypted_padded[:-pad_len].decode('utf-8')\n",
    "#         decrypted_key = json.loads(decrypted_data)\n",
    "        \n",
    "#         if decrypted_key['api_key'] == api_key:\n",
    "#             print(\"‚úÖ Decryption test successful!\")\n",
    "#             print(f\"   Recovered API key: {decrypted_key['api_key'][:8]}...{decrypted_key['api_key'][-8:]}\")\n",
    "#         else:\n",
    "#             print(\"‚ùå Decryption test failed!\")\n",
    "#             return None, None\n",
    "        \n",
    "#         return encrypted_file, password\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Encryption failed: {e}\")\n",
    "#         return None, None\n",
    "    \n",
    "#     finally:\n",
    "#         # Security cleanup - overwrite sensitive variables\n",
    "#         try:\n",
    "#             api_key = 'X' * len(api_key) if api_key else ''\n",
    "#             password = 'X' * len(password) if password else ''\n",
    "#             if 'key_data' in locals():\n",
    "#                 key_data['api_key'] = 'X' * len(key_data['api_key'])\n",
    "#             import gc\n",
    "#             gc.collect()\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "# # Run the encryption\n",
    "# encrypted_file, password = encrypt_census_api_key()\n",
    "\n",
    "# if encrypted_file and password:\n",
    "#     print(f\"\\nüéâ SUCCESS!\")\n",
    "#     print(f\"   ‚úÖ Encrypted file: {encrypted_file}\")\n",
    "#     print(f\"   üîí Password: {password}\")\n",
    "#     print(f\"\\nüìã NEXT STEPS:\")\n",
    "#     print(f\"   1. Upload '{encrypted_file}' to your GitHub release:\")\n",
    "#     print(f\"      https://github.com/SeenaKhosravi/NASS/releases/tag/v1.0.0.0\")\n",
    "#     print(f\"   2. Use this URL in your notebook:\")\n",
    "#     print(f\"      https://github.com/SeenaKhosravi/NASS/releases/download/v1.0.0.0/census_key.enc\")\n",
    "#     print(f\"   3. Use password: {password}\")\n",
    "#     print(f\"\\nüí° Your notebook is already configured to use this!\")\n",
    "#     print(f\"\\nüîí Security note: API key never stored in code or visible on screen\")\n",
    "# else:\n",
    "#     print(\"‚ùå Encryption failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCS Service Agent Key Encryption\n",
    "\n",
    "Requires raw key json file downloaded from Service Agent settings in Google console. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import base64\n",
    "# import getpass\n",
    "# from Crypto.Cipher import AES\n",
    "# from Crypto.Hash import SHA256\n",
    "# from Crypto.Random import get_random_bytes\n",
    "# import tempfile\n",
    "# import os\n",
    "\n",
    "# def encrypt_and_test_key():\n",
    "#     \"\"\"Encrypt a service account key and immediately test decryption\"\"\"\n",
    "    \n",
    "#     print(\"üîê SERVICE ACCOUNT KEY ENCRYPTION & TEST\")\n",
    "#     print(\"=\" * 50)\n",
    "    \n",
    "#     # Step 1: Load your key file\n",
    "#     key_file_path = input(\"Enter path to your service account JSON file: \").strip()\n",
    "    \n",
    "#     if not os.path.exists(key_file_path):\n",
    "#         print(f\"‚ùå File not found: {key_file_path}\")\n",
    "#         return None, None  # Return tuple instead of just None\n",
    "    \n",
    "#     try:\n",
    "#         with open(key_file_path, 'r') as f:\n",
    "#             key_data = json.load(f)\n",
    "#         print(\"‚úÖ Key file loaded successfully\")\n",
    "#         print(f\"   Project: {key_data.get('project_id')}\")\n",
    "#         print(f\"   Email: {key_data.get('client_email')}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Failed to load key file: {e}\")\n",
    "#         return None, None  # Return tuple instead of just None\n",
    "    \n",
    "#     # Step 2: Get encryption password\n",
    "#     password = getpass.getpass(\"Enter encryption password: \")\n",
    "#     if not password:\n",
    "#         print(\"‚ùå No password provided\")\n",
    "#         return None, None  # Return tuple instead of just None\n",
    "    \n",
    "#     print(f\"‚úÖ Password set ({len(password)} characters)\")\n",
    "    \n",
    "#     # Step 3: Encrypt the key\n",
    "#     print(\"\\nüîí ENCRYPTING KEY...\")\n",
    "    \n",
    "#     try:\n",
    "#         # Convert to JSON string\n",
    "#         key_json = json.dumps(key_data)\n",
    "        \n",
    "#         # Create encryption key from password\n",
    "#         encryption_key = SHA256.new(password.encode()).digest()\n",
    "        \n",
    "#         # Pad data to AES block size (PKCS7 padding)\n",
    "#         pad_len = 16 - (len(key_json) % 16)\n",
    "#         padded_data = key_json + chr(pad_len) * pad_len\n",
    "        \n",
    "#         # Generate random IV\n",
    "#         iv = get_random_bytes(16)\n",
    "        \n",
    "#         # Encrypt\n",
    "#         cipher = AES.new(encryption_key, AES.MODE_CBC, iv)\n",
    "#         encrypted_data = cipher.encrypt(padded_data.encode('utf-8'))\n",
    "        \n",
    "#         # Combine IV + encrypted data and base64 encode\n",
    "#         final_encrypted = base64.b64encode(iv + encrypted_data)\n",
    "        \n",
    "#         print(\"‚úÖ Encryption successful\")\n",
    "#         print(f\"   Original size: {len(key_json)} bytes\")\n",
    "#         print(f\"   Encrypted size: {len(final_encrypted)} bytes\")\n",
    "        \n",
    "#         # Save encrypted file\n",
    "#         encrypted_file = key_file_path.replace('.json', '.enc')\n",
    "#         with open(encrypted_file, 'wb') as f:\n",
    "#             f.write(final_encrypted)\n",
    "#         print(f\"‚úÖ Encrypted file saved: {encrypted_file}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Encryption failed: {e}\")\n",
    "#         return None, None\n",
    "    \n",
    "#     # Step 4: Immediately test decryption\n",
    "#     print(\"\\nüîì TESTING DECRYPTION...\")\n",
    "    \n",
    "#     try:\n",
    "#         # Read the encrypted file we just created\n",
    "#         with open(encrypted_file, 'rb') as f:\n",
    "#             encrypted_content = f.read()\n",
    "        \n",
    "#         # Decode base64\n",
    "#         encrypted_data_with_iv = base64.b64decode(encrypted_content)\n",
    "        \n",
    "#         # Extract IV and encrypted data\n",
    "#         iv = encrypted_data_with_iv[:16]\n",
    "#         encrypted_data = encrypted_data_with_iv[16:]\n",
    "        \n",
    "#         # Decrypt\n",
    "#         cipher = AES.new(encryption_key, AES.MODE_CBC, iv)\n",
    "#         decrypted_padded = cipher.decrypt(encrypted_data)\n",
    "        \n",
    "#         # Remove PKCS7 padding\n",
    "#         pad_len = decrypted_padded[-1]\n",
    "#         decrypted_data = decrypted_padded[:-pad_len].decode('utf-8')\n",
    "        \n",
    "#         # Parse as JSON\n",
    "#         decrypted_key = json.loads(decrypted_data)\n",
    "        \n",
    "#         print(\"‚úÖ Decryption successful!\")\n",
    "#         print(f\"   Project: {decrypted_key.get('project_id')}\")\n",
    "#         print(f\"   Email: {decrypted_key.get('client_email')}\")\n",
    "        \n",
    "#         # Verify it matches original\n",
    "#         if decrypted_key == key_data:\n",
    "#             print(\"‚úÖ Decrypted data matches original perfectly!\")\n",
    "#         else:\n",
    "#             print(\"‚ö†Ô∏è  Decrypted data differs from original\")\n",
    "        \n",
    "#         return encrypted_file, password\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Decryption test failed: {e}\")\n",
    "#         return None, None\n",
    "\n",
    "# def test_gcs_access(encrypted_file, password):\n",
    "#     \"\"\"Test GCS access using the encrypted key\"\"\"\n",
    "    \n",
    "#     print(\"\\n‚òÅÔ∏è  TESTING GCS ACCESS...\")\n",
    "    \n",
    "#     try:\n",
    "#         from google.cloud import storage\n",
    "        \n",
    "#         # Decrypt the key for GCS access\n",
    "#         with open(encrypted_file, 'rb') as f:\n",
    "#             encrypted_content = f.read()\n",
    "        \n",
    "#         # Decode and decrypt\n",
    "#         encrypted_data_with_iv = base64.b64decode(encrypted_content)\n",
    "#         iv = encrypted_data_with_iv[:16]\n",
    "#         encrypted_data = encrypted_data_with_iv[16:]\n",
    "        \n",
    "#         encryption_key = SHA256.new(password.encode()).digest()\n",
    "#         cipher = AES.new(encryption_key, AES.MODE_CBC, iv)\n",
    "#         decrypted_padded = cipher.decrypt(encrypted_data)\n",
    "        \n",
    "#         pad_len = decrypted_padded[-1]\n",
    "#         decrypted_data = decrypted_padded[:-pad_len].decode('utf-8')\n",
    "#         decrypted_key = json.loads(decrypted_data)\n",
    "        \n",
    "#         # Create temporary file for GCS client\n",
    "#         with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as temp_file:\n",
    "#             json.dump(decrypted_key, temp_file)\n",
    "#             temp_key_path = temp_file.name\n",
    "        \n",
    "#         try:\n",
    "#             # Test GCS access\n",
    "#             client = storage.Client.from_service_account_json(temp_key_path)\n",
    "            \n",
    "#             # List buckets\n",
    "#             buckets = list(client.list_buckets())\n",
    "#             print(f\"‚úÖ GCS authentication successful!\")\n",
    "#             print(f\"   Found {len(buckets)} accessible buckets\")\n",
    "            \n",
    "#             # Show bucket names\n",
    "#             for bucket in buckets[:5]:  # Show first 5\n",
    "#                 print(f\"   - {bucket.name}\")\n",
    "            \n",
    "#             # Test specific bucket access if you have one\n",
    "#             bucket_name = input(\"\\nEnter bucket name to test (or press Enter to skip): \").strip()\n",
    "#             if bucket_name:\n",
    "#                 try:\n",
    "#                     bucket = client.bucket(bucket_name)\n",
    "#                     if bucket.exists():\n",
    "#                         print(f\"‚úÖ Bucket '{bucket_name}' exists and is accessible\")\n",
    "                        \n",
    "#                         # List some files\n",
    "#                         blobs = list(bucket.list_blobs(max_results=5))\n",
    "#                         if blobs:\n",
    "#                             print(f\"   Found {len(blobs)} files (showing first 5):\")\n",
    "#                             for blob in blobs:\n",
    "#                                 print(f\"   - {blob.name} ({blob.size / (1024*1024):.1f} MB)\")\n",
    "#                         else:\n",
    "#                             print(\"   Bucket is empty\")\n",
    "#                     else:\n",
    "#                         print(f\"‚ùå Bucket '{bucket_name}' not found or not accessible\")\n",
    "#                 except Exception as bucket_error:\n",
    "#                     print(f\"‚ùå Bucket access failed: {bucket_error}\")\n",
    "            \n",
    "#         finally:\n",
    "#             # Clean up temp file\n",
    "#             os.unlink(temp_key_path)\n",
    "            \n",
    "#     except ImportError:\n",
    "#         print(\"‚ùå google-cloud-storage not installed\")\n",
    "#         print(\"   Install with: pip install google-cloud-storage\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå GCS test failed: {e}\")\n",
    "\n",
    "# def create_upload_script(encrypted_file, password):\n",
    "#     \"\"\"Create a script to upload the encrypted key to GitHub releases\"\"\"\n",
    "    \n",
    "#     script_content = f'''\n",
    "# # Upload encrypted key to GitHub releases\n",
    "# # 1. Create a new release on GitHub\n",
    "# # 2. Upload the encrypted file: {encrypted_file}\n",
    "# # 3. Use this URL pattern in your notebook:\n",
    "# #    https://github.com/YOUR_USERNAME/YOUR_REPO/releases/download/TAG_NAME/{os.path.basename(encrypted_file)}\n",
    "\n",
    "# # Test URL (replace with your actual details):\n",
    "# # https://github.com/SeenaKhosravi/NASS/releases/download/v1.0.0.0/{os.path.basename(encrypted_file)}\n",
    "\n",
    "# # Password for decryption: {password}\n",
    "# # (Keep this secure - don't commit to git!)\n",
    "# '''\n",
    "    \n",
    "#     script_file = \"upload_instructions.txt\"\n",
    "#     with open(script_file, 'w') as f:\n",
    "#         f.write(script_content)\n",
    "    \n",
    "#     print(f\"\\nüìù Upload instructions saved to: {script_file}\")\n",
    "\n",
    "# # Run the complete test with better error handling\n",
    "# print(\"üîê Starting GCS service account encryption test...\")\n",
    "\n",
    "# # Try the encryption and test\n",
    "# encrypted_file, password = encrypt_and_test_key()\n",
    "\n",
    "# if encrypted_file and password:\n",
    "#     print(f\"\\nüéâ SUCCESS! Your key is encrypted and tested.\")\n",
    "#     print(f\"   Encrypted file: {encrypted_file}\")\n",
    "#     print(f\"   Password: {password}\")\n",
    "    \n",
    "#     # Test GCS access\n",
    "#     test_choice = input(\"\\nTest GCS access with encrypted key? (y/n): \").lower()\n",
    "#     if test_choice == 'y':\n",
    "#         test_gcs_access(encrypted_file, password)\n",
    "    \n",
    "#     # Create upload instructions\n",
    "#     create_upload_script(encrypted_file, password)\n",
    "    \n",
    "#     print(f\"\\nüìã SUMMARY:\")\n",
    "#     print(f\"   ‚úÖ Original key: Loaded and validated\")\n",
    "#     print(f\"   ‚úÖ Encryption: Working with AES-256-CBC\")\n",
    "#     print(f\"   ‚úÖ Decryption: Verified identical to original\")\n",
    "#     print(f\"   ‚úÖ File saved: {encrypted_file}\")\n",
    "#     print(f\"\\nüîí Keep your password secure: {password}\")\n",
    "#     print(f\"\\nüí° Next steps:\")\n",
    "#     print(f\"   1. Upload {encrypted_file} to GitHub releases\")\n",
    "#     print(f\"   2. Use the password in your notebook\")\n",
    "#     print(f\"   3. Delete the original .json file for security\")\n",
    "# else:\n",
    "#     print(\"\\n‚ùå Encryption/testing failed\")\n",
    "#     print(\"üí° Check the file path and try again\")\n",
    "#     print(\"   Expected format: C:\\\\path\\\\to\\\\your\\\\file.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud Storage Connection Test\n",
    "\n",
    "Requires public url encrypted key last cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Standalone GCS Authentication Test via GitHub Encrypted Key\n",
    "# # This cell is completely self-contained and works with a clean kernel\n",
    "\n",
    "# print(\"üîë STANDALONE GCS AUTHENTICATION TEST\")\n",
    "# print(\"=\" * 50)\n",
    "\n",
    "# # Step 1: Get user inputs\n",
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# print(\"Please provide the following information:\")\n",
    "# gcs_encrypted_url = input(\"Enter GCS encrypted key URL: \").strip()\n",
    "# if not gcs_encrypted_url:\n",
    "#     gcs_encrypted_url = \"https://github.com/SeenaKhosravi/NASS/releases/download/v1.0.0.0/gcs_key.enc\"\n",
    "#     print(f\"Using default URL: {gcs_encrypted_url}\")\n",
    "\n",
    "# gcs_password = getpass.getpass(\"Enter GCS decryption password: \")\n",
    "# if not gcs_password:\n",
    "#     print(\"‚ùå No password provided - cannot proceed\")\n",
    "#     exit()\n",
    "\n",
    "# gcs_bucket = input(\"Enter target bucket name (default: nass_2020): \").strip()\n",
    "# if not gcs_bucket:\n",
    "#     gcs_bucket = \"nass_2020\"\n",
    "\n",
    "# gcs_blob = input(\"Enter target file name (default: nass_2020_all.csv): \").strip()\n",
    "# if not gcs_blob:\n",
    "#     gcs_blob = \"nass_2020_all.csv\"\n",
    "\n",
    "# print(f\"\\nüìã Configuration:\")\n",
    "# print(f\"   URL: {gcs_encrypted_url}\")\n",
    "# print(f\"   Password: {'*' * len(gcs_password)}\")\n",
    "# print(f\"   Bucket: {gcs_bucket}\")\n",
    "# print(f\"   File: {gcs_blob}\")\n",
    "\n",
    "# # Step 2: Install required packages if needed\n",
    "# import subprocess\n",
    "# import sys\n",
    "\n",
    "# def install_if_missing(package, pip_name=None):\n",
    "#     \"\"\"Install package if not available\"\"\"\n",
    "#     if pip_name is None:\n",
    "#         pip_name = package\n",
    "    \n",
    "#     try:\n",
    "#         __import__(package)\n",
    "#         return True\n",
    "#     except ImportError:\n",
    "#         print(f\"Installing {pip_name}...\")\n",
    "#         try:\n",
    "#             subprocess.check_call([sys.executable, '-m', 'pip', 'install', pip_name])\n",
    "#             return True\n",
    "#         except subprocess.CalledProcessError:\n",
    "#             print(f\"‚ùå Failed to install {pip_name}\")\n",
    "#             return False\n",
    "\n",
    "# # Install required packages\n",
    "# required_packages = [\n",
    "#     ('requests', 'requests'),\n",
    "#     ('Crypto', 'pycryptodome'),\n",
    "#     ('google.cloud.storage', 'google-cloud-storage')\n",
    "# ]\n",
    "\n",
    "# print(f\"\\nüì¶ Checking required packages...\")\n",
    "# all_packages_available = True\n",
    "# for package, pip_name in required_packages:\n",
    "#     if not install_if_missing(package, pip_name):\n",
    "#         all_packages_available = False\n",
    "\n",
    "# if not all_packages_available:\n",
    "#     print(\"‚ùå Some packages failed to install - cannot proceed\")\n",
    "#     exit()\n",
    "\n",
    "# print(\"‚úÖ All required packages available\")\n",
    "\n",
    "# # Step 3: Define decryption function\n",
    "# def decrypt_key(encrypted_url, password):\n",
    "#     \"\"\"Decrypt key using pycryptodome\"\"\"\n",
    "#     try:\n",
    "#         import requests\n",
    "#         from Crypto.Cipher import AES\n",
    "#         from Crypto.Hash import SHA256\n",
    "#         import base64\n",
    "#         import json\n",
    "        \n",
    "#         print(\"üîì Downloading and decrypting access key...\")\n",
    "        \n",
    "#         # Download encrypted data\n",
    "#         response = requests.get(encrypted_url, timeout=30)\n",
    "#         response.raise_for_status()\n",
    "#         encrypted_data = base64.b64decode(response.content)\n",
    "        \n",
    "#         # Derive key from password\n",
    "#         key = SHA256.new(password.encode()).digest()\n",
    "        \n",
    "#         # Decrypt (assuming AES-CBC with IV)\n",
    "#         cipher = AES.new(key, AES.MODE_CBC, encrypted_data[:16])\n",
    "#         decrypted = cipher.decrypt(encrypted_data[16:])\n",
    "        \n",
    "#         # Remove PKCS7 padding\n",
    "#         pad_len = decrypted[-1]\n",
    "#         decrypted_key = decrypted[:-pad_len].decode()\n",
    "        \n",
    "#         # Validate it's valid JSON\n",
    "#         json.loads(decrypted_key)\n",
    "        \n",
    "#         print(\"‚úÖ Successfully decrypted access key\")\n",
    "#         return decrypted_key\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Decryption failed: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Step 4: Test GCS authentication\n",
    "# try:\n",
    "#     from google.cloud import storage\n",
    "#     import json\n",
    "#     import tempfile\n",
    "    \n",
    "#     print(f\"\\nüîì Testing GCS authentication...\")\n",
    "    \n",
    "#     # Decrypt the service account key\n",
    "#     decrypted_key = decrypt_key(gcs_encrypted_url, gcs_password)\n",
    "    \n",
    "#     if decrypted_key:\n",
    "#         print(\"‚úÖ Successfully decrypted service account key\")\n",
    "        \n",
    "#         # Parse as JSON and create temporary file\n",
    "#         key_data = json.loads(decrypted_key)\n",
    "#         print(f\"   Project ID: {key_data.get('project_id')}\")\n",
    "#         print(f\"   Service Account: {key_data.get('client_email')}\")\n",
    "        \n",
    "#         with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:\n",
    "#             json.dump(key_data, f)\n",
    "#             temp_key_file = f.name\n",
    "        \n",
    "#         try:\n",
    "#             # Test GCS client creation\n",
    "#             print(\"\\n‚òÅÔ∏è  Creating GCS client...\")\n",
    "#             client = storage.Client.from_service_account_json(temp_key_file)\n",
    "            \n",
    "#             # List accessible buckets\n",
    "#             print(\"üìä Listing accessible buckets...\")\n",
    "#             buckets = list(client.list_buckets())\n",
    "#             print(f\"‚úÖ Found {len(buckets)} accessible buckets:\")\n",
    "            \n",
    "#             for bucket in buckets:\n",
    "#                 print(f\"   ‚Ä¢ {bucket.name}\")\n",
    "            \n",
    "#             # Test specific bucket access\n",
    "#             print(f\"\\nüéØ Testing access to target bucket: {gcs_bucket}\")\n",
    "#             target_bucket = client.bucket(gcs_bucket)\n",
    "            \n",
    "#             if target_bucket.exists():\n",
    "#                 print(f\"‚úÖ Bucket '{gcs_bucket}' exists and is accessible\")\n",
    "                \n",
    "#                 # Check for the target file\n",
    "#                 target_blob = target_bucket.blob(gcs_blob)\n",
    "#                 if target_blob.exists():\n",
    "#                     target_blob.reload()\n",
    "#                     size_gb = target_blob.size / (1024**3)\n",
    "#                     print(f\"‚úÖ Target file '{gcs_blob}' found\")\n",
    "#                     print(f\"   Size: {size_gb:.2f} GB\")\n",
    "#                     print(f\"   Last modified: {target_blob.updated}\")\n",
    "#                     print(f\"   Content type: {target_blob.content_type}\")\n",
    "#                     print(f\"   MD5 hash: {target_blob.md5_hash}\")\n",
    "                    \n",
    "#                     # Test download capability\n",
    "#                     print(f\"\\nüì• Testing download capability...\")\n",
    "#                     try:\n",
    "#                         # Download first 1KB to test\n",
    "#                         sample_data = target_blob.download_as_bytes(start=0, end=1023)\n",
    "#                         print(f\"‚úÖ Download test successful ({len(sample_data)} bytes)\")\n",
    "                        \n",
    "#                         # Show first few characters if it's text\n",
    "#                         try:\n",
    "#                             preview = sample_data.decode('utf-8')[:100]\n",
    "#                             print(f\"   Preview: {preview}...\")\n",
    "#                         except:\n",
    "#                             print(f\"   Binary file detected\")\n",
    "                            \n",
    "#                     except Exception as download_error:\n",
    "#                         print(f\"‚ùå Download test failed: {download_error}\")\n",
    "                    \n",
    "#                 else:\n",
    "#                     print(f\"‚ùå Target file '{gcs_blob}' not found in bucket\")\n",
    "                    \n",
    "#                     # List some files to see what's available\n",
    "#                     print(\"üìÅ Available files in bucket (first 10):\")\n",
    "#                     blobs = list(target_bucket.list_blobs(max_results=10))\n",
    "#                     if blobs:\n",
    "#                         for blob in blobs:\n",
    "#                             print(f\"   ‚Ä¢ {blob.name} ({blob.size / (1024*1024):.1f} MB)\")\n",
    "#                     else:\n",
    "#                         print(\"   (bucket is empty)\")\n",
    "#             else:\n",
    "#                 print(f\"‚ùå Bucket '{gcs_bucket}' not found or not accessible\")\n",
    "            \n",
    "#             print(f\"\\nüéâ GCS AUTHENTICATION TEST COMPLETE!\")\n",
    "#             print(f\"   ‚Ä¢ Package installation: ‚úÖ Success\")\n",
    "#             print(f\"   ‚Ä¢ Key decryption: ‚úÖ Working\")\n",
    "#             print(f\"   ‚Ä¢ GCS authentication: ‚úÖ Working\") \n",
    "#             print(f\"   ‚Ä¢ Bucket access: ‚úÖ Working\")\n",
    "#             print(f\"   ‚Ä¢ File detection: ‚úÖ Working\")\n",
    "#             print(f\"   ‚Ä¢ Download capability: ‚úÖ Tested\")\n",
    "#             print(f\"\\nüöÄ Ready for full data loading!\")\n",
    "            \n",
    "#         finally:\n",
    "#             # Clean up temporary file\n",
    "#             try:\n",
    "#                 os.unlink(temp_key_file)\n",
    "#                 print(\"üßπ Cleaned up temporary key file\")\n",
    "#             except:\n",
    "#                 pass\n",
    "#     else:\n",
    "#         print(\"‚ùå Failed to decrypt service account key\")\n",
    "#         print(\"üí° Check your password and encrypted key URL\")\n",
    "\n",
    "# except ImportError as e:\n",
    "#     print(f\"‚ùå Missing required package after installation: {e}\")\n",
    "#     print(\"üí° Try manually installing: pip install google-cloud-storage pycryptodome\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Authentication test failed: {e}\")\n",
    "#     print(\"üí° Check your configuration and try again\")\n",
    "\n",
    "# finally:\n",
    "#     # Security cleanup\n",
    "#     try:\n",
    "#         gcs_password = 'X' * len(gcs_password)\n",
    "#         del gcs_password\n",
    "#         import gc\n",
    "#         gc.collect()\n",
    "#         print(\"üîí Security cleanup completed\")\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 50)\n",
    "# print(\"Test completed! This cell is fully self-contained.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
