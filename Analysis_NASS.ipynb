{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open on GitHub](https://img.shields.io/badge/GitHub-View%20Source-181717?style=for-the-badge&logo=github)](https://github.com/SeenaKhosravi/NASS/blob/main/Analysis_NASS.ipynb)\n",
    "[![Open In Colab](https://img.shields.io/badge/Colab-Open%20Notebook-F9AB00?style=for-the-badge&logo=google-colab)](https://colab.research.google.com/github/SeenaKhosravi/NASS/blob/main/Analysis_NASS.ipynb)\n",
    "[![Launch on GCE](https://img.shields.io/badge/Google%20Cloud-Launch%20Instance-4285F4?style=for-the-badge&logo=google-cloud)](https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https://github.com/SeenaKhosravi/NASS&cloudshell_tutorial=deploy/gce-tutorial.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbe6c3e6"
   },
   "source": [
    "# Socioeconomic and Demographic Drivers of Ambulatory Surgery Usage\n",
    "### HCUP NASS 2020 - Reproducible Pipeline (Python + R)\n",
    "\n",
    "**Author:** Seena Khosravi, MD  \n",
    "**LLMs Utilized:** Claude Sonnet 4, Opus 4; ChatGPT 4o, o4; Deepseek 3.1; Gemini 2.5 Pro  \n",
    "**Last Updated:** September 14, 2025  \n",
    "\n",
    "\n",
    "**Data Source:**  \n",
    "Department of Health & Human Services (HHS)  \n",
    "Agency for Healthcare Research and Quality (AHRQ)  \n",
    "Healthcare Cost and Utilization Project (HCUP)  \n",
    "National Ambulatory Surgical Sample (NASS) 2020\n",
    "\n",
    " **[HCUP NASS 2020 Introduction](https://hcup-us.ahrq.gov/db/nation/nass/NASS_Introduction_2020.jsp)** - Official AHRQ documentation\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOmuZCsusrTj"
   },
   "source": [
    "\n",
    "## Overview\n",
    "This notebook provides a reproducible analysis pipeline for examining socioeconomic and demographic factors influencing ambulatory surgery usage patterns. The analysis combines Python for data processing and R for statistical modeling.\n",
    "\n",
    "### Data Usage Agreement\n",
    "**DUA Compliant Online Implementation** - This notebook uses a simulated, artificial, smaller dataset with identical structure to the file created by [Raw_NASS_Processing.R](https://github.com/SeenaKhosravi/NASS/blob/a7764ce80be8a82fc449831821c27d957176c410/Raw%20NASS%20%20Processing.R). The simulated dataset production methodology is found in [Generate_Simulated_NASS.R](https://github.com/SeenaKhosravi/NASS/blob/161bf2b5c149da9654c0e887655b361fa2176db0/Generate_Simulated_NASS.R). If DUA signed and data purchased from HCUP, this notebook can run on full dataset in cloud storage (with provided passcode).\n",
    "\n",
    "[Please see the DUA Agreement here.](https://hcup-us.ahrq.gov/team/NationwideDUA.jsp)\n",
    "\n",
    "### Key Features\n",
    "- **Multi-platform:** Works on Jupyter implementations via local environments, server, cloud VM instance, or platform as a service\n",
    "- **Flexible Data Storage:** GitHub (simulated, static, open access), Google Drive, Google Cloud Storage, or local file\n",
    "- **Reproducible:** All dependencies and environment setup included; Automated, Click-through VM set-up\n",
    "- **Scalable:** Handles both simulated (0.2GB, 139k rows) and full dataset (12GB, 7.8M rows) with scalable cloud options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "019b6fe9"
   },
   "source": [
    "---\n",
    "\n",
    "## Design Notes\n",
    "\n",
    "### Architecture\n",
    "- **Python primary, w/ R run via rpy2 python extension**\n",
    "- **Python cells** handle \"plumbing\" (file I/O, environment setup, rpy2 configuration, data previews)\n",
    "- **R cells** (prefixed by `%%R`) perform statistical analysis: survey weights, Census lookups, multilevel models, plots, classifiers, etc.\n",
    "\n",
    "### Data Sources\n",
    "- **Default:** Simulated dataset (0.2GB) from GitHub releases\n",
    "- **Local:** Switch to locally stored files via configuration\n",
    "- **Drive:** Google Drive (Only available in Colab)\n",
    "- **Cloud:** Google Cloud Storage support for large datasets\n",
    "\n",
    "### Environment Support\n",
    "- **Local:** JupyterLab w/ Python 3.8+ kernel (requires R 4.0+, 4.4+ preferred)\n",
    "- **Jupyter Server:** May require configuration depending on implementation (conda recommended)\n",
    "- **Google Colab:** Pro recommended for full dataset, high-RAM runtime suggested\n",
    "- **Virtual Machine Instance:** Automated GCE deployment via scripts, pre-configured R setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvQPKX8GrfZr"
   },
   "source": [
    "---\n",
    "\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUEIOQlXrPrd"
   },
   "source": [
    "\n",
    "## 1. Configuration\n",
    "\n",
    "Configure all settings here prior to run - data sources, debugging options, and file paths. Defaults to simulated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KUU1wwMKrPre"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NASS 2020 Research Analysis Pipeline\n",
      " Configuration Status \n",
      "\n",
      "üîí Encrypted Dataset Access\n",
      "   ‚úì Full dataset access enabled\n",
      "üåê Census API Integration\n",
      "   ‚Üí Manual API key entry required later\n",
      "\n",
      "==================================================\n",
      "‚úÖ Ready: ‚òÅÔ∏è  Full Dataset (7.8M records) | Census: üîë Manual Entry\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================== CONFIGURATION ====================\n",
    "# Data Source Options\n",
    "DATA_SOURCE = \"gcs\"      # Options: \"github\", \"local\", \"gcs\", \"drive\"\n",
    "VERBOSE_PRINTS = True    # False ‚Üí suppress debug output\n",
    "\n",
    "# GitHub source (default - simulated data)\n",
    "GITHUB_URL = \"https://github.com/SeenaKhosravi/NASS/releases/download/v1.0.0/nass_2020_simulated.csv\"\n",
    "\n",
    "# Local file options\n",
    "LOCAL_FILENAME = \"nass_2020_local.csv\"\n",
    "\n",
    "# Google Cloud Storage options\n",
    "GCS_BUCKET = \"nass_2020\"\n",
    "GCS_BLOB = \"nass_2020_all.csv\"\n",
    "\n",
    "# Google Drive options (for Colab)\n",
    "DRIVE_PATH = \"/content/drive/MyDrive/NASS/nass_2020_full.csv\"\n",
    "\n",
    "# === SECURE ACCESS CONFIGURATION ===\n",
    "import getpass\n",
    "import hashlib\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "class SecurePasswordManager:\n",
    "    \"\"\"Secure password handling with memory cleanup.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._password_hash = None\n",
    "    \n",
    "    def get_password(self, prompt: str) -> Optional[str]:\n",
    "        \"\"\"Get password with enhanced security.\"\"\"\n",
    "        try:\n",
    "            password = getpass.getpass(prompt)\n",
    "            \n",
    "            if not password.strip():\n",
    "                return None\n",
    "            \n",
    "            # Store only hash for verification\n",
    "            self._password_hash = hashlib.sha256(password.encode()).hexdigest()\n",
    "            \n",
    "            return password\n",
    "            \n",
    "        except (KeyboardInterrupt, EOFError):\n",
    "            print(\"\\nPassword entry cancelled\")\n",
    "            return None\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Attempt to clear sensitive data.\"\"\"\n",
    "        self._password_hash = None\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "def configure_access():\n",
    "    \"\"\"Configure secure data source and Census access.\"\"\"\n",
    "    print(\" NASS 2020 Research Analysis Pipeline\")\n",
    "    print(\" Configuration Status \")\n",
    "    print()\n",
    "    \n",
    "    password_manager = SecurePasswordManager()\n",
    "    gcs_password = None\n",
    "    census_password = None\n",
    "    \n",
    "    # Only show GCS prompts if GCS is selected\n",
    "    if DATA_SOURCE == \"gcs\":\n",
    "        print(\"üîí Encrypted Dataset Access\")\n",
    "        gcs_password = password_manager.get_password(\n",
    "            \"   Enter GCS access key: \"\n",
    "        )\n",
    "        \n",
    "        if not gcs_password:\n",
    "            print(\"   ‚Üí Switching to public dataset\")\n",
    "        else:\n",
    "            print(\"   ‚úì Full dataset access enabled\")\n",
    "    \n",
    "    # Streamlined Census API setup\n",
    "    print(\"üåê Census API Integration\")\n",
    "    census_password = password_manager.get_password(\n",
    "        \"   Enter Census API key: \"\n",
    "    )\n",
    "    \n",
    "    if census_password:\n",
    "        print(\"   ‚úì Encrypted Census integration ready\")\n",
    "    else:\n",
    "        print(\"   ‚Üí Manual API key entry required later\")\n",
    "    \n",
    "    # Clear password manager\n",
    "    password_manager.clear_memory()\n",
    "    \n",
    "    return gcs_password, census_password\n",
    "\n",
    "# Configure access\n",
    "GCS_PASSWORD, CENSUS_PASSWORD = configure_access()\n",
    "\n",
    "# Set up encrypted URLs if passwords provided\n",
    "if GCS_PASSWORD and DATA_SOURCE == \"gcs\":\n",
    "    GCS_ENCRYPTED_KEY_URL = \"https://github.com/SeenaKhosravi/NASS/releases/download/v1.0.0.0/gcs_key.enc\"\n",
    "elif DATA_SOURCE == \"gcs\":\n",
    "    # Switch to simulated data if no GCS password\n",
    "    DATA_SOURCE = \"github\"\n",
    "\n",
    "if CENSUS_PASSWORD:\n",
    "    CENSUS_ENCRYPTED_KEY_URL = \"https://github.com/SeenaKhosravi/NASS/releases/download/v1.0.0.0/census_key.enc\"\n",
    "\n",
    "# Clean final status\n",
    "print()\n",
    "print(\"=\" * 50)\n",
    "data_status = {\n",
    "    \"github\": \"üìä Public Dataset (139K records)\",\n",
    "    \"local\": \"üíæ Local Dataset\", \n",
    "    \"gcs\": \"‚òÅÔ∏è  Full Dataset (7.8M records)\",\n",
    "    \"drive\": \"üìÇ Google Drive Dataset\"\n",
    "}\n",
    "\n",
    "census_status = \"üîê Encrypted\" if CENSUS_PASSWORD else \"üîë Manual Entry\"\n",
    "\n",
    "print(f\"‚úÖ Ready: {data_status.get(DATA_SOURCE, DATA_SOURCE)} | Census: {census_status}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WJ8_5sRrPre"
   },
   "source": [
    "---\n",
    "## 2. Python Environment Setup \n",
    "\n",
    "Detect environment and install Python packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "x9RcJjrWrPrf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment detected: Local/Jupyter\n",
      "Installing and checking packages...\n",
      "Installing pycryptodome...\n",
      "All packages ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "class EnvironmentManager:\n",
    "    def __init__(self):\n",
    "        self.detect_environment()\n",
    "        self.setup_packages()\n",
    "\n",
    "    def detect_environment(self):\n",
    "        \"\"\"Detect runtime environment\"\"\"\n",
    "        self.is_colab = 'COLAB_GPU' in os.environ or 'google.colab' in sys.modules\n",
    "        self.is_vertex = 'DL_ANACONDA_HOME' in os.environ\n",
    "\n",
    "        if self.is_colab:\n",
    "            self.env_type = \"Google Colab\"\n",
    "        elif self.is_vertex:\n",
    "            self.env_type = \"Vertex AI\"\n",
    "        else:\n",
    "            self.env_type = \"Local/Jupyter\"\n",
    "\n",
    "        print(f\"Environment detected: {self.env_type}\")\n",
    "\n",
    "    def check_conda_available(self):\n",
    "        \"\"\"Check if conda is available\"\"\"\n",
    "        try:\n",
    "            subprocess.check_call(['conda', '--version'],\n",
    "                                stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "            return True\n",
    "        except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "            return False\n",
    "\n",
    "    def install_package(self, package, conda_name=None):\n",
    "        \"\"\"Smart package installation with fallback\"\"\"\n",
    "        try:\n",
    "            __import__(package)\n",
    "            return True\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "\n",
    "            # Try conda first if available and not in Colab\n",
    "            if conda_name and not self.is_colab and self.check_conda_available():\n",
    "                try:\n",
    "                    subprocess.check_call(['conda', 'install', '-y', conda_name],\n",
    "                                        stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "                    return True\n",
    "                except subprocess.CalledProcessError:\n",
    "                    print(f\"  Conda install failed for {conda_name}, trying pip...\")\n",
    "\n",
    "            # Fallback to pip\n",
    "            try:\n",
    "                subprocess.check_call([sys.executable, '-m', 'pip', 'install', package],\n",
    "                                    stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "                return True\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"  Pip install failed for {package}: {e}\")\n",
    "                return False\n",
    "\n",
    "    def setup_packages(self):\n",
    "        packages = {\n",
    "            'pandas': 'pandas',\n",
    "            'requests': 'requests',\n",
    "            'rpy2': 'rpy2',\n",
    "            'google.cloud.storage': 'google-cloud-storage',\n",
    "            'pycryptodome': 'pycryptodome'  # For AES decryption\n",
    "        }\n",
    "\n",
    "        print(\"Installing and checking packages...\")\n",
    "        failed = []\n",
    "\n",
    "        for pkg, install_name in packages.items():\n",
    "            if not self.install_package(pkg, install_name):\n",
    "                failed.append(pkg)\n",
    "\n",
    "        # Store failed packages globally for recovery\n",
    "        globals()['failed_packages'] = failed\n",
    "\n",
    "        if failed:\n",
    "            print(f\"Warning: Failed to install: {', '.join(failed)}\")\n",
    "            print(\"Some features may not work\")\n",
    "\n",
    "            # Provide specific guidance for rpy2\n",
    "            if 'rpy2' in failed:\n",
    "                print(\"\\nFor rpy2 installation issues:\")\n",
    "                if self.is_vertex:\n",
    "                    print(\"   - Vertex AI: R may not be installed by default\")\n",
    "                    print(\"   - Run the next cell for automated R setup\")\n",
    "                else:\n",
    "                    print(\"   - On Windows: May need Visual Studio Build Tools\")\n",
    "                    print(\"   - Try: conda install -c conda-forge rpy2\")\n",
    "                    print(\"   - Or: pip install rpy2 (requires R to be installed)\")\n",
    "        else:\n",
    "            print(\"All packages ready\")\n",
    "\n",
    "        # Mount Google Drive if needed (check if DATA_SOURCE exists)\n",
    "        try:\n",
    "            if globals().get('DATA_SOURCE') == \"drive\" and self.is_colab:\n",
    "                self.mount_drive()\n",
    "        except NameError:\n",
    "            pass  # DATA_SOURCE not defined yet\n",
    "\n",
    "    def mount_drive(self):\n",
    "        \"\"\"Mount Google Drive in Colab\"\"\"\n",
    "        try:\n",
    "            from google.colab import drive\n",
    "            drive.mount('/content/drive')\n",
    "            print(\"Google Drive mounted successfully\")\n",
    "        except:\n",
    "            print(\"Error: Failed to mount Google Drive\")\n",
    "\n",
    "# Initialize environment\n",
    "env_manager = EnvironmentManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mm4dTQb1rPrh"
   },
   "source": [
    "---\n",
    "## 3. R Environment Setup\n",
    "\n",
    "Load R integration and install R packages efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7wpUbizDrPrh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error importing in API mode: ImportError('On Windows, cffi mode \"ANY\" is only \"ABI\".')\n",
      "Trying to import in ABI mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R integration loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load rpy2 extension for R integration\n",
    "try:\n",
    "    %load_ext rpy2.ipython\n",
    "    print(\"R integration loaded successfully\")\n",
    "    globals()['R_AVAILABLE'] = True\n",
    "except Exception as e:\n",
    "    print(f\"Error: Failed to load R integration: {e}\")\n",
    "\n",
    "    # Windows-specific troubleshooting\n",
    "    if \"R.dll\" in str(e) or \"error 0x7e\" in str(e):\n",
    "        print(\"\\nWindows R.dll loading issue detected:\")\n",
    "        print(\"   This is a common Windows + rpy2 compatibility issue\")\n",
    "        print(\"   Solutions:\")\n",
    "        print(\"   1. Restart Python kernel and try again\")\n",
    "        print(\"   2. Check R version compatibility with rpy2\")\n",
    "        print(\"   3. Try reinstalling R and rpy2\")\n",
    "        print(\"   4. Use Python-only analysis (fallback available)\")\n",
    "        globals()['R_AVAILABLE'] = False\n",
    "    else:\n",
    "        print(\"Install rpy2: pip install rpy2\")\n",
    "        globals()['R_AVAILABLE'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5iP0V_1rPri"
   },
   "source": [
    "Install essential R packages and attempt installation of optional packages.\n",
    "\n",
    "**Note:** Other packages needed for specific analysis (advanced modeling packages) will be installed and called as needed later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QGFfQQC5rPri"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local environment detected\n",
      "Essential packages loaded: 3 / 3 \n",
      "Optional packages loaded: 1 / 1 \n",
      "Core environment ready! (data.table + ggplot2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "In addition: Warning messages:\n",
       "1: package 'data.table' was built under R version 4.4.3 \n",
       "2: package 'ggplot2' was built under R version 4.4.3 \n",
       "3: package 'scales' was built under R version 4.4.3 \n",
       "4: package 'survey' was built under R version 4.4.3 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R -i VERBOSE_PRINTS\n",
    "\n",
    "# Environment-aware R package setup for Local, Colab, and GCE/Linux VMs\n",
    "\n",
    "# Detect environment\n",
    "is_colab <- Sys.getenv(\"COLAB_GPU\") != \"\"\n",
    "is_gce <- file.exists(\"/opt/nass\") || file.exists(\"/etc/apt\")  # GCE/Linux VM indicators\n",
    "\n",
    "if(is_colab) {\n",
    "  cat(\"Google Colab detected\\n\")\n",
    "} else if(is_gce) {\n",
    "  cat(\"GCE/Linux VM detected\\n\")\n",
    "} else {\n",
    "  cat(\"Local environment detected\\n\")\n",
    "}\n",
    "\n",
    "# Essential packages for all environments\n",
    "essential_packages <- c(\n",
    "  \"data.table\",    # Fast data manipulation\n",
    "  \"ggplot2\",       # Plotting\n",
    "  \"scales\"         # For ggplot2 percentage scales\n",
    ")\n",
    "\n",
    "optional_packages <- c(\n",
    "  \"survey\"        # Survey statistics\n",
    ")\n",
    "\n",
    "# Fast installation settings\n",
    "repos <- \"https://cloud.r-project.org\"\n",
    "options(repos = repos)\n",
    "Sys.setenv(MAKEFLAGS = paste0(\"-j\", parallel::detectCores()))\n",
    "\n",
    "# Package check and load functions\n",
    "pkg_available <- function(pkg) {\n",
    "  tryCatch({\n",
    "    find.package(pkg, quiet = TRUE)\n",
    "    TRUE\n",
    "  }, error = function(e) FALSE)\n",
    "}\n",
    "\n",
    "load_pkg <- function(pkg) {\n",
    "  tryCatch({\n",
    "    suppressMessages(library(pkg, character.only = TRUE, quietly = TRUE))\n",
    "    TRUE\n",
    "  }, error = function(e) FALSE)\n",
    "}\n",
    "\n",
    "# Install missing essential packages\n",
    "missing_essential <- essential_packages[!sapply(essential_packages, pkg_available)]\n",
    "\n",
    "if(length(missing_essential) > 0) {\n",
    "  cat(\"Installing essential packages:\", paste(missing_essential, collapse = \", \"), \"\\n\")\n",
    "\n",
    "  tryCatch({\n",
    "    install.packages(missing_essential,\n",
    "                    repos = repos,\n",
    "                    type = getOption(\"pkgType\"),\n",
    "                    dependencies = FALSE,\n",
    "                    quiet = !VERBOSE_PRINTS,\n",
    "                    Ncpus = parallel::detectCores())\n",
    "  }, error = function(e) {\n",
    "    cat(\"Binary install failed, trying source...\\n\")\n",
    "    install.packages(missing_essential,\n",
    "                    repos = repos,\n",
    "                    type = \"source\",\n",
    "                    dependencies = FALSE,\n",
    "                    quiet = !VERBOSE_PRINTS)\n",
    "  })\n",
    "}\n",
    "\n",
    "# Load essential packages\n",
    "essential_loaded <- sapply(essential_packages, load_pkg)\n",
    "essential_success <- sum(essential_loaded)\n",
    "\n",
    "cat(\"Essential packages loaded:\", essential_success, \"/\", length(essential_packages), \"\\n\")\n",
    "\n",
    "# Quick install optional packages (30s timeout)\n",
    "missing_optional <- optional_packages[!sapply(optional_packages, pkg_available)]\n",
    "\n",
    "if(length(missing_optional) > 0) {\n",
    "  cat(\"Installing optional packages...\\n\")\n",
    "\n",
    "  for(pkg in missing_optional) {\n",
    "    tryCatch({\n",
    "      setTimeLimit(cpu = 30, elapsed = 30, transient = TRUE)\n",
    "      install.packages(pkg, repos = repos,\n",
    "                      type = getOption(\"pkgType\"),\n",
    "                      dependencies = FALSE,\n",
    "                      quiet = TRUE)\n",
    "      cat(\"Installed:\", pkg, \"\\n\")\n",
    "    }, error = function(e) {\n",
    "      cat(\"Skipped (timeout):\", pkg, \"\\n\")\n",
    "    })\n",
    "\n",
    "    setTimeLimit(cpu = Inf, elapsed = Inf, transient = FALSE)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Load optional packages\n",
    "optional_loaded <- sapply(optional_packages, load_pkg)\n",
    "optional_success <- sum(optional_loaded)\n",
    "\n",
    "cat(\"Optional packages loaded:\", optional_success, \"/\", length(optional_packages), \"\\n\")\n",
    "\n",
    "# Check core functionality\n",
    "has_datatable <- require(\"data.table\", quietly = TRUE)\n",
    "has_ggplot <- require(\"ggplot2\", quietly = TRUE)\n",
    "\n",
    "# Enhanced GCE/Linux setup if packages are missing\n",
    "if(is_gce && (!has_datatable || !has_ggplot)) {\n",
    "  cat(\"\\nGCE/Linux detected - attempting enhanced installation...\\n\")\n",
    "\n",
    "  # System dependencies for Linux VMs\n",
    "  if(Sys.which(\"apt-get\") != \"\") {\n",
    "    cat(\"Installing system dependencies...\\n\")\n",
    "    system_deps <- c(\n",
    "      \"apt-get update -qq\",\n",
    "      \"apt-get install -y libfontconfig1-dev libcairo2-dev\",\n",
    "      \"apt-get install -y libxml2-dev libcurl4-openssl-dev libssl-dev\",\n",
    "      \"apt-get install -y libharfbuzz-dev libfribidi-dev\",\n",
    "      \"apt-get install -y libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev\"\n",
    "    )\n",
    "    \n",
    "    for(cmd in system_deps) {\n",
    "      system(paste(\"sudo\", cmd), ignore.stdout = TRUE, ignore.stderr = TRUE)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  # Retry failed packages with multiple repositories\n",
    "  failed_packages <- c()\n",
    "  if(!has_datatable) failed_packages <- c(failed_packages, \"data.table\")\n",
    "  if(!has_ggplot) failed_packages <- c(failed_packages, \"ggplot2\", \"scales\")\n",
    "\n",
    "  repos_gce <- c(\"https://cran.rstudio.com/\", \"https://cloud.r-project.org\")\n",
    "\n",
    "  for(pkg in failed_packages) {\n",
    "    cat(\"Installing\", pkg, \"...\")\n",
    "    installed <- FALSE\n",
    "\n",
    "    for(repo in repos_gce) {\n",
    "      tryCatch({\n",
    "        install.packages(pkg, repos = repo, dependencies = TRUE, quiet = TRUE)\n",
    "        if(require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "          cat(\" Success\\n\")\n",
    "          installed <- TRUE\n",
    "          break\n",
    "        }\n",
    "      }, error = function(e) NULL)\n",
    "    }\n",
    "\n",
    "    if(!installed) cat(\" FAILED\\n\")\n",
    "  }\n",
    "\n",
    "  # Re-check after enhanced installation\n",
    "  has_datatable <- require(\"data.table\", quietly = TRUE)\n",
    "  has_ggplot <- require(\"ggplot2\", quietly = TRUE)\n",
    "\n",
    "  cat(\"After enhanced installation: data.table =\", has_datatable, \"| ggplot2 =\", has_ggplot, \"\\n\")\n",
    "}\n",
    "\n",
    "# Final status check\n",
    "if(has_datatable && has_ggplot) {\n",
    "  cat(\"Core environment ready! (data.table + ggplot2)\\n\")\n",
    "  setDTthreads(0)  # Use all cores\n",
    "\n",
    "} else if(has_datatable) {\n",
    "  cat(\"Warning: Partial setup - data.table ready, plotting may be limited\\n\")\n",
    "  setDTthreads(0)\n",
    "\n",
    "} else {\n",
    "  cat(\"Error: Critical failure - data.table not available\\n\")\n",
    "  stop(\"Cannot proceed without data.table\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAtjPad_rPri"
   },
   "source": [
    "Verify R setup is complete and ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WMh-C5gvrPrj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying R environment...\n",
      "data.table ready\n",
      "ggplot2 ready\n",
      "R environment optimized and ready!\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "# Quick verification and setup\n",
    "cat(\"Verifying R environment...\\n\")\n",
    "\n",
    "# Test core functionality\n",
    "tryCatch({\n",
    "  # Test data.table (essential)\n",
    "  dt_test <- data.table(x = 1:3, y = letters[1:3])\n",
    "  cat(\"data.table ready\\n\")\n",
    "\n",
    "  # Test ggplot2 (optional)\n",
    "  if(require(\"ggplot2\", quietly = TRUE)) {\n",
    "    cat(\"ggplot2 ready\\n\")\n",
    "  } else {\n",
    "    cat(\"Warning: ggplot2 not available (plots disabled)\\n\")\n",
    "  }\n",
    "\n",
    "  # Set up data.table options for performance\n",
    "  setDTthreads(0)  # Use all cores\n",
    "\n",
    "  cat(\"R environment optimized and ready!\\n\")\n",
    "\n",
    "}, error = function(e) {\n",
    "  cat(\"Error: R environment verification failed:\", e$message, \"\\n\")\n",
    "  stop(\"R setup incomplete\")\n",
    "})\n",
    "\n",
    "# Clean up test objects\n",
    "rm(list = ls()[!ls() %in% c(\"VERBOSE_PRINTS\")])\n",
    "invisible(gc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8pUXVbsrPrg"
   },
   "source": [
    "---\n",
    "## 4. Data Loading\n",
    "\n",
    "Config based data loader with error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "s-KUtlK-rPrh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: Local/Jupyter\n",
      "Loading NASS data from gcs...\n",
      "Loading data from source: gcs\n",
      "Downloading and decrypting access key...\n",
      "‚úÖ Successfully decrypted access key\n",
      "‚úÖ Authenticated using encrypted service account key\n",
      "Accessing gs://nass_2020/nass_2020_all.csv\n",
      "File size: 11.18 GB\n",
      "Downloading from GCS...\n",
      "Progress: 3.1%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 512\u001b[39m\n\u001b[32m    510\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DATA_SOURCE \u001b[38;5;129;01min\u001b[39;00m config_map:\n\u001b[32m    511\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading NASS data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_SOURCE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m     df = \u001b[43mdata_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_SOURCE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDATA_SOURCE\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    515\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Successfully loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows √ó \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m columns\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36mNASSDataLoader.load_data\u001b[39m\u001b[34m(self, source, **config)\u001b[39m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._load_from_drive(config.get(\u001b[33m'\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m source == \u001b[33m\"\u001b[39m\u001b[33mgcs\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_from_gcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._report_error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported source \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m for current environment\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 303\u001b[39m, in \u001b[36mNASSDataLoader._load_from_gcs\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    301\u001b[39m \u001b[38;5;66;03m# Use resumable download for large files\u001b[39;00m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m blob.size > \u001b[32m100\u001b[39m * \u001b[32m1024\u001b[39m * \u001b[32m1024\u001b[39m:  \u001b[38;5;66;03m# 100MB\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_large_blob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    305\u001b[39m     blob.download_to_filename(cache_file)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 394\u001b[39m, in \u001b[36mNASSDataLoader._download_large_blob\u001b[39m\u001b[34m(self, blob, cache_file)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk_start \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, blob.size, chunk_size):\n\u001b[32m    392\u001b[39m     chunk_end = \u001b[38;5;28mmin\u001b[39m(chunk_start + chunk_size - \u001b[32m1\u001b[39m, blob.size - \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     chunk_data = \u001b[43mblob\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_as_bytes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_end\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m     f.write(chunk_data)\n\u001b[32m    400\u001b[39m     downloaded += \u001b[38;5;28mlen\u001b[39m(chunk_data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\cloud\\storage\\blob.py:1541\u001b[39m, in \u001b[36mBlob.download_as_bytes\u001b[39m\u001b[34m(self, client, start, end, raw_download, if_etag_match, if_etag_not_match, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry, single_shot_download)\u001b[39m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m create_trace_span(name=\u001b[33m\"\u001b[39m\u001b[33mStorage.Blob.downloadAsBytes\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1539\u001b[39m     string_buffer = BytesIO()\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prep_and_do_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1542\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstring_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1544\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1546\u001b[39m \u001b[43m        \u001b[49m\u001b[43mraw_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1547\u001b[39m \u001b[43m        \u001b[49m\u001b[43mif_etag_match\u001b[49m\u001b[43m=\u001b[49m\u001b[43mif_etag_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1548\u001b[39m \u001b[43m        \u001b[49m\u001b[43mif_etag_not_match\u001b[49m\u001b[43m=\u001b[49m\u001b[43mif_etag_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1549\u001b[39m \u001b[43m        \u001b[49m\u001b[43mif_generation_match\u001b[49m\u001b[43m=\u001b[49m\u001b[43mif_generation_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1550\u001b[39m \u001b[43m        \u001b[49m\u001b[43mif_generation_not_match\u001b[49m\u001b[43m=\u001b[49m\u001b[43mif_generation_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_match\u001b[49m\u001b[43m=\u001b[49m\u001b[43mif_metageneration_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[43m=\u001b[49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1553\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1554\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchecksum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchecksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1555\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1556\u001b[39m \u001b[43m        \u001b[49m\u001b[43msingle_shot_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43msingle_shot_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1557\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1558\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m string_buffer.getvalue()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\cloud\\storage\\blob.py:4439\u001b[39m, in \u001b[36mBlob._prep_and_do_download\u001b[39m\u001b[34m(self, file_obj, client, start, end, raw_download, if_etag_match, if_etag_not_match, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry, single_shot_download, command)\u001b[39m\n\u001b[32m   4436\u001b[39m transport = client._http\n\u001b[32m   4438\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4439\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4440\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4441\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4443\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4446\u001b[39m \u001b[43m        \u001b[49m\u001b[43mraw_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4447\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4448\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchecksum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchecksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4449\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4450\u001b[39m \u001b[43m        \u001b[49m\u001b[43msingle_shot_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43msingle_shot_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4451\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4452\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidResponse \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m   4453\u001b[39m     _raise_from_invalid_response(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\cloud\\storage\\blob.py:1105\u001b[39m, in \u001b[36mBlob._do_download\u001b[39m\u001b[34m(self, transport, file_obj, download_url, headers, start, end, raw_download, timeout, checksum, retry, single_shot_download)\u001b[39m\n\u001b[32m   1087\u001b[39m     download = klass(\n\u001b[32m   1088\u001b[39m         download_url,\n\u001b[32m   1089\u001b[39m         stream=file_obj,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1098\u001b[39m         single_shot_download=single_shot_download,\n\u001b[32m   1099\u001b[39m     )\n\u001b[32m   1100\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m create_trace_span(\n\u001b[32m   1101\u001b[39m         name=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStorage.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdownload_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/consume\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1102\u001b[39m         attributes=extra_attributes,\n\u001b[32m   1103\u001b[39m         api_request=args,\n\u001b[32m   1104\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         response = \u001b[43mdownload\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconsume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m         \u001b[38;5;28mself\u001b[39m._extract_headers_from_download(response)\n\u001b[32m   1107\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\cloud\\storage\\_media\\requests\\download.py:280\u001b[39m, in \u001b[36mDownload.consume\u001b[39m\u001b[34m(self, transport, timeout)\u001b[39m\n\u001b[32m    276\u001b[39m         \u001b[38;5;28mself\u001b[39m._write_to_stream(result)\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_request_helpers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_and_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretriable_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry_strategy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\cloud\\storage\\_media\\requests\\_request_helpers.py:107\u001b[39m, in \u001b[36mwait_and_retry\u001b[39m\u001b[34m(func, retry_strategy)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retry_strategy:\n\u001b[32m    106\u001b[39m     func = retry_strategy(func)\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\cloud\\storage\\_media\\requests\\download.py:253\u001b[39m, in \u001b[36mDownload.consume.<locals>.retriable_request\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    250\u001b[39m         query_param = {\u001b[33m\"\u001b[39m\u001b[33mgeneration\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._object_generation}\n\u001b[32m    251\u001b[39m         url = _helpers.add_query_parameters(\u001b[38;5;28mself\u001b[39m.media_url, query_param)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m result = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[38;5;66;03m# If a generation hasn't been specified, and this is the first response we get, let's record the\u001b[39;00m\n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# generation. In future requests we'll specify the generation query param to avoid data races.\u001b[39;00m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._object_generation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\auth\\transport\\requests.py:540\u001b[39m, in \u001b[36mAuthorizedSession.request\u001b[39m\u001b[34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m TimeoutGuard(remaining_time) \u001b[38;5;28;01mas\u001b[39;00m guard:\n\u001b[32m    539\u001b[39m     _helpers.request_log(_LOGGER, method, url, data, headers)\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m     response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mAuthorizedSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    548\u001b[39m remaining_time = guard.remaining_timeout\n\u001b[32m    550\u001b[39m \u001b[38;5;66;03m# If the response indicated that the credentials needed to be\u001b[39;00m\n\u001b[32m    551\u001b[39m \u001b[38;5;66;03m# refreshed, then refresh the credentials and re-attempt the\u001b[39;00m\n\u001b[32m    552\u001b[39m \u001b[38;5;66;03m# request.\u001b[39;00m\n\u001b[32m    553\u001b[39m \u001b[38;5;66;03m# A stored token may expire between the time it is retrieved and\u001b[39;00m\n\u001b[32m    554\u001b[39m \u001b[38;5;66;03m# the time the request is made, so we may need to try twice.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:1378\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1377\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1378\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1379\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1380\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:318\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    320\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:279\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    281\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:706\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    708\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1311\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1307\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1308\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1309\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1310\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1167\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1168\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1169\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Optional, Dict, Any\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "class NASSDataLoader:\n",
    "    \"\"\"Secure data loader for NASS dataset with encryption support.\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose: bool = True):\n",
    "        self.verbose = verbose\n",
    "        self.data_dir = Path.home() / 'data'\n",
    "        self.data_dir.mkdir(exist_ok=True)\n",
    "        self._setup_environment()\n",
    "        \n",
    "        # Initialize encryption attributes\n",
    "        self._gcs_encrypted_url = None\n",
    "        self._gcs_password = None\n",
    "        self._census_encrypted_url = None\n",
    "        self._census_password = None\n",
    "    \n",
    "    def _setup_environment(self):\n",
    "        \"\"\"Detect environment and set appropriate settings.\"\"\"\n",
    "        self.is_colab = 'google.colab' in sys.modules\n",
    "        self.is_local = not self.is_colab\n",
    "        \n",
    "        if self.verbose:\n",
    "            env_name = \"Google Colab\" if self.is_colab else \"Local/Jupyter\"\n",
    "            print(f\"Environment: {env_name}\")\n",
    "    \n",
    "    def set_encryption_config(self, gcs_url=None, gcs_password=None, census_url=None, census_password=None):\n",
    "        \"\"\"Set encryption configuration securely.\"\"\"\n",
    "        self._gcs_encrypted_url = gcs_url\n",
    "        self._gcs_password = gcs_password\n",
    "        self._census_encrypted_url = census_url\n",
    "        self._census_password = census_password\n",
    "    \n",
    "    def _decrypt_key(self, encrypted_url: str, password: str) -> Optional[str]:\n",
    "        \"\"\"Decrypt key using pycryptodome with security measures.\"\"\"\n",
    "        decrypted_key = None\n",
    "        \n",
    "        try:\n",
    "            from Crypto.Cipher import AES\n",
    "            from Crypto.Hash import SHA256\n",
    "            import base64\n",
    "            import json\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"Downloading and decrypting access key...\")\n",
    "            \n",
    "            # Download encrypted data\n",
    "            response = requests.get(encrypted_url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            encrypted_data = base64.b64decode(response.content)\n",
    "            \n",
    "            # Derive key from password\n",
    "            key = SHA256.new(password.encode()).digest()\n",
    "            \n",
    "            # Decrypt (assuming AES-CBC with IV)\n",
    "            cipher = AES.new(key, AES.MODE_CBC, encrypted_data[:16])\n",
    "            decrypted = cipher.decrypt(encrypted_data[16:])\n",
    "            \n",
    "            # Remove PKCS7 padding\n",
    "            pad_len = decrypted[-1]\n",
    "            decrypted_key = decrypted[:-pad_len].decode()\n",
    "            \n",
    "            # Validate it's valid JSON (basic integrity check)\n",
    "            json.loads(decrypted_key)  # Will raise if invalid\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"‚úÖ Successfully decrypted access key\")\n",
    "            \n",
    "            return decrypted_key\n",
    "            \n",
    "        except ImportError:\n",
    "            self._report_error(\"pycryptodome not installed - cannot decrypt keys\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            if self.verbose:\n",
    "                print(\"‚ùå Decryption failed: Invalid password or corrupted data\")\n",
    "            return None\n",
    "            \n",
    "        finally:\n",
    "            # Security cleanup\n",
    "            try:\n",
    "                if 'password' in locals():\n",
    "                    password = 'X' * len(password)\n",
    "                if 'key' in locals():\n",
    "                    key = b'X' * len(key) \n",
    "                if 'decrypted' in locals():\n",
    "                    decrypted = b'X' * len(decrypted)\n",
    "                \n",
    "                import gc\n",
    "                gc.collect()\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    def load_data(self, source: str, **config) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Load data based on source configuration.\n",
    "        \n",
    "        Args:\n",
    "            source: One of 'github', 'local', 'drive', 'gcs'\n",
    "            **config: Source-specific configuration parameters\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame if successful, None if failed\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"Loading data from source: {source}\")\n",
    "        \n",
    "        try:\n",
    "            if source == \"github\":\n",
    "                return self._load_from_github(config.get('url'))\n",
    "            elif source == \"local\":\n",
    "                return self._load_from_local(config.get('filename'))\n",
    "            elif source == \"drive\" and self.is_colab:\n",
    "                return self._load_from_drive(config.get('path'))\n",
    "            elif source == \"gcs\":\n",
    "                return self._load_from_gcs(config)\n",
    "            else:\n",
    "                self._report_error(f\"Unsupported source '{source}' for current environment\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            self._report_error(f\"Failed to load from {source}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _load_from_github(self, url: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Load data from GitHub public URL.\"\"\"\n",
    "        if not url:\n",
    "            self._report_error(\"GitHub URL not provided in configuration\")\n",
    "            return None\n",
    "        \n",
    "        cache_file = self.data_dir / \"nass_github_cache.csv\"\n",
    "        \n",
    "        # Check cache first\n",
    "        if cache_file.exists():\n",
    "            cache_age = time.time() - cache_file.stat().st_mtime\n",
    "            if cache_age < 3600:  # 1 hour cache\n",
    "                if self.verbose:\n",
    "                    print(\"‚úÖ Using cached GitHub data\")\n",
    "                return pd.read_csv(cache_file)\n",
    "        \n",
    "        try:\n",
    "            if self.verbose:\n",
    "                print(\"Downloading from GitHub...\")\n",
    "            \n",
    "            # Stream download with progress\n",
    "            response = requests.get(url, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            with open(cache_file, 'wb') as f:\n",
    "                downloaded = 0\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        downloaded += len(chunk)\n",
    "                        if self.verbose and total_size > 0:\n",
    "                            progress = (downloaded / total_size) * 100\n",
    "                            print(f\"\\rProgress: {progress:.1f}%\", end='', flush=True)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"\\n‚úÖ Downloaded {downloaded / (1024*1024):.1f} MB\")\n",
    "            \n",
    "            return pd.read_csv(cache_file)\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            self._report_error(f\"GitHub download failed: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _load_from_local(self, filename: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Load data from local file.\"\"\"\n",
    "        if not filename:\n",
    "            self._report_error(\"Local filename not provided in configuration\")\n",
    "            return None\n",
    "        \n",
    "        # Try multiple possible locations\n",
    "        possible_paths = [\n",
    "            Path(filename),\n",
    "            self.data_dir / filename,\n",
    "            Path.cwd() / filename,\n",
    "            Path.cwd() / 'data' / filename\n",
    "        ]\n",
    "        \n",
    "        for file_path in possible_paths:\n",
    "            if file_path.exists():\n",
    "                try:\n",
    "                    if self.verbose:\n",
    "                        size_mb = file_path.stat().st_size / (1024*1024)\n",
    "                        print(f\"Loading local file: {file_path} ({size_mb:.1f} MB)\")\n",
    "                    \n",
    "                    # Use chunked reading for large files\n",
    "                    if file_path.stat().st_size > 100 * 1024 * 1024:  # 100MB\n",
    "                        return self._load_large_csv(file_path)\n",
    "                    else:\n",
    "                        return pd.read_csv(file_path)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    self._report_error(f\"Failed to read {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        self._report_error(f\"Local file '{filename}' not found in any expected location\")\n",
    "        return None\n",
    "    \n",
    "    def _load_from_drive(self, drive_path: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Load data from Google Drive (Colab only).\"\"\"\n",
    "        if not self.is_colab:\n",
    "            self._report_error(\"Google Drive access only available in Colab\")\n",
    "            return None\n",
    "        \n",
    "        if not drive_path:\n",
    "            self._report_error(\"Google Drive path not provided in configuration\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Mount Drive if not already mounted\n",
    "            if not Path('/content/drive').exists():\n",
    "                if self.verbose:\n",
    "                    print(\"Mounting Google Drive...\")\n",
    "                from google.colab import drive\n",
    "                drive.mount('/content/drive')\n",
    "            \n",
    "            file_path = Path(drive_path)\n",
    "            if not file_path.exists():\n",
    "                self._report_error(f\"File not found on Google Drive: {drive_path}\")\n",
    "                return None\n",
    "            \n",
    "            if self.verbose:\n",
    "                size_mb = file_path.stat().st_size / (1024*1024)\n",
    "                print(f\"Loading from Google Drive: {size_mb:.1f} MB\")\n",
    "            \n",
    "            return self._load_large_csv(file_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self._report_error(f\"Google Drive access failed: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _load_from_gcs(self, config: Dict[str, Any]) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Load data from Google Cloud Storage with encrypted authentication.\"\"\"\n",
    "        bucket_name = config.get('bucket')\n",
    "        blob_name = config.get('blob')\n",
    "        \n",
    "        if not bucket_name or not blob_name:\n",
    "            self._report_error(\"GCS bucket and blob names required in configuration\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            from google.cloud import storage\n",
    "            \n",
    "            # Try encrypted service account key first\n",
    "            client = self._try_encrypted_service_account()\n",
    "            \n",
    "            if not client:\n",
    "                # Fallback to other authentication methods\n",
    "                auth_methods = [\n",
    "                    self._try_default_credentials,\n",
    "                    self._try_service_account,\n",
    "                    self._try_anonymous_access\n",
    "                ]\n",
    "                \n",
    "                for auth_method in auth_methods:\n",
    "                    client = auth_method()\n",
    "                    if client:\n",
    "                        break\n",
    "            \n",
    "            if not client:\n",
    "                self._report_error(\"Failed to authenticate with Google Cloud Storage\")\n",
    "                return None\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Accessing gs://{bucket_name}/{blob_name}\")\n",
    "            \n",
    "            bucket = client.bucket(bucket_name)\n",
    "            blob = bucket.blob(blob_name)\n",
    "            \n",
    "            if not blob.exists():\n",
    "                self._report_error(f\"File not found: gs://{bucket_name}/{blob_name}\")\n",
    "                return None\n",
    "            \n",
    "            # Get file info\n",
    "            blob.reload()\n",
    "            size_gb = blob.size / (1024**3)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"File size: {size_gb:.2f} GB\")\n",
    "            \n",
    "            # Download with progress tracking\n",
    "            cache_file = self.data_dir / f\"nass_gcs_{bucket_name}_{blob_name.replace('/', '_')}\"\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"Downloading from GCS...\")\n",
    "            \n",
    "            # Use resumable download for large files\n",
    "            if blob.size > 100 * 1024 * 1024:  # 100MB\n",
    "                self._download_large_blob(blob, cache_file)\n",
    "            else:\n",
    "                blob.download_to_filename(cache_file)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"‚úÖ Download complete, loading into memory...\")\n",
    "            \n",
    "            return self._load_large_csv(cache_file)\n",
    "            \n",
    "        except ImportError:\n",
    "            self._report_error(\"google-cloud-storage package not installed\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self._report_error(f\"GCS access failed: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _try_encrypted_service_account(self):\n",
    "        \"\"\"Try encrypted service account key.\"\"\"\n",
    "        if (self._gcs_encrypted_url and self._gcs_password):\n",
    "            try:\n",
    "                from google.cloud import storage\n",
    "                import json\n",
    "                import tempfile\n",
    "                \n",
    "                # Decrypt the service account key\n",
    "                decrypted_key = self._decrypt_key(self._gcs_encrypted_url, self._gcs_password)\n",
    "                \n",
    "                if decrypted_key:\n",
    "                    # Parse as JSON and create temporary file\n",
    "                    key_data = json.loads(decrypted_key)\n",
    "                    \n",
    "                    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:\n",
    "                        json.dump(key_data, f)\n",
    "                        temp_key_file = f.name\n",
    "                    \n",
    "                    try:\n",
    "                        client = storage.Client.from_service_account_json(temp_key_file)\n",
    "                        if self.verbose:\n",
    "                            print(\"‚úÖ Authenticated using encrypted service account key\")\n",
    "                        return client\n",
    "                    finally:\n",
    "                        # Clean up temporary file\n",
    "                        try:\n",
    "                            os.unlink(temp_key_file)\n",
    "                        except:\n",
    "                            pass\n",
    "                            \n",
    "            except Exception as e:\n",
    "                if self.verbose:\n",
    "                    print(f\"Encrypted service account auth failed: {str(e)}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _try_default_credentials(self):\n",
    "        \"\"\"Try default application credentials.\"\"\"\n",
    "        try:\n",
    "            from google.cloud import storage\n",
    "            return storage.Client()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def _try_service_account(self):\n",
    "        \"\"\"Try service account key file.\"\"\"\n",
    "        try:\n",
    "            from google.cloud import storage\n",
    "            key_file = self.data_dir / 'gcs_service_account.json'\n",
    "            if key_file.exists():\n",
    "                return storage.Client.from_service_account_json(str(key_file))\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    def _try_anonymous_access(self):\n",
    "        \"\"\"Try anonymous access for public buckets.\"\"\"\n",
    "        try:\n",
    "            from google.cloud import storage\n",
    "            return storage.Client.create_anonymous_client()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def _download_large_blob(self, blob, cache_file):\n",
    "        \"\"\"Download large blob with progress tracking.\"\"\"\n",
    "        chunk_size = 1024 * 1024  # 1MB chunks\n",
    "        \n",
    "        with open(cache_file, 'wb') as f:\n",
    "            downloaded = 0\n",
    "            \n",
    "            # Download in chunks\n",
    "            for chunk_start in range(0, blob.size, chunk_size):\n",
    "                chunk_end = min(chunk_start + chunk_size - 1, blob.size - 1)\n",
    "                \n",
    "                chunk_data = blob.download_as_bytes(\n",
    "                    start=chunk_start,\n",
    "                    end=chunk_end\n",
    "                )\n",
    "                \n",
    "                f.write(chunk_data)\n",
    "                downloaded += len(chunk_data)\n",
    "                \n",
    "                if self.verbose:\n",
    "                    progress = (downloaded / blob.size) * 100\n",
    "                    print(f\"\\rProgress: {progress:.1f}%\", end='', flush=True)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print()  # New line after progress\n",
    "    \n",
    "    def _load_large_csv(self, file_path: Path) -> pd.DataFrame:\n",
    "        \"\"\"Load large CSV files efficiently with progress tracking.\"\"\"\n",
    "        file_size = file_path.stat().st_size\n",
    "        \n",
    "        if file_size < 50 * 1024 * 1024:  # Less than 50MB, load normally\n",
    "            return pd.read_csv(file_path)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Loading large file in chunks...\")\n",
    "        \n",
    "        # Read in chunks and combine\n",
    "        chunk_size = 50000  # 50k rows per chunk\n",
    "        chunks = []\n",
    "        \n",
    "        try:\n",
    "            total_chunks = sum(1 for _ in pd.read_csv(file_path, chunksize=chunk_size))\n",
    "            \n",
    "            for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):\n",
    "                chunks.append(chunk)\n",
    "                if self.verbose:\n",
    "                    progress = ((i + 1) / total_chunks) * 100\n",
    "                    print(f\"\\rLoading: {progress:.1f}%\", end='', flush=True)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print()  # New line after progress\n",
    "            \n",
    "            result = pd.concat(chunks, ignore_index=True)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"‚úÖ Loaded {len(result):,} rows\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self._report_error(f\"Failed to load large CSV: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _report_error(self, message: str):\n",
    "        \"\"\"Report error in a consistent format.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"‚ùå Error: {message}\")\n",
    "    \n",
    "    def get_cache_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get information about cached files.\"\"\"\n",
    "        cache_files = list(self.data_dir.glob(\"nass_*\"))\n",
    "        \n",
    "        info = {\n",
    "            'cache_directory': str(self.data_dir),\n",
    "            'cached_files': []\n",
    "        }\n",
    "        \n",
    "        for file_path in cache_files:\n",
    "            stat = file_path.stat()\n",
    "            info['cached_files'].append({\n",
    "                'name': file_path.name,\n",
    "                'size_mb': stat.st_size / (1024*1024),\n",
    "                'modified': time.ctime(stat.st_mtime)\n",
    "            })\n",
    "        \n",
    "        return info\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear all cached files.\"\"\"\n",
    "        cache_files = list(self.data_dir.glob(\"nass_*\"))\n",
    "        \n",
    "        for file_path in cache_files:\n",
    "            try:\n",
    "                file_path.unlink()\n",
    "                if self.verbose:\n",
    "                    print(f\"‚úÖ Deleted: {file_path.name}\")\n",
    "            except Exception as e:\n",
    "                if self.verbose:\n",
    "                    print(f\"‚ùå Failed to delete {file_path.name}: {str(e)}\")\n",
    "\n",
    "# Initialize the data loader\n",
    "data_loader = NASSDataLoader(verbose=VERBOSE_PRINTS)\n",
    "\n",
    "# Pass encrypted URLs and passwords to the data loader if available\n",
    "if 'GCS_ENCRYPTED_KEY_URL' in globals() and 'GCS_PASSWORD' in globals():\n",
    "    data_loader.set_encryption_config(\n",
    "        gcs_url=GCS_ENCRYPTED_KEY_URL,\n",
    "        gcs_password=GCS_PASSWORD\n",
    "    )\n",
    "\n",
    "if 'CENSUS_ENCRYPTED_KEY_URL' in globals() and 'CENSUS_PASSWORD' in globals():\n",
    "    data_loader.set_encryption_config(\n",
    "        census_url=CENSUS_ENCRYPTED_KEY_URL,\n",
    "        census_password=CENSUS_PASSWORD\n",
    "    )\n",
    "\n",
    "# Load data based on configuration\n",
    "config_map = {\n",
    "    \"github\": {\"url\": GITHUB_URL},\n",
    "    \"local\": {\"filename\": LOCAL_FILENAME},\n",
    "    \"drive\": {\"path\": DRIVE_PATH},\n",
    "    \"gcs\": {\n",
    "        \"bucket\": GCS_BUCKET,\n",
    "        \"blob\": GCS_BLOB\n",
    "    }\n",
    "}\n",
    "\n",
    "if DATA_SOURCE in config_map:\n",
    "    print(f\"Loading NASS data from {DATA_SOURCE}...\")\n",
    "    df = data_loader.load_data(DATA_SOURCE, **config_map[DATA_SOURCE])\n",
    "    \n",
    "    if df is not None:\n",
    "        print(f\"‚úÖ Successfully loaded: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "        print(f\"Memory usage: {df.memory_usage(deep=True).sum() / (1024*1024):.1f} MB\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to load data\")\n",
    "        print(\"\\nTroubleshooting:\")\n",
    "        if DATA_SOURCE == \"local\":\n",
    "            print(f\"  - Ensure file '{LOCAL_FILENAME}' exists in one of these locations:\")\n",
    "            print(f\"    ‚Ä¢ {Path.cwd()}\")\n",
    "            print(f\"    ‚Ä¢ {data_loader.data_dir}\")\n",
    "        elif DATA_SOURCE == \"gcs\":\n",
    "            print(f\"  - Verify bucket '{GCS_BUCKET}' and file '{GCS_BLOB}' exist\")\n",
    "            print(f\"  - Check decryption password or authentication\")\n",
    "        elif DATA_SOURCE == \"drive\":\n",
    "            print(f\"  - Ensure file exists at: {DRIVE_PATH}\")\n",
    "            print(f\"  - Google Drive must be mounted in Colab\")\n",
    "        elif DATA_SOURCE == \"github\":\n",
    "            print(f\"  - Check URL is accessible: {GITHUB_URL}\")\n",
    "            print(f\"  - Verify internet connection\")\n",
    "else:\n",
    "    print(f\"‚ùå Invalid data source: {DATA_SOURCE}\")\n",
    "    print(f\"Valid sources: {list(config_map.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxQ5fG5grPrj"
   },
   "source": [
    "Check if data has been loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqUzCuZOrPrj"
   },
   "outputs": [],
   "source": [
    "# Verify data is available before R processing\n",
    "try:\n",
    "    if 'df' not in globals():\n",
    "        print(\"‚ùå Data not loaded!\")\n",
    "        print(\"üí° Please run the 'Data Loading' section first (cell 12)\")\n",
    "        print(\"   This will create the 'df' variable needed for R analysis\")\n",
    "        raise NameError(\"df variable not found - run data loading first\")\n",
    "    \n",
    "    # Check if df exists but is None or empty\n",
    "    if df is None:\n",
    "        print(\"‚ùå Data loading failed!\")\n",
    "        print(\"üí° The data loader returned None - check the previous cell for errors\")\n",
    "        print(\"   Common issues:\")\n",
    "        print(\"   ‚Ä¢ Network connection problems\")\n",
    "        print(\"   ‚Ä¢ Invalid file path or URL\")\n",
    "        print(\"   ‚Ä¢ Authentication issues (for cloud storage)\")\n",
    "        raise ValueError(\"df is None - data loading failed\")\n",
    "    \n",
    "    # Check if df is empty\n",
    "    if hasattr(df, 'shape') and df.shape[0] == 0:\n",
    "        print(\"‚ùå Data is empty!\")\n",
    "        print(\"üí° The dataset loaded but contains no rows\")\n",
    "        raise ValueError(\"df is empty - no data to analyze\")\n",
    "    \n",
    "    # All checks passed\n",
    "    print(f\"‚úÖ Data verified: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "    print(\"‚úÖ Ready for R analysis\")\n",
    "\n",
    "except (NameError, ValueError, AttributeError) as e:\n",
    "    print(f\"‚ùå {e}\")\n",
    "    print(\"\\nüîÑ Quick fix: Run these cells in order:\")\n",
    "    print(\"   1. Configuration (cell 6)\")\n",
    "    print(\"   2. Environment Setup (cell 8)\")  \n",
    "    print(\"   3. Data Loading (cell 13)\")\n",
    "    print(\"   4. Then continue with R analysis\")\n",
    "    \n",
    "    print(\"\\nüîç Troubleshooting:\")\n",
    "    print(\"   ‚Ä¢ Check your DATA_SOURCE setting in configuration\")\n",
    "    print(\"   ‚Ä¢ Verify internet connection for GitHub/cloud sources\")\n",
    "    print(\"   ‚Ä¢ Ensure file exists for local sources\")\n",
    "    print(\"   ‚Ä¢ Check authentication for cloud storage\")\n",
    "    \n",
    "    # Don't raise the error - just warn and continue\n",
    "    print(\"\\n‚ö†Ô∏è  Continuing without data - subsequent cells may fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3j3Ms-OArPrj"
   },
   "source": [
    "## 5. Complete Data Preprocessing\n",
    "\n",
    "Streamlined preprocessing: remove variables, clean data types, and create new variables - in Python prior to passing to R for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQzhWzohrPrj"
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing in Python (before R transfer)\n",
    "print(\"Complete preprocessing: removing variables + cleaning data types...\")\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "\n",
    "# ===== 1. REMOVE UNNECESSARY VARIABLES =====\n",
    "print(\"\\n1Ô∏è‚É£ Removing unnecessary variables...\")\n",
    "\n",
    "# Smart pattern-based removal in pandas (much faster than R)\n",
    "drop_patterns = [\n",
    "    r'^CPTCCS[2-9]$',      # CPTCCS2-CPTCCS9\n",
    "    r'^CPTCCS[1-3][0-9]$', # CPTCCS10-30\n",
    "    r'^CPT[2-9]$',         # CPT2-CPT9\n",
    "    r'^CPT[1-3][0-9]$',    # CPT10-30\n",
    "    r'^DXCCSR_',           # All DXCCSR columns (500+)\n",
    "]\n",
    "\n",
    "# Find columns to drop using vectorized operations\n",
    "drop_cols = []\n",
    "for pattern in drop_patterns:\n",
    "    matches = df.columns[df.columns.str.match(pattern)].tolist()\n",
    "    drop_cols.extend(matches)\n",
    "\n",
    "# Remove duplicates\n",
    "drop_cols = list(set(drop_cols))\n",
    "\n",
    "print(f\"   Found {len(drop_cols)} columns to drop\")\n",
    "print(f\"   Patterns: CPTCCS2-30, CPT2-30, all DXCCSR_*\")\n",
    "\n",
    "# Drop the columns\n",
    "df = df.drop(columns=drop_cols)\n",
    "print(f\"   ‚úÖ Reduced from {df.shape[1] + len(drop_cols)} to {df.shape[1]} columns\")\n",
    "\n",
    "# ===== 2. CLEAN DATA TYPES FOR rpy2 =====\n",
    "print(\"\\n2Ô∏è‚É£ Cleaning data types for rpy2 compatibility...\")\n",
    "\n",
    "# Convert all object columns to strings (prevents mixed-type issues)\n",
    "object_columns = df.select_dtypes(include=['object']).columns\n",
    "if len(object_columns) > 0:\n",
    "    for col in object_columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "    print(f\"   ‚úÖ Converted {len(object_columns)} object columns to strings\")\n",
    "\n",
    "# Handle NaN/inf values consistently - FIXED FOR CATEGORICAL COLUMNS\n",
    "# First handle categorical columns separately\n",
    "categorical_columns = df.select_dtypes(include=['category']).columns\n",
    "if len(categorical_columns) > 0:\n",
    "    for col in categorical_columns:\n",
    "        # Convert categorical to string first, then handle NaN\n",
    "        df[col] = df[col].astype(str)\n",
    "    print(f\"   ‚úÖ Converted {len(categorical_columns)} categorical columns to strings\")\n",
    "\n",
    "# Now handle all non-categorical columns\n",
    "non_categorical_columns = df.select_dtypes(exclude=['category']).columns\n",
    "if len(non_categorical_columns) > 0:\n",
    "    # Replace NaN with empty strings for non-categorical columns\n",
    "    df[non_categorical_columns] = df[non_categorical_columns].fillna('')\n",
    "\n",
    "# Handle inf values in float columns\n",
    "float_cols = df.select_dtypes(include=['float64', 'float32']).columns\n",
    "if len(float_cols) > 0:\n",
    "    for col in float_cols:\n",
    "        df[col] = df[col].replace([float('inf'), float('-inf')], '')\n",
    "    print(f\"   ‚úÖ Cleaned inf values in {len(float_cols)} float columns\")\n",
    "\n",
    "print(f\"   ‚úÖ Cleaned NaN values in all columns\")\n",
    "\n",
    "# ===== 3. CREATE KEY ANALYTICAL VARIABLES IN PANDAS =====\n",
    "print(\"\\n3Ô∏è‚É£ Creating analytical variables...\")\n",
    "\n",
    "# Create WHITE indicator (1=White, 0=Non-White)\n",
    "if 'RACE' in df.columns:\n",
    "    df['WHITE'] = (df['RACE'].astype(str) == '1').astype(int)\n",
    "    print(\"   ‚úÖ Created race indicator boolean\")\n",
    "\n",
    "# Create age groups\n",
    "if 'AGE' in df.columns:\n",
    "    df['AGE'] = pd.to_numeric(df['AGE'], errors='coerce')  # Ensure numeric\n",
    "    df['AGE_GROUP'] = pd.cut(df['AGE'],\n",
    "                            bins=[0, 18, 30, 45, 65, float('inf')],\n",
    "                            labels=['0-17', '18-29', '30-44', '45-64', '65+'],\n",
    "                            right=False)\n",
    "    df['AGE_GROUP'] = df['AGE_GROUP'].astype(str)  # Convert to string for R\n",
    "    print(\"   ‚úÖ Created AGE_GROUP categories\")\n",
    "\n",
    "# Create income level labels\n",
    "if 'ZIPINC_QRTL' in df.columns:\n",
    "    income_map = {1: 'Q1-Lowest', 2: 'Q2', 3: 'Q3', 4: 'Q4-Highest'}\n",
    "    df['INCOME_LEVEL'] = df['ZIPINC_QRTL'].astype(str).map(lambda x: income_map.get(int(x) if x.isdigit() else 0, 'Unknown'))\n",
    "    print(\"   ‚úÖ Created INCOME_LEVEL labels\")\n",
    "\n",
    "# Ensure key numeric variables are properly typed\n",
    "numeric_vars = ['AGE', 'DISCWT', 'TOTCHG']\n",
    "for var in numeric_vars:\n",
    "    if var in df.columns:\n",
    "        df[var] = pd.to_numeric(df[var], errors='coerce')\n",
    "\n",
    "# save copy of cleaned data for R transfer\n",
    "cleaned_path = data_loader.data_dir / \"nass_data_cleaned.csv\"\n",
    "df.to_csv(cleaned_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ PREPROCESSING COMPLETE!\")\n",
    "print(f\"Final shape: {df.shape}\")\n",
    "print(f\"Cleaned data saved to: {cleaned_path}\")\n",
    "print(f\"Ready for R transfer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1smsFldXrPrj"
   },
   "source": [
    "## 6. Final R Transfer & Processing\n",
    "\n",
    "Transfer the clean data to R and apply any final R-specific formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10dad708"
   },
   "outputs": [],
   "source": [
    "%%R -i df -i VERBOSE_PRINTS\n",
    "\n",
    "# Convert to data.table and apply R types\n",
    "NASS <- as.data.table(df)\n",
    "\n",
    "# Factor variables\n",
    "factor_vars <- c(\"ZIPINC_QRTL\", \"PAY1\", \"CPTCCS1\", \"HOSP_LOCATION\",\n",
    "                 \"HOSP_TEACH\", \"HOSP_NASS\", \"RACE\", \"AGE_GROUP\", \"INCOME_LEVEL\")\n",
    "existing_factors <- factor_vars[factor_vars %in% names(NASS)]\n",
    "NASS[, (existing_factors) := lapply(.SD, as.factor), .SDcols = existing_factors]\n",
    "\n",
    "# Boolean variables\n",
    "if(\"FEMALE\" %in% names(NASS)) NASS[, FEMALE := as.logical(as.numeric(FEMALE))]\n",
    "if(\"WHITE\" %in% names(NASS)) NASS[, WHITE := as.logical(as.numeric(WHITE))]\n",
    "\n",
    "# Compact output\n",
    "cat(\"‚úÖ R Complete:\", nrow(NASS), \"rows,\", ncol(NASS), \"cols,\",\n",
    "    round(object.size(NASS)/1024^2, 1), \"MB\\n\")\n",
    "cat(\"Converted\", length(existing_factors), \"factors + 2 booleans\\n\")\n",
    "\n",
    "if(VERBOSE_PRINTS) {\n",
    "  cat(\"\\nColumns:\\n\")\n",
    "  print(colnames(NASS))\n",
    "}\n",
    "\n",
    "cat(\"Data in R!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Status Check/Recovery\n",
    "\n",
    "Please use this cell to reset analysis environment in case of cell crash/hang. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PRACTICAL HEALTH CHECK & RECOVERY ===\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "def quick_health_check():\n",
    "    \"\"\"Quick, accurate health check focused on actual functionality\"\"\"\n",
    "    print(\"üîç ENVIRONMENT STATUS CHECK\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # 1. Check core data\n",
    "    if 'df' in globals():\n",
    "        df_shape = globals()['df'].shape\n",
    "        print(f\"‚úÖ Data: {df_shape[0]:,} rows √ó {df_shape[1]} columns\")\n",
    "    else:\n",
    "        print(\"‚ùå Data: Not loaded\")\n",
    "        issues.append(\"data_missing\")\n",
    "    \n",
    "    # 2. Check R integration properly\n",
    "    r_working = False\n",
    "    try:\n",
    "        # Actually test R functionality\n",
    "        get_ipython().run_cell_magic('R', '', 'cat(\"R test successful\\\\n\")')\n",
    "        r_working = True\n",
    "        print(\"‚úÖ R integration: Working\")\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå R integration: Failed\")\n",
    "        issues.append(\"r_failed\")\n",
    "    \n",
    "    # 3. Check R data if R works\n",
    "    if r_working and 'df' in globals():\n",
    "        try:\n",
    "            get_ipython().run_cell_magic('R', '', '''\n",
    "            if(exists(\"NASS\")) {\n",
    "                cat(\"‚úÖ R data: Available (\", nrow(NASS), \"rows)\\\\n\")\n",
    "            } else {\n",
    "                cat(\"‚ùå R data: Missing\\\\n\")\n",
    "            }\n",
    "            ''')\n",
    "        except:\n",
    "            print(\"‚ùå R data: Transfer failed\")\n",
    "            issues.append(\"r_data_missing\")\n",
    "    \n",
    "    # 4. Check configuration\n",
    "    config_ok = all(var in globals() for var in ['DATA_SOURCE', 'VERBOSE_PRINTS'])\n",
    "    print(f\"{'‚úÖ' if config_ok else '‚ùå'} Configuration: {'Set' if config_ok else 'Missing'}\")\n",
    "    if not config_ok:\n",
    "        issues.append(\"config_missing\")\n",
    "    \n",
    "    # 5. Check memory usage\n",
    "    try:\n",
    "        import psutil\n",
    "        memory_pct = psutil.virtual_memory().percent\n",
    "        memory_ok = memory_pct < 90\n",
    "        print(f\"{'‚úÖ' if memory_ok else '‚ö†Ô∏è '} Memory: {memory_pct:.1f}% used\")\n",
    "        if not memory_ok:\n",
    "            issues.append(\"high_memory\")\n",
    "    except:\n",
    "        print(\"? Memory: Cannot check\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if not issues:\n",
    "        print(\"‚úÖ All systems operational - Ready for analysis!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  {len(issues)} issues detected\")\n",
    "        return issues\n",
    "\n",
    "def quick_recovery():\n",
    "    \"\"\"Simple recovery for common issues\"\"\"\n",
    "    print(\"üîß ATTEMPTING RECOVERY...\")\n",
    "    \n",
    "    # Restore config if missing\n",
    "    if 'DATA_SOURCE' not in globals():\n",
    "        globals()['DATA_SOURCE'] = \"github\"\n",
    "        globals()['VERBOSE_PRINTS'] = True\n",
    "        print(\"   ‚úÖ Configuration restored\")\n",
    "    \n",
    "    # Reload data if missing\n",
    "    if 'df' not in globals():\n",
    "        try:\n",
    "            # Try to reload from cache\n",
    "            from pathlib import Path\n",
    "            cache_file = Path.home() / 'data' / 'nass_data_github.csv'\n",
    "            if cache_file.exists():\n",
    "                import pandas as pd\n",
    "                globals()['df'] = pd.read_csv(cache_file)\n",
    "                print(f\"   ‚úÖ Data reloaded: {globals()['df'].shape[0]:,} rows\")\n",
    "            else:\n",
    "                print(\"   ‚ùå No cached data found - run data loading section\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Data reload failed: {e}\")\n",
    "    \n",
    "    # Restore R data if possible\n",
    "    if globals().get('R_AVAILABLE', True) and 'df' in globals():\n",
    "        try:\n",
    "            get_ipython().run_cell_magic('R', '-i df', '''\n",
    "            if(!exists(\"NASS\") && exists(\"df\")) {\n",
    "                library(data.table)\n",
    "                NASS <- as.data.table(df)\n",
    "                cat(\"   ‚úÖ R data restored\\\\n\")\n",
    "            }\n",
    "            ''')\n",
    "        except:\n",
    "            print(\"   ‚ùå R data restore failed\")\n",
    "    \n",
    "    # Clear memory\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(\"   ‚úÖ Memory cleaned\")\n",
    "\n",
    "def mini_reset():\n",
    "    \"\"\"Clear variables but keep essential functions\"\"\"\n",
    "    keep = ['quick_health_check', 'quick_recovery', 'mini_reset', \n",
    "            'DATA_SOURCE', 'VERBOSE_PRINTS', 'GITHUB_URL']\n",
    "    \n",
    "    cleared = 0\n",
    "    for var in list(globals().keys()):\n",
    "        if not var.startswith('_') and var not in keep:\n",
    "            try:\n",
    "                del globals()[var]\n",
    "                cleared += 1\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(f\"üßπ Cleared {cleared} variables\")\n",
    "    print(\"üí° Re-run setup cells to restore environment\")\n",
    "\n",
    "# Run check\n",
    "issues = quick_health_check()\n",
    "\n",
    "if issues and issues != True:\n",
    "    response = input(\"\\nAttempt recovery? (y/n): \").lower()\n",
    "    if response == 'y':\n",
    "        quick_recovery()\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(\"Re-checking after recovery...\")\n",
    "        quick_health_check()\n",
    "\n",
    "print(f\"\\nüéÆ Quick actions available:\")\n",
    "print(f\"   quick_recovery() - Fix common issues\")\n",
    "print(f\"   mini_reset() - Clear environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43230586"
   },
   "source": [
    "---\n",
    "\n",
    "## 1. Dataset Overview and Summary\n",
    "\n",
    "Generate comprehensive summary statistics and overview of the NASS dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Simple, reliable summary table\n",
    "cat(\"=== NASS 2020 DATASET SUMMARY ===\\n\")\n",
    "cat(\"Total observations:\", nrow(NASS), \"\\n\")\n",
    "cat(\"Total variables:\", ncol(NASS), \"\\n\\n\")\n",
    "\n",
    "# Key variables for summary\n",
    "summary_vars <- c(\"AGE\", \"FEMALE\", \"RACE\", \"ZIPINC_QRTL\", \"PAY1\", \n",
    "                 \"HOSP_LOCATION\", \"HOSP_TEACH\", \"HOSP_REGION\")\n",
    "\n",
    "available_vars <- summary_vars[summary_vars %in% names(NASS)]\n",
    "\n",
    "for(var in available_vars) {\n",
    "  cat(\"-----------------------------------------------\\n\")\n",
    "  cat(\"Variable:\", var, \"\\n\")\n",
    "  \n",
    "  if(is.numeric(NASS[[var]])) {\n",
    "    # Numeric variable summary\n",
    "    var_summary <- summary(NASS[[var]])\n",
    "    cat(\"  Type: Continuous\\n\")\n",
    "    cat(\"  Mean (SD):\", round(mean(NASS[[var]], na.rm = TRUE), 1), \n",
    "        \"(\", round(sd(NASS[[var]], na.rm = TRUE), 1), \")\\n\")\n",
    "    cat(\"  Median [IQR]:\", round(median(NASS[[var]], na.rm = TRUE), 1),\n",
    "        \"[\", round(quantile(NASS[[var]], 0.25, na.rm = TRUE), 1), \"-\",\n",
    "        round(quantile(NASS[[var]], 0.75, na.rm = TRUE), 1), \"]\\n\")\n",
    "    cat(\"  Range:\", round(min(NASS[[var]], na.rm = TRUE), 1), \"to\", \n",
    "        round(max(NASS[[var]], na.rm = TRUE), 1), \"\\n\")\n",
    "    cat(\"  Missing:\", sum(is.na(NASS[[var]])), \"observations\\n\")\n",
    "    \n",
    "  } else {\n",
    "    # Categorical variable summary\n",
    "    freq_table <- table(NASS[[var]], useNA = \"ifany\")\n",
    "    total_n <- sum(freq_table)\n",
    "    \n",
    "    cat(\"  Type: Categorical\\n\")\n",
    "    cat(\"  Levels:\", length(freq_table), \"\\n\")\n",
    "    \n",
    "    # Show top categories (up to 10)\n",
    "    sorted_freq <- sort(freq_table, decreasing = TRUE)\n",
    "    max_show <- min(10, length(sorted_freq))\n",
    "    \n",
    "    for(i in 1:max_show) {\n",
    "      cat(\"    \", names(sorted_freq)[i], \":\", sorted_freq[i], \n",
    "          \"(\", round(100 * sorted_freq[i] / total_n, 1), \"%)\\n\")\n",
    "    }\n",
    "    \n",
    "    if(length(sorted_freq) > max_show) {\n",
    "      cat(\"    ... and\", length(sorted_freq) - max_show, \"more categories\\n\")\n",
    "    }\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "cat(\"=== SUMMARY COMPLETE ===\\n\")\n",
    "\n",
    "cat(\"\\nSummary complete - ready for detailed analysis\\n\")\n",
    "flush.console()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Install and load required packages for comprehensive analysis\n",
    "required_packages <- c(\"ggplot2\", \"data.table\", \"scales\", \"RColorBrewer\")\n",
    "\n",
    "for(pkg in required_packages) {\n",
    "  if(!require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "    cat(\"Installing\", pkg, \"...\\n\")\n",
    "    install.packages(pkg, quiet = TRUE)\n",
    "    library(pkg, character.only = TRUE, quietly = TRUE)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Set consistent theme for all visualizations\n",
    "theme_nass <- theme_minimal() +\n",
    "  theme(\n",
    "    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n",
    "    plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray40\"),\n",
    "    axis.title = element_text(size = 11, face = \"bold\"),\n",
    "    axis.text = element_text(size = 10),\n",
    "    legend.title = element_text(size = 11, face = \"bold\"),\n",
    "    legend.text = element_text(size = 10),\n",
    "    strip.text = element_text(size = 10, face = \"bold\"),\n",
    "    panel.grid.minor = element_blank()\n",
    "  )\n",
    "\n",
    "# Define consistent color palettes\n",
    "race_colors <- RColorBrewer::brewer.pal(6, \"Set2\")\n",
    "pay_colors <- RColorBrewer::brewer.pal(6, \"Dark2\")\n",
    "region_colors <- RColorBrewer::brewer.pal(4, \"Set1\")\n",
    "\n",
    "cat(\"Setup complete - consistent theme and colors defined\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hospital Characteristics Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hospital Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Hospital distribution by region and characteristics\n",
    "if(all(c(\"HOSP_REGION\", \"HOSP_BEDSIZE_CAT\", \"HOSP_LOCATION\", \"HOSP_TEACH\") %in% names(NASS))) {\n",
    "  \n",
    "  # Get unique hospital characteristics\n",
    "  hospital_chars <- unique(NASS[, .(HOSP_NASS, HOSP_LOCATION, HOSP_TEACH, \n",
    "                                   HOSP_REGION, HOSP_BEDSIZE_CAT)])\n",
    "  \n",
    "  # Define labels\n",
    "  bed_labels <- c(\"1\" = \"Small (0-99)\", \"2\" = \"Medium (100-299)\", \"3\" = \"Large (300+)\")\n",
    "  region_labels <- c(\"1\" = \"Northeast\", \"2\" = \"Midwest\", \"3\" = \"South\", \"4\" = \"West\")\n",
    "  teach_labels <- c(\"0\" = \"Non-Teaching\", \"1\" = \"Teaching\")\n",
    "  location_labels <- c(\"0\" = \"Rural\", \"1\" = \"Urban\")\n",
    "  \n",
    "  p1 <- ggplot(hospital_chars, aes(x = factor(HOSP_REGION), fill = factor(HOSP_BEDSIZE_CAT))) + \n",
    "    geom_bar(alpha = 0.8, color = \"white\", size = 0.3) + \n",
    "    theme_nass + \n",
    "    labs(\n",
    "      x = \"US Region\", \n",
    "      y = \"Number of Hospitals\",\n",
    "      title = \"Hospital Distribution in NASS 2020 Dataset\", \n",
    "      subtitle = \"By Region, Location, Teaching Status, and Bed Size\",\n",
    "      fill = \"Bed Size Category\"\n",
    "    ) + \n",
    "    scale_fill_manual(values = region_colors, labels = bed_labels) + \n",
    "    scale_x_discrete(labels = region_labels) +\n",
    "    facet_grid(HOSP_LOCATION ~ HOSP_TEACH, \n",
    "               labeller = labeller(HOSP_TEACH = teach_labels, \n",
    "                                 HOSP_LOCATION = location_labels)) +\n",
    "    theme(legend.position = \"bottom\")\n",
    "  \n",
    "  print(p1)\n",
    "  \n",
    "  cat(\"Hospital distribution plot generated for\", nrow(hospital_chars), \"hospitals\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hospital Volume Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#  Hospital encounter volume distribution\n",
    "if(\"TOTAL_AS_ENCOUNTERS\" %in% names(NASS)) {\n",
    "  \n",
    "  hospital_volumes <- unique(NASS[, .(HOSP_NASS, HOSP_LOCATION, HOSP_TEACH, \n",
    "                                     HOSP_REGION, TOTAL_AS_ENCOUNTERS)])\n",
    "  \n",
    "  p2 <- ggplot(hospital_volumes, aes(x = factor(HOSP_REGION), y = TOTAL_AS_ENCOUNTERS)) + \n",
    "    geom_boxplot(aes(fill = factor(HOSP_REGION)), alpha = 0.7, outlier.alpha = 0.6) + \n",
    "    theme_nass + \n",
    "    labs(\n",
    "      x = \"US Region\", \n",
    "      y = \"Total Ambulatory Surgery Encounters\",\n",
    "      title = \"Hospital Ambulatory Surgery Volume Distribution\", \n",
    "      subtitle = \"By Region, Location, and Teaching Status\"\n",
    "    ) + \n",
    "    scale_fill_manual(values = region_colors, labels = region_labels, guide = \"none\") + \n",
    "    scale_x_discrete(labels = region_labels) +\n",
    "    scale_y_continuous(labels = comma_format()) +\n",
    "    facet_grid(HOSP_LOCATION ~ HOSP_TEACH, \n",
    "               labeller = labeller(HOSP_TEACH = teach_labels, \n",
    "                                 HOSP_LOCATION = location_labels))\n",
    "  \n",
    "  print(p2)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Procedures Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Procedures Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Calculate and display top 10 procedures\n",
    "if(\"CPTCCS1\" %in% names(NASS)) {\n",
    "  \n",
    "  # Calculate top procedures\n",
    "  top_procedures <- NASS[, .N, by = CPTCCS1][order(-N)][1:10]\n",
    "  TopCPT <- top_procedures$CPTCCS1\n",
    "  \n",
    "  cat(\"Top 10 procedures (CPTCCS1):\\n\")\n",
    "  print(top_procedures)\n",
    "  \n",
    "  # Calculate coverage\n",
    "  coverage <- sum(top_procedures$N) / nrow(NASS)\n",
    "  cat(\"\\nTop 10 procedures represent\", round(coverage * 100, 1), \"% of all procedures\\n\")\n",
    "  \n",
    "  # Create subset for detailed analysis\n",
    "  NASS_top_procedures <- NASS[CPTCCS1 %in% TopCPT]\n",
    "  cat(\"Created subset with\", nrow(NASS_top_procedures), \"records for top procedures\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedures by Income Quartile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Top procedures by income quartile\n",
    "if(exists(\"NASS_top_procedures\") && \"ZIPINC_QRTL\" %in% names(NASS)) {\n",
    "  \n",
    "  plot_data <- NASS_top_procedures[ZIPINC_QRTL %in% 1:4]\n",
    "  \n",
    "  if(nrow(plot_data) > 0) {\n",
    "    # Order procedures by frequency\n",
    "    proc_order <- plot_data[, .N, by = CPTCCS1][order(-N)]$CPTCCS1\n",
    "    plot_data[, CPTCCS1 := factor(CPTCCS1, levels = rev(proc_order))]\n",
    "    \n",
    "    p3 <- ggplot(plot_data, aes(x = CPTCCS1, fill = factor(ZIPINC_QRTL))) + \n",
    "      geom_bar(alpha = 0.8, color = \"white\", size = 0.2) + \n",
    "      coord_flip() +\n",
    "      theme_nass + \n",
    "      labs(\n",
    "        x = \"CCS Procedure Codes\", \n",
    "        y = \"Number of Procedures\",\n",
    "        title = \"Most Common Ambulatory Surgery Procedures\", \n",
    "        subtitle = \"Distribution by Patient Income Quartile (by ZIP code)\",\n",
    "        fill = \"Income Quartile\"\n",
    "      ) +\n",
    "      scale_fill_manual(values = pay_colors[1:4], \n",
    "                       labels = c(\"Q1 (Lowest)\", \"Q2\", \"Q3\", \"Q4 (Highest)\")) +\n",
    "      theme(legend.position = \"bottom\")\n",
    "    \n",
    "    print(p3)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedures by Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Top procedures by race\n",
    "if(exists(\"NASS_top_procedures\") && \"RACE\" %in% names(NASS)) {\n",
    "  \n",
    "  plot_data <- NASS_top_procedures[RACE %in% 1:6]\n",
    "  \n",
    "  if(nrow(plot_data) > 0) {\n",
    "    # Order procedures by frequency\n",
    "    proc_order <- plot_data[, .N, by = CPTCCS1][order(-N)]$CPTCCS1\n",
    "    plot_data[, CPTCCS1 := factor(CPTCCS1, levels = rev(proc_order))]\n",
    "    \n",
    "    race_labels <- c(\"1\" = \"White\", \"2\" = \"Black\", \"3\" = \"Hispanic\", \n",
    "                    \"4\" = \"Asian/Pacific\", \"5\" = \"Native American\", \"6\" = \"Other\")\n",
    "    \n",
    "    p4 <- ggplot(plot_data, aes(x = CPTCCS1, fill = factor(RACE))) + \n",
    "      geom_bar(alpha = 0.8, color = \"white\", size = 0.2) + \n",
    "      coord_flip() +\n",
    "      theme_nass + \n",
    "      labs(\n",
    "        x = \"CCS Procedure Codes\", \n",
    "        y = \"Number of Procedures\",\n",
    "        title = \"Most Common Ambulatory Surgery Procedures\", \n",
    "        subtitle = \"Distribution by Patient Race/Ethnicity\",\n",
    "        fill = \"Race/Ethnicity\"\n",
    "      ) + \n",
    "      scale_fill_manual(values = race_colors, labels = race_labels) +\n",
    "      theme(legend.position = \"bottom\")\n",
    "    \n",
    "    print(p4)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patient Characteristics Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age Distribution by Race and Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Age distribution by race and region - COUNT CURVES instead of density\n",
    "if(all(c(\"AGE\", \"RACE\", \"HOSP_REGION\", \"PL_NCHS\") %in% names(NASS))) {\n",
    "  \n",
    "  age_data <- NASS[RACE %in% 1:6 & PL_NCHS %in% 1:6 & AGE >= 0 & AGE <= 100]\n",
    "  \n",
    "  if(nrow(age_data) > 0) {\n",
    "    \n",
    "    race_labels <- c(\"1\" = \"White\", \"2\" = \"Black\", \"3\" = \"Hispanic\", \n",
    "                    \"4\" = \"Asian/Pacific\", \"5\" = \"Native American\", \"6\" = \"Other\")\n",
    "    region_labels <- c(\"1\" = \"Northeast\", \"2\" = \"Midwest\", \"3\" = \"South\", \"4\" = \"West\")\n",
    "    pl_nchs_labels <- c(\"1\" = \"Large Central\", \"2\" = \"Large Fringe\", \"3\" = \"Medium\", \n",
    "                       \"4\" = \"Small\", \"5\" = \"Micro\", \"6\" = \"Non-core\")\n",
    "    \n",
    "    p5 <- ggplot(age_data, aes(x = AGE)) + \n",
    "      geom_density(aes(fill = factor(RACE), y = after_stat(count)), alpha = 0.7, color = \"white\", \n",
    "                   linewidth = 0.2) + \n",
    "      theme_nass + \n",
    "      labs(\n",
    "        x = \"Age (years)\", \n",
    "        y = \"Count\",\n",
    "        title = \"Patient Age Distribution in NASS 2020\", \n",
    "        subtitle = \"By Urban-Rural Classification and US Region, Segmented by Race\",\n",
    "        fill = \"Race/Ethnicity\"\n",
    "      ) + \n",
    "      scale_fill_manual(values = race_colors, labels = race_labels) + \n",
    "      xlim(0, 100) +\n",
    "      facet_grid(HOSP_REGION ~ PL_NCHS, \n",
    "                 labeller = labeller(HOSP_REGION = region_labels, PL_NCHS = pl_nchs_labels)) + \n",
    "      theme(legend.position = \"bottom\", \n",
    "            strip.text = element_text(size = 8))\n",
    "    \n",
    "    print(p5)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age Distribution by Payer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Age distribution by payer and region \n",
    "if(all(c(\"AGE\", \"PAY1\", \"HOSP_REGION\", \"PL_NCHS\") %in% names(NASS))) {\n",
    "  \n",
    "  age_payer_data <- NASS[PAY1 %in% 1:6 & PL_NCHS %in% 1:6 & AGE >= 0 & AGE <= 100]\n",
    "  \n",
    "  if(nrow(age_payer_data) > 0) {\n",
    "    \n",
    "    pay_labels <- c(\"1\" = \"Medicare\", \"2\" = \"Medicaid\", \"3\" = \"Private\", \n",
    "                   \"4\" = \"Self-pay\", \"5\" = \"No Charge\", \"6\" = \"Other\")\n",
    "    \n",
    "    p6 <- ggplot(age_payer_data, aes(x = AGE)) + \n",
    "      geom_density(aes(fill = factor(PAY1), y = after_stat(count)), alpha = 0.7, color = \"white\", \n",
    "                   linewidth = 0.2) + \n",
    "      theme_nass + \n",
    "      labs(\n",
    "        x = \"Age (years)\", \n",
    "        y = \"Count\",\n",
    "        title = \"Patient Age Distribution in NASS 2020\", \n",
    "        subtitle = \"By Urban-Rural Classification and US Region, Segmented by Payer\",\n",
    "        fill = \"Primary Payer\"\n",
    "      ) + \n",
    "      scale_fill_manual(values = pay_colors, labels = pay_labels) + \n",
    "      xlim(0, 100) +\n",
    "      facet_grid(HOSP_REGION ~ PL_NCHS, \n",
    "                 labeller = labeller(HOSP_REGION = region_labels, PL_NCHS = pl_nchs_labels)) + \n",
    "      theme(legend.position = \"bottom\",\n",
    "            strip.text = element_text(size = 8))\n",
    "    \n",
    "    print(p6)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Payer Distribution by Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Payer distribution by race and age group\n",
    "if(all(c(\"RACE\", \"PAY1\", \"AGE\") %in% names(NASS))) {\n",
    "  \n",
    "  # Create better age groups: 0-17, 18-64, 65+\n",
    "  plot_data <- NASS[RACE %in% 1:6 & PAY1 %in% 1:6 & !is.na(AGE)]\n",
    "  \n",
    "  if(nrow(plot_data) > 0) {\n",
    "    \n",
    "    # Create simplified age groups\n",
    "    plot_data[, AGE_GROUP_SIMPLE := cut(AGE, \n",
    "                                       breaks = c(-Inf, 17, 64, Inf),\n",
    "                                       labels = c(\"0-17\", \"18-64\", \"65+\"),\n",
    "                                       right = TRUE)]\n",
    "    \n",
    "    # Remove any missing age groups\n",
    "    plot_data <- plot_data[!is.na(AGE_GROUP_SIMPLE)]\n",
    "    \n",
    "    # Define cleaner labels\n",
    "    race_labels <- c(\"1\" = \"White\", \"2\" = \"Black\", \"3\" = \"Hispanic\", \n",
    "                    \"4\" = \"Asian/Pacific\", \"5\" = \"Native American\", \"6\" = \"Other\")\n",
    "    pay_labels <- c(\"1\" = \"Medicare\", \"2\" = \"Medicaid\", \"3\" = \"Private\", \n",
    "                   \"4\" = \"Self-pay\", \"5\" = \"No Charge\", \"6\" = \"Other\")\n",
    "    \n",
    "    p7 <- ggplot(plot_data, aes(x = factor(RACE), fill = factor(PAY1))) + \n",
    "      geom_bar(position = \"fill\", alpha = 0.8, color = \"white\", linewidth = 0.2) + \n",
    "      theme_nass + \n",
    "      labs(\n",
    "        x = \"Race/Ethnicity\", \n",
    "        y = \"Proportion\",\n",
    "        title = \"Primary Payer Distribution by Demographics\", \n",
    "        subtitle = \"Insurance Coverage Patterns by Age Group and Race/Ethnicity\",\n",
    "        fill = \"Primary Payer\"\n",
    "      ) + \n",
    "      scale_fill_manual(values = pay_colors, labels = pay_labels) + \n",
    "      scale_x_discrete(labels = race_labels) +\n",
    "      scale_y_continuous(labels = percent_format()) +\n",
    "      facet_wrap(~ AGE_GROUP_SIMPLE, ncol = 3) +\n",
    "      theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 9),\n",
    "            legend.position = \"bottom\",\n",
    "            strip.text = element_text(size = 11, face = \"bold\"),\n",
    "            panel.spacing = unit(1, \"lines\"))\n",
    "    \n",
    "    print(p7)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Payer Distribution by Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Payer distribution by income quartile and age group\n",
    "if(all(c(\"ZIPINC_QRTL\", \"PAY1\", \"AGE\") %in% names(NASS))) {\n",
    "  \n",
    "  # Create data with income quartiles and simplified age groups\n",
    "  plot_data <- NASS[ZIPINC_QRTL %in% 1:4 & PAY1 %in% 1:6 & !is.na(AGE)]\n",
    "  \n",
    "  if(nrow(plot_data) > 0) {\n",
    "    \n",
    "    # Create simplified age groups: 0-17, 18-64, 65+\n",
    "    plot_data[, AGE_GROUP_SIMPLE := cut(AGE, \n",
    "                                       breaks = c(-Inf, 17, 64, Inf),\n",
    "                                       labels = c(\"0-17\", \"18-64\", \"65+\"),\n",
    "                                       right = TRUE)]\n",
    "    \n",
    "    # Remove any missing age groups\n",
    "    plot_data <- plot_data[!is.na(AGE_GROUP_SIMPLE)]\n",
    "    \n",
    "    # Define cleaner labels\n",
    "    income_labels <- c(\"1\" = \"Q1 (Lowest)\", \"2\" = \"Q2\", \"3\" = \"Q3\", \"4\" = \"Q4 (Highest)\")\n",
    "    pay_labels <- c(\"1\" = \"Medicare\", \"2\" = \"Medicaid\", \"3\" = \"Private\", \n",
    "                   \"4\" = \"Self-pay\", \"5\" = \"No Charge\", \"6\" = \"Other\")\n",
    "    \n",
    "    p8 <- ggplot(plot_data, aes(x = factor(ZIPINC_QRTL), fill = factor(PAY1))) + \n",
    "      geom_bar(position = \"fill\", alpha = 0.8, color = \"white\", linewidth = 0.2) + \n",
    "      theme_nass + \n",
    "      labs(\n",
    "        x = \"Income Quartile (by ZIP Code)\", \n",
    "        y = \"Proportion\",\n",
    "        title = \"Primary Payer Distribution by Socioeconomic Status\", \n",
    "        subtitle = \"Insurance Coverage Patterns by Age Group and Income Quartile\",\n",
    "        fill = \"Primary Payer\"\n",
    "      ) + \n",
    "      scale_fill_manual(values = pay_colors, labels = pay_labels) + \n",
    "      scale_x_discrete(labels = income_labels) +\n",
    "      scale_y_continuous(labels = percent_format()) +\n",
    "      facet_wrap(~ AGE_GROUP_SIMPLE, ncol = 3) +\n",
    "      theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 9),\n",
    "            legend.position = \"bottom\",\n",
    "            strip.text = element_text(size = 11, face = \"bold\"),\n",
    "            panel.spacing = unit(1, \"lines\"))\n",
    "    \n",
    "    print(p8)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehensive Dataset Summary\n",
    "\n",
    "Final summary statistics providing an overview of all key variables in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# ===============================================\n",
    "# COMPREHENSIVE NASS 2020 DATASET SUMMARY\n",
    "# ===============================================\n",
    "\n",
    "cat(\"================================================================\\n\")\n",
    "cat(\"                    NASS 2020 DATASET SUMMARY                  \\n\")\n",
    "cat(\"================================================================\\n\\n\")\n",
    "\n",
    "# Dataset Overview\n",
    "cat(\"DATASET OVERVIEW\\n\")\n",
    "cat(\"================\\n\")\n",
    "cat(\"Total observations:\", format(nrow(NASS), big.mark = \",\"), \"\\n\")\n",
    "cat(\"Total variables:\", ncol(NASS), \"\\n\")\n",
    "cat(\"Memory usage:\", round(object.size(NASS)/1024^2, 1), \"MB\\n\")\n",
    "cat(\"Missing data patterns:\", sum(is.na(NASS)), \"total missing values\\n\\n\")\n",
    "\n",
    "# Key variables for comprehensive summary\n",
    "summary_vars <- c(\"AGE\", \"FEMALE\", \"RACE\", \"WHITE\", \"ZIPINC_QRTL\", \"PAY1\", \n",
    "                 \"HOSP_LOCATION\", \"HOSP_TEACH\", \"HOSP_REGION\", \"HOSP_BEDSIZE_CAT\",\n",
    "                 \"PL_NCHS\", \"CPTCCS1\", \"TOTCHG\", \"DISCWT\")\n",
    "\n",
    "available_vars <- summary_vars[summary_vars %in% names(NASS)]\n",
    "missing_vars <- summary_vars[!summary_vars %in% names(NASS)]\n",
    "\n",
    "if(length(missing_vars) > 0) {\n",
    "  cat(\"WARNING: Variables not available:\", paste(missing_vars, collapse = \", \"), \"\\n\\n\")\n",
    "}\n",
    "\n",
    "# Variable-by-variable analysis\n",
    "for(var in available_vars) {\n",
    "  cat(\"----------------------------------------------------------------\\n\")\n",
    "  cat(\"Variable:\", toupper(var), \"\\n\")\n",
    "  cat(\"----------------------------------------------------------------\\n\")\n",
    "  \n",
    "  if(is.numeric(NASS[[var]])) {\n",
    "    # Enhanced numeric variable summary\n",
    "    valid_values <- NASS[[var]][!is.na(NASS[[var]])]\n",
    "    missing_count <- sum(is.na(NASS[[var]]))\n",
    "    \n",
    "    cat(\"Type: Continuous/Numeric\\n\")\n",
    "    cat(\"Valid observations:\", format(length(valid_values), big.mark = \",\"), \n",
    "        \"(\", round(100 * length(valid_values) / nrow(NASS), 1), \"%)\\n\")\n",
    "    cat(\"Missing values:\", format(missing_count, big.mark = \",\"), \n",
    "        \"(\", round(100 * missing_count / nrow(NASS), 1), \"%)\\n\")\n",
    "    \n",
    "    if(length(valid_values) > 0) {\n",
    "      # Central tendency\n",
    "      cat(\"\\nCENTRAL TENDENCY:\\n\")\n",
    "      cat(\"   Mean:\", format(round(mean(valid_values), 2), big.mark = \",\"), \"\\n\")\n",
    "      cat(\"   Median:\", format(round(median(valid_values), 2), big.mark = \",\"), \"\\n\")\n",
    "      cat(\"   Mode region:\", format(round(median(valid_values), 2), big.mark = \",\"), \n",
    "          \" +/-\", format(round(mad(valid_values), 2), big.mark = \",\"), \"\\n\")\n",
    "      \n",
    "      # Variability\n",
    "      cat(\"\\nVARIABILITY:\\n\")\n",
    "      cat(\"   Standard Dev:\", format(round(sd(valid_values), 2), big.mark = \",\"), \"\\n\")\n",
    "      cat(\"   IQR:\", format(round(quantile(valid_values, 0.25), 2), big.mark = \",\"), \"to\", \n",
    "          format(round(quantile(valid_values, 0.75), 2), big.mark = \",\"), \"\\n\")\n",
    "      cat(\"   Range:\", format(round(min(valid_values), 2), big.mark = \",\"), \"to\", \n",
    "          format(round(max(valid_values), 2), big.mark = \",\"), \"\\n\")\n",
    "      \n",
    "      # Distribution shape\n",
    "      cat(\"\\nDISTRIBUTION:\\n\")\n",
    "      q1 <- quantile(valid_values, 0.25)\n",
    "      q3 <- quantile(valid_values, 0.75)\n",
    "      skewness_approx <- (mean(valid_values) - median(valid_values)) / sd(valid_values)\n",
    "      \n",
    "      cat(\"   Skewness (approx):\", round(skewness_approx, 3), \n",
    "          ifelse(abs(skewness_approx) < 0.5, \"(approximately symmetric)\", \n",
    "                ifelse(skewness_approx > 0, \"(right-skewed)\", \"(left-skewed)\")), \"\\n\")\n",
    "      \n",
    "      # Outliers (using IQR method)\n",
    "      iqr <- q3 - q1\n",
    "      lower_fence <- q1 - 1.5 * iqr\n",
    "      upper_fence <- q3 + 1.5 * iqr\n",
    "      outliers <- sum(valid_values < lower_fence | valid_values > upper_fence)\n",
    "      cat(\"   Potential outliers:\", outliers, \n",
    "          \"(\", round(100 * outliers / length(valid_values), 1), \"%)\\n\")\n",
    "      \n",
    "      # Percentile breakdown for key variables\n",
    "      if(var %in% c(\"AGE\", \"TOTCHG\", \"DISCWT\")) {\n",
    "        cat(\"\\nPERCENTILE BREAKDOWN:\\n\")\n",
    "        percentiles <- c(0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99)\n",
    "        for(p in percentiles) {\n",
    "          cat(\"   P\", round(p*100), \":\", format(round(quantile(valid_values, p), 1), big.mark = \",\"), \"\\n\")\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \n",
    "  } else {\n",
    "    # Enhanced categorical variable summary\n",
    "    freq_table <- table(NASS[[var]], useNA = \"ifany\")\n",
    "    total_n <- sum(freq_table)\n",
    "    missing_count <- sum(is.na(NASS[[var]]))\n",
    "    \n",
    "    cat(\"Type: Categorical/Factor\\n\")\n",
    "    cat(\"Total categories:\", length(freq_table), \"\\n\")\n",
    "    cat(\"Valid observations:\", format(total_n - missing_count, big.mark = \",\"), \n",
    "        \"(\", round(100 * (total_n - missing_count) / nrow(NASS), 1), \"%)\\n\")\n",
    "    \n",
    "    if(missing_count > 0) {\n",
    "      cat(\"Missing values:\", format(missing_count, big.mark = \",\"), \n",
    "          \"(\", round(100 * missing_count / nrow(NASS), 1), \"%)\\n\")\n",
    "    }\n",
    "    \n",
    "    # Sort by frequency\n",
    "    sorted_freq <- sort(freq_table, decreasing = TRUE)\n",
    "    \n",
    "    # Show distribution\n",
    "    cat(\"\\nFREQUENCY DISTRIBUTION:\\n\")\n",
    "    max_show <- min(15, length(sorted_freq))  # Show more categories\n",
    "    \n",
    "    for(i in 1:max_show) {\n",
    "      category_name <- names(sorted_freq)[i]\n",
    "      count <- sorted_freq[i]\n",
    "      percentage <- round(100 * count / total_n, 1)\n",
    "      \n",
    "      # Create a simple bar visualization\n",
    "      bar_length <- min(20, round(20 * count / max(sorted_freq)))\n",
    "      bar <- paste(rep(\"*\", bar_length), collapse = \"\")\n",
    "      \n",
    "      cat(\"   \", sprintf(\"%-20s\", category_name), \":\", \n",
    "          sprintf(\"%8s\", format(count, big.mark = \",\")), \n",
    "          sprintf(\"(%5.1f%%)\", percentage), \" \", bar, \"\\n\")\n",
    "    }\n",
    "    \n",
    "    if(length(sorted_freq) > max_show) {\n",
    "      remaining_count <- sum(sorted_freq[(max_show+1):length(sorted_freq)])\n",
    "      remaining_pct <- round(100 * remaining_count / total_n, 1)\n",
    "      cat(\"   ... \", length(sorted_freq) - max_show, \" more categories:\", \n",
    "          format(remaining_count, big.mark = \",\"), \"(\", remaining_pct, \"%)\\n\")\n",
    "    }\n",
    "    \n",
    "    # Diversity metrics\n",
    "    cat(\"\\nDIVERSITY METRICS:\\n\")\n",
    "    # Simpson's diversity index (1 - sum of squared proportions)\n",
    "    proportions <- as.numeric(freq_table) / sum(freq_table)\n",
    "    simpson_diversity <- 1 - sum(proportions^2)\n",
    "    cat(\"   Simpson's Diversity:\", round(simpson_diversity, 3), \n",
    "        \"(0=no diversity, 1=max diversity)\\n\")\n",
    "    \n",
    "    # Effective number of categories (inverse Simpson)\n",
    "    effective_categories <- 1 / sum(proportions^2)\n",
    "    cat(\"   Effective categories:\", round(effective_categories, 1), \n",
    "        \"out of\", length(freq_table), \"total\\n\")\n",
    "    \n",
    "    # Concentration ratio (top 3 categories)\n",
    "    top3_concentration <- sum(sorted_freq[1:min(3, length(sorted_freq))]) / total_n\n",
    "    cat(\"   Top-3 concentration:\", round(100 * top3_concentration, 1), \"%\\n\")\n",
    "    \n",
    "    # Special insights for key variables\n",
    "    if(var == \"RACE\") {\n",
    "      cat(\"\\nRACE/ETHNICITY INSIGHTS:\\n\")\n",
    "      cat(\"   Racial diversity reflects ambulatory surgery access patterns\\n\")\n",
    "      if(\"1\" %in% names(freq_table)) {\n",
    "        white_vs_nonwhite <- round(freq_table[\"1\"] / sum(freq_table[names(freq_table) != \"1\"]), 2)\n",
    "        cat(\"   White vs Non-White ratio:\", white_vs_nonwhite, \":1\\n\")\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    if(var == \"PAY1\") {\n",
    "      cat(\"\\nPAYER MIX INSIGHTS:\\n\")\n",
    "      public_payers <- sum(freq_table[c(\"1\", \"2\")], na.rm = TRUE)  # Medicare + Medicaid\n",
    "      cat(\"   Public insurance coverage:\", \n",
    "          round(100 * public_payers / total_n, 1), \"%\\n\")\n",
    "    }\n",
    "    \n",
    "    if(var == \"ZIPINC_QRTL\") {\n",
    "      cat(\"\\nINCOME DISTRIBUTION INSIGHTS:\\n\")\n",
    "      low_income <- sum(freq_table[c(\"1\", \"2\")], na.rm = TRUE)  # Q1 + Q2\n",
    "      cat(\"   Lower-income representation:\", \n",
    "          round(100 * low_income / total_n, 1), \"%\\n\")\n",
    "    }\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Cross-tabulation insights\n",
    "cat(\"----------------------------------------------------------------\\n\")\n",
    "cat(\"KEY RELATIONSHIPS & CROSS-TABULATIONS\\n\")\n",
    "cat(\"----------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "# Race vs Payer cross-tab\n",
    "if(all(c(\"RACE\", \"PAY1\") %in% available_vars)) {\n",
    "  cat(\"RACE x PAYER DISTRIBUTION:\\n\")\n",
    "  cross_tab <- table(NASS$RACE, NASS$PAY1)\n",
    "  prop_tab <- round(100 * prop.table(cross_tab, 1), 1)\n",
    "  \n",
    "  race_labels <- c(\"1\"=\"White\", \"2\"=\"Black\", \"3\"=\"Hispanic\", \"4\"=\"Asian/Pacific\", \"5\"=\"Native Am\", \"6\"=\"Other\")\n",
    "  pay_labels <- c(\"1\"=\"Medicare\", \"2\"=\"Medicaid\", \"3\"=\"Private\", \"4\"=\"Self-pay\", \"5\"=\"No Charge\", \"6\"=\"Other\")\n",
    "  \n",
    "  for(race in rownames(prop_tab)) {\n",
    "    if(race %in% names(race_labels)) {\n",
    "      cat(\"   \", race_labels[race], \"patients:\\n\")\n",
    "      race_row <- prop_tab[race, ]\n",
    "      sorted_payers <- sort(race_row, decreasing = TRUE)\n",
    "      for(i in 1:min(3, length(sorted_payers))) {\n",
    "        payer <- names(sorted_payers)[i]\n",
    "        if(payer %in% names(pay_labels)) {\n",
    "          cat(\"     \", pay_labels[payer], \":\", sorted_payers[i], \"%\\n\")\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Age vs Income relationship\n",
    "if(all(c(\"AGE\", \"ZIPINC_QRTL\") %in% available_vars)) {\n",
    "  cat(\"AGE x INCOME PATTERNS:\\n\")\n",
    "  age_income <- NASS[!is.na(AGE) & !is.na(ZIPINC_QRTL), .(mean_age = round(mean(AGE), 1)), by = ZIPINC_QRTL]\n",
    "  setorder(age_income, ZIPINC_QRTL)\n",
    "  \n",
    "  income_labels <- c(\"1\"=\"Q1 (Lowest)\", \"2\"=\"Q2\", \"3\"=\"Q3\", \"4\"=\"Q4 (Highest)\")\n",
    "  for(i in 1:nrow(age_income)) {\n",
    "    quartile <- as.character(age_income$ZIPINC_QRTL[i])\n",
    "    if(quartile %in% names(income_labels)) {\n",
    "      cat(\"   \", income_labels[quartile], \"- Average age:\", age_income$mean_age[i], \"years\\n\")\n",
    "    }\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Hospital characteristics summary\n",
    "if(all(c(\"HOSP_REGION\", \"HOSP_TEACH\", \"HOSP_LOCATION\") %in% available_vars)) {\n",
    "  cat(\"HOSPITAL CHARACTERISTICS:\\n\")\n",
    "  \n",
    "  # Unique hospital count\n",
    "  if(\"HOSP_NASS\" %in% names(NASS)) {\n",
    "    unique_hospitals <- length(unique(NASS$HOSP_NASS))\n",
    "    cat(\"   Total hospitals in sample:\", unique_hospitals, \"\\n\")\n",
    "  }\n",
    "  \n",
    "  # Teaching hospital distribution\n",
    "  teaching_dist <- table(NASS$HOSP_TEACH)\n",
    "  if(\"1\" %in% names(teaching_dist)) {\n",
    "    cat(\"   Teaching hospitals:\", round(100 * teaching_dist[\"1\"] / sum(teaching_dist), 1), \"%\\n\")\n",
    "  }\n",
    "  \n",
    "  # Urban/Rural distribution\n",
    "  location_dist <- table(NASS$HOSP_LOCATION)\n",
    "  if(\"1\" %in% names(location_dist)) {\n",
    "    cat(\"   Urban hospitals:\", round(100 * location_dist[\"1\"] / sum(location_dist), 1), \"%\\n\")\n",
    "  }\n",
    "  \n",
    "  # Regional distribution\n",
    "  region_dist <- table(NASS$HOSP_REGION)\n",
    "  region_labels <- c(\"1\"=\"Northeast\", \"2\"=\"Midwest\", \"3\"=\"South\", \"4\"=\"West\")\n",
    "  cat(\"   Regional distribution:\\n\")\n",
    "  for(region in names(region_dist)) {\n",
    "    if(region %in% names(region_labels)) {\n",
    "      pct <- round(100 * region_dist[region] / sum(region_dist), 1)\n",
    "      cat(\"     \", region_labels[region], \":\", pct, \"%\\n\")\n",
    "    }\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Data quality assessment\n",
    "cat(\"----------------------------------------------------------------\\n\")\n",
    "cat(\"DATA QUALITY ASSESSMENT\\n\")\n",
    "cat(\"----------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "# Missing data patterns\n",
    "total_cells <- nrow(NASS) * ncol(NASS)\n",
    "missing_cells <- sum(is.na(NASS))\n",
    "complete_cases <- sum(complete.cases(NASS))\n",
    "\n",
    "cat(\"COMPLETENESS METRICS:\\n\")\n",
    "cat(\"   Overall completeness:\", round(100 * (1 - missing_cells/total_cells), 2), \"%\\n\")\n",
    "cat(\"   Complete cases (no missing):\", format(complete_cases, big.mark = \",\"), \n",
    "    \"(\", round(100 * complete_cases / nrow(NASS), 1), \"%)\\n\")\n",
    "cat(\"   Variables with missing data:\", sum(sapply(NASS, function(x) any(is.na(x)))), \n",
    "    \"out of\", ncol(NASS), \"\\n\\n\")\n",
    "\n",
    "# Variables with highest missing rates\n",
    "missing_rates <- sapply(NASS, function(x) round(100 * sum(is.na(x)) / length(x), 1))\n",
    "high_missing <- missing_rates[missing_rates > 0]\n",
    "if(length(high_missing) > 0) {\n",
    "  cat(\"VARIABLES WITH MISSING DATA:\\n\")\n",
    "  sorted_missing <- sort(high_missing, decreasing = TRUE)\n",
    "  for(i in 1:min(10, length(sorted_missing))) {\n",
    "    cat(\"   \", names(sorted_missing)[i], \":\", sorted_missing[i], \"% missing\\n\")\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Survey weights summary\n",
    "if(\"DISCWT\" %in% available_vars) {\n",
    "  cat(\"SURVEY WEIGHTS SUMMARY:\\n\")\n",
    "  weights <- NASS$DISCWT[!is.na(NASS$DISCWT)]\n",
    "  cat(\"   Weight range:\", round(min(weights), 2), \"to\", round(max(weights), 2), \"\\n\")\n",
    "  cat(\"   Effective sample size:\", round(sum(weights)^2 / sum(weights^2)), \"\\n\")\n",
    "  cat(\"   Design effect (approx):\", round(nrow(NASS) / (sum(weights)^2 / sum(weights^2)), 2), \"\\n\\n\")\n",
    "}\n",
    "\n",
    "cat(\"================================================================\\n\")\n",
    "cat(\"                     SUMMARY COMPLETE                          \\n\")\n",
    "cat(\"     Dataset ready for advanced statistical analysis           \\n\")\n",
    "cat(\"================================================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Census Data Comparisons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20a6a8a8"
   },
   "source": [
    "### Census API Setup\n",
    "\n",
    "Set up Census API integration and pull 2020 DHC population data for comparison with NASS sample proportions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ Please Register at the Census website for an API Key ](https://api.census.gov/data/key_signup.html). Enter your Census API key for data retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6d0c69b9"
   },
   "outputs": [],
   "source": [
    "import getpass, os, json, textwrap\n",
    "\n",
    "# Try encrypted Census API key first if available\n",
    "census_api_key = None\n",
    "\n",
    "if 'CENSUS_ENCRYPTED_KEY_URL' in globals() and 'CENSUS_PASSWORD' in globals():\n",
    "    try:\n",
    "        # Try to decrypt Census API key\n",
    "        decrypted_key = data_loader._decrypt_key(CENSUS_ENCRYPTED_KEY_URL, CENSUS_PASSWORD)\n",
    "        if decrypted_key:\n",
    "            # Parse the decrypted key (assume it's JSON with api_key field)\n",
    "            key_data = json.loads(decrypted_key)\n",
    "            census_api_key = key_data.get('api_key')\n",
    "            print(\"‚úÖ Using encrypted Census API key\")\n",
    "        else:\n",
    "            print(\"‚ùå Failed to decrypt Census API key - manual entry required\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Census key decryption error - manual entry required\")\n",
    "\n",
    "# Fallback to manual entry if encryption failed\n",
    "if not census_api_key:\n",
    "    print(\"Manual Census API key entry required:\")\n",
    "    census_api_key = getpass.getpass(\"Enter your Census API key (will not echo):\")\n",
    "\n",
    "# Set the API key\n",
    "os.environ[\"CENSUS_API_KEY\"] = census_api_key\n",
    "\n",
    "print(\"‚úÖ Census API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3c39ad38"
   },
   "source": [
    "### Census Data Retrieval and Processing\n",
    "\n",
    "Pull 2020 DHC population data by age, gender, and race for states included in NASS sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "240fd569"
   },
   "outputs": [],
   "source": [
    "%%R -i VERBOSE_PRINTS\n",
    "\n",
    "# Install and load required packages for Census analysis\n",
    "required_packages <- c(\"tidycensus\", \"dplyr\", \"tidyr\", \"survey\")\n",
    "\n",
    "for(pkg in required_packages) {\n",
    "  if(!require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "    install.packages(pkg, quiet = TRUE)\n",
    "    library(pkg, character.only = TRUE, quietly = TRUE)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Set Census API key\n",
    "census_api_key(Sys.getenv(\"CENSUS_API_KEY\"), overwrite = FALSE, install = FALSE)\n",
    "\n",
    "# Define states included in NASS 2020 dataset\n",
    "states_in_nass <- c(\"Alaska\", \"California\", \"Colorado\", \"Connecticut\", \"District of Columbia\", \n",
    "                    \"Florida\", \"Georgia\", \"Hawaii\", \"Iowa\", \"Illinois\", \"Indiana\", \"Kansas\", \n",
    "                    \"Kentucky\", \"Maryland\", \"Maine\", \"Michigan\", \"Minnesota\", \"Missouri\", \n",
    "                    \"North Carolina\", \"North Dakota\", \"Nebraska\", \"New Jersey\", \"Nevada\", \n",
    "                    \"New York\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"South Carolina\", \n",
    "                    \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Wisconsin\")\n",
    "\n",
    "cat(\"Defined\", length(states_in_nass), \"states included in NASS sample\\n\")\n",
    "\n",
    "# Function to construct population variables for Census queries\n",
    "get_population_variables <- function(base_variable) {\n",
    "    variables <- paste0(base_variable, \"_\", sprintf(\"%03dN\", 1:49))\n",
    "    \n",
    "    labels <- c(\n",
    "        \"Total\",\n",
    "        \"Male: Total\", \"Male: Under 5 years\", \"Male: 5 to 9 years\", \"Male: 10 to 14 years\",\n",
    "        \"Male: 15 to 17 years\", \"Male: 18 and 19 years\", \"Male: 20 years\", \"Male: 21 years\",\n",
    "        \"Male: 22 to 24 years\", \"Male: 25 to 29 years\", \"Male: 30 to 34 years\", \"Male: 35 to 39 years\",\n",
    "        \"Male: 40 to 44 years\", \"Male: 45 to 49 years\", \"Male: 50 to 54 years\", \"Male: 55 to 59 years\",\n",
    "        \"Male: 60 and 61 years\", \"Male: 62 to 64 years\", \"Male: 65 and 66 years\", \"Male: 67 to 69 years\",\n",
    "        \"Male: 70 to 74 years\", \"Male: 75 to 79 years\", \"Male: 80 to 84 years\", \"Male: 85 years and over\",\n",
    "        \"Female: Total\", \"Female: Under 5 years\", \"Female: 5 to 9 years\", \"Female: 10 to 14 years\",\n",
    "        \"Female: 15 to 17 years\", \"Female: 18 and 19 years\", \"Female: 20 years\", \"Female: 21 years\",\n",
    "        \"Female: 22 to 24 years\", \"Female: 25 to 29 years\", \"Female: 30 to 34 years\", \"Female: 35 to 39 years\",\n",
    "        \"Female: 40 to 44 years\", \"Female: 45 to 49 years\", \"Female: 50 to 54 years\", \"Female: 55 to 59 years\",\n",
    "        \"Female: 60 and 61 years\", \"Female: 62 to 64 years\", \"Female: 65 and 66 years\", \"Female: 67 to 69 years\",\n",
    "        \"Female: 70 to 74 years\", \"Female: 75 to 79 years\", \"Female: 80 to 84 years\", \"Female: 85 years and over\"\n",
    "    )\n",
    "    \n",
    "    names(labels) <- variables\n",
    "    return(list(variables = variables, labels = labels))\n",
    "}\n",
    "\n",
    "# Function to get population data from Census\n",
    "get_population_data <- function(variables, labels) {\n",
    "    population_data <- get_decennial(\n",
    "        geography = \"state\",\n",
    "        variables = variables,\n",
    "        year = 2020,\n",
    "        sumfile = \"dhc\"\n",
    "    )\n",
    "    \n",
    "    # Replace variable codes with descriptive labels\n",
    "    population_data <- population_data %>% \n",
    "        mutate(variable = recode(variable, !!!setNames(labels, variables)))\n",
    "    \n",
    "    # Reshape data for analysis\n",
    "    population_data <- population_data %>% \n",
    "        pivot_wider(names_from = variable, values_from = value)\n",
    "    \n",
    "    return(population_data)\n",
    "}\n",
    "\n",
    "# Get total population data (all races)\n",
    "cat(\"Retrieving total population data from 2020 Census...\\n\")\n",
    "population_info_total <- get_population_variables(\"P12\")\n",
    "total_population_by_age_gender <- get_population_data(population_info_total$variables, population_info_total$labels)\n",
    "\n",
    "# Get white alone population data\n",
    "cat(\"Retrieving white alone population data from 2020 Census...\\n\")\n",
    "population_info_white <- get_population_variables(\"P12I\")\n",
    "total_population_by_age_gender_white <- get_population_data(population_info_white$variables, population_info_white$labels)\n",
    "\n",
    "cat(\"Census data retrieval complete\\n\")\n",
    "\n",
    "if(VERBOSE_PRINTS) {\n",
    "  cat(\"\\nTotal population data structure:\\n\")\n",
    "  str(total_population_by_age_gender)\n",
    "  cat(\"\\nWhite population data structure:\\n\")\n",
    "  str(total_population_by_age_gender_white)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Comparison: NASS vs Census Proportions\n",
    "\n",
    "Compare unadjusted and weighted proportions of white individuals in NASS sample against Census benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# ========================================\n",
    "# Stage 1a: Unadjusted Proportion Analysis\n",
    "# ========================================\n",
    "\n",
    "cat(\"=== STAGE 1A: UNADJUSTED PROPORTION ANALYSIS ===\\n\")\n",
    "\n",
    "# Calculate unadjusted proportion of white individuals in NASS\n",
    "unadjusted_proportion_white <- mean(NASS$WHITE, na.rm = TRUE)\n",
    "cat(\"Unadjusted proportion of WHITE in NASS:\", round(unadjusted_proportion_white, 4), \"\\n\")\n",
    "\n",
    "# Calculate reference proportion from entire US Census\n",
    "us_census_white_proportion <- sum(total_population_by_age_gender_white$Total, na.rm = TRUE) / \n",
    "                             sum(total_population_by_age_gender$Total, na.rm = TRUE)\n",
    "cat(\"US Census White alone proportion:\", round(us_census_white_proportion, 4), \"\\n\")\n",
    "\n",
    "# Statistical test for unadjusted proportion\n",
    "unadjusted_test <- prop.test(sum(NASS$WHITE, na.rm = TRUE), \n",
    "                            sum(!is.na(NASS$WHITE)), \n",
    "                            p = us_census_white_proportion)\n",
    "cat(\"\\nUnadjusted proportion test results:\\n\")\n",
    "print(unadjusted_test)\n",
    "\n",
    "# ========================================\n",
    "# Stage 1b: Weighted Proportion Analysis\n",
    "# ========================================\n",
    "\n",
    "cat(\"\\n=== STAGE 1B: WEIGHTED PROPORTION ANALYSIS ===\\n\")\n",
    "\n",
    "# Filter Census data for NASS-included states only\n",
    "filtered_total_population <- total_population_by_age_gender %>% \n",
    "    filter(NAME %in% states_in_nass)\n",
    "\n",
    "filtered_white_population <- total_population_by_age_gender_white %>% \n",
    "    filter(NAME %in% states_in_nass)\n",
    "\n",
    "# Calculate true proportion for NASS states\n",
    "total_population_nass_states <- sum(filtered_total_population$Total, na.rm = TRUE)\n",
    "total_white_population_nass_states <- sum(filtered_white_population$Total, na.rm = TRUE)\n",
    "true_proportion_white_nass_states <- total_white_population_nass_states / total_population_nass_states\n",
    "\n",
    "cat(\"True proportion of WHITE in NASS states:\", round(true_proportion_white_nass_states, 4), \"\\n\")\n",
    "\n",
    "# Calculate weighted proportion using survey design\n",
    "survey_design <- svydesign(ids = ~1, weights = ~DISCWT, data = NASS)\n",
    "weighted_proportion_white <- svymean(~WHITE, design = survey_design)\n",
    "cat(\"Weighted proportion of WHITE in NASS:\", round(coef(weighted_proportion_white), 4), \"\\n\")\n",
    "\n",
    "# Statistical test for weighted proportion\n",
    "weighted_test <- svyttest(WHITE ~ 1, design = survey_design, mu = true_proportion_white_nass_states)\n",
    "cat(\"\\nWeighted proportion test results:\\n\")\n",
    "print(weighted_test)\n",
    "\n",
    "cat(\"\\n=== PROPORTION ANALYSIS COMPLETE ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age-Gender Stratified Analysis\n",
    "\n",
    "Detailed comparison of white proportions by age group and gender between NASS sample and Census data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# ========================================\n",
    "# Stage 2: Age-Gender Stratified Analysis\n",
    "# ========================================\n",
    "\n",
    "cat(\"=== STAGE 2: AGE-GENDER STRATIFIED ANALYSIS ===\\n\")\n",
    "\n",
    "# Define age groups matching Census categories\n",
    "age_breaks <- c(-Inf, 4, 9, 14, 17, 19, 20, 21, 24, 29, 34, 39, 44, 49, 54, 59, 61, 64, 66, 69, 74, 79, 84, Inf)\n",
    "age_labels <- c(\"Under 5 years\", \"5 to 9 years\", \"10 to 14 years\", \"15 to 17 years\", \"18 and 19 years\",\n",
    "                \"20 years\", \"21 years\", \"22 to 24 years\", \"25 to 29 years\", \"30 to 34 years\",\n",
    "                \"35 to 39 years\", \"40 to 44 years\", \"45 to 49 years\", \"50 to 54 years\", \"55 to 59 years\",\n",
    "                \"60 and 61 years\", \"62 to 64 years\", \"65 and 66 years\", \"67 to 69 years\", \"70 to 74 years\",\n",
    "                \"75 to 79 years\", \"80 to 84 years\", \"85 years and over\")\n",
    "\n",
    "# Create age group variable in NASS dataset\n",
    "NASS[, AGE_GROUP := cut(AGE, breaks = age_breaks, labels = age_labels, right = TRUE)]\n",
    "NASS[, GENDER := ifelse(FEMALE == 0, \"Male\", \"Female\")]\n",
    "\n",
    "cat(\"Created age groups and gender variables\\n\")\n",
    "\n",
    "# Calculate NASS proportions by age group and gender\n",
    "nass_proportions <- NASS[!is.na(AGE_GROUP) & !is.na(WHITE), \n",
    "                        .(total = .N,\n",
    "                          white = sum(WHITE, na.rm = TRUE),\n",
    "                          proportion_white = mean(WHITE, na.rm = TRUE)), \n",
    "                        by = .(AGE_GROUP, GENDER)]\n",
    "\n",
    "# Add confidence intervals\n",
    "nass_proportions[, ':='(\n",
    "  ci_lower = proportion_white - 1.96 * sqrt((proportion_white * (1 - proportion_white)) / total),\n",
    "  ci_upper = proportion_white + 1.96 * sqrt((proportion_white * (1 - proportion_white)) / total)\n",
    ")]\n",
    "\n",
    "cat(\"Calculated NASS proportions by age-gender groups\\n\")\n",
    "\n",
    "# Process Census data for comparison\n",
    "census_proportions <- total_population_by_age_gender_white %>% \n",
    "    select(NAME, starts_with(\"Male\"), starts_with(\"Female\")) %>% \n",
    "    pivot_longer(cols = -NAME, names_to = \"age_gender\", values_to = \"white_population\") %>% \n",
    "    separate(age_gender, into = c(\"gender\", \"age_group\"), sep = \": \") %>% \n",
    "    left_join(\n",
    "        total_population_by_age_gender %>% \n",
    "            select(NAME, starts_with(\"Male\"), starts_with(\"Female\")) %>% \n",
    "            pivot_longer(cols = -NAME, names_to = \"age_gender\", values_to = \"total_population\") %>% \n",
    "            separate(age_gender, into = c(\"gender\", \"age_group\"), sep = \": \"),\n",
    "        by = c(\"NAME\", \"gender\", \"age_group\")\n",
    "    ) %>% \n",
    "    filter(NAME %in% states_in_nass) %>%  # Filter for NASS states only\n",
    "    group_by(gender, age_group) %>% \n",
    "    summarize(\n",
    "        total_population = sum(total_population, na.rm = TRUE),\n",
    "        white_population = sum(white_population, na.rm = TRUE),\n",
    "        proportion_white = white_population / total_population,\n",
    "        .groups = 'drop'\n",
    "    ) %>% \n",
    "    filter(!is.na(age_group) & age_group != \"Total\")\n",
    "\n",
    "cat(\"Processed Census proportions by age-gender groups\\n\")\n",
    "\n",
    "# Convert to data.table for easier manipulation\n",
    "setDT(census_proportions)\n",
    "setDT(nass_proportions)\n",
    "\n",
    "# Convert age groups to factors for proper plotting\n",
    "census_proportions[, age_group := factor(age_group, levels = age_labels)]\n",
    "nass_proportions[, AGE_GROUP := factor(AGE_GROUP, levels = age_labels)]\n",
    "\n",
    "# Remove any missing age groups\n",
    "census_proportions <- census_proportions[!is.na(age_group)]\n",
    "nass_proportions <- nass_proportions[!is.na(AGE_GROUP)]\n",
    "\n",
    "cat(\"Data preparation complete\\n\")\n",
    "cat(\"NASS age-gender groups:\", nrow(nass_proportions), \"\\n\")\n",
    "cat(\"Census age-gender groups:\", nrow(census_proportions), \"\\n\")\n",
    "\n",
    "if(VERBOSE_PRINTS) {\n",
    "  cat(\"\\nSample NASS proportions:\\n\")\n",
    "  print(head(nass_proportions))\n",
    "  cat(\"\\nSample Census proportions:\\n\")\n",
    "  print(head(census_proportions))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Testing and Visualization\n",
    "\n",
    "Generate statistical tests comparing NASS and Census proportions across age-gender groups, with visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# ========================================\n",
    "# Statistical Testing by Age-Gender Groups\n",
    "# ========================================\n",
    "\n",
    "cat(\"=== STATISTICAL TESTING BY AGE-GENDER GROUPS ===\\n\")\n",
    "\n",
    "# Perform statistical tests for each age-gender combination\n",
    "test_results <- merge(nass_proportions, census_proportions, \n",
    "                     by.x = c(\"AGE_GROUP\", \"GENDER\"), \n",
    "                     by.y = c(\"age_group\", \"gender\"),\n",
    "                     all.x = TRUE)\n",
    "\n",
    "# Function to perform proportion test safely\n",
    "safe_prop_test <- function(white_count, total_count, census_prop) {\n",
    "  if(is.na(white_count) || is.na(total_count) || is.na(census_prop) || \n",
    "     total_count == 0 || census_prop == 0 || census_prop == 1) {\n",
    "    return(list(p.value = NA, significant = FALSE))\n",
    "  }\n",
    "  \n",
    "  tryCatch({\n",
    "    test_result <- prop.test(white_count, total_count, p = census_prop)\n",
    "    return(list(p.value = test_result$p.value, \n",
    "               significant = test_result$p.value < 0.05))\n",
    "  }, error = function(e) {\n",
    "    return(list(p.value = NA, significant = FALSE))\n",
    "  })\n",
    "}\n",
    "\n",
    "# Apply tests\n",
    "test_results[, c(\"p_value\", \"significant\") := {\n",
    "  test_res = safe_prop_test(white, total, proportion_white.y)\n",
    "  list(test_res$p.value, test_res$significant)\n",
    "}, by = 1:nrow(test_results)]\n",
    "\n",
    "cat(\"Statistical tests completed\\n\")\n",
    "\n",
    "# Summary of significant differences\n",
    "sig_count <- sum(test_results$significant, na.rm = TRUE)\n",
    "total_tests <- sum(!is.na(test_results$p_value))\n",
    "cat(\"Significant differences found:\", sig_count, \"out of\", total_tests, \"tests\\n\")\n",
    "\n",
    "# Display results table\n",
    "results_summary <- test_results[, .(\n",
    "  AGE_GROUP, GENDER,\n",
    "  NASS_count = total,\n",
    "  NASS_white_prop = round(proportion_white.x, 3),\n",
    "  Census_white_prop = round(proportion_white.y, 3),\n",
    "  p_value = round(p_value, 6),\n",
    "  significant = significant\n",
    ")][order(AGE_GROUP, GENDER)]\n",
    "\n",
    "cat(\"\\nDetailed results by age-gender group:\\n\")\n",
    "print(results_summary)\n",
    "\n",
    "# ========================================\n",
    "# Visualization\n",
    "# ========================================\n",
    "\n",
    "cat(\"\\n=== GENERATING VISUALIZATIONS ===\\n\")\n",
    "\n",
    "# Check if ggplot2 is available for plotting\n",
    "if(require(\"ggplot2\", quietly = TRUE)) {\n",
    "  \n",
    "  # Prepare data for plotting\n",
    "  plot_data_nass <- nass_proportions[, .(\n",
    "    AGE_GROUP, GENDER, proportion_white, ci_lower, ci_upper, source = \"NASS\"\n",
    "  )]\n",
    "  \n",
    "  plot_data_census <- census_proportions[, .(\n",
    "    AGE_GROUP = age_group, GENDER = gender, proportion_white, source = \"Census\"\n",
    "  )]\n",
    "  plot_data_census[, ':='(ci_lower = proportion_white, ci_upper = proportion_white)]\n",
    "  \n",
    "  plot_data <- rbind(plot_data_nass, plot_data_census, fill = TRUE)\n",
    "  \n",
    "  # Create the plot\n",
    "  age_gender_plot <- ggplot(plot_data, aes(x = AGE_GROUP, y = proportion_white, \n",
    "                                          color = source, group = interaction(source, GENDER))) +\n",
    "    geom_line(linewidth = 1) +\n",
    "    geom_point() +\n",
    "    geom_ribbon(data = plot_data[source == \"NASS\"], \n",
    "                aes(ymin = ci_lower, ymax = ci_upper, fill = source), \n",
    "                alpha = 0.2, color = NA) +\n",
    "    facet_wrap(~GENDER, ncol = 1) +\n",
    "    labs(title = \"White Proportion by Age Group: NASS vs Census\",\n",
    "         subtitle = \"Comparison across age groups and gender\",\n",
    "         x = \"Age Group\",\n",
    "         y = \"Proportion White\",\n",
    "         color = \"Data Source\",\n",
    "         fill = \"Confidence Interval\") +\n",
    "    theme_minimal() +\n",
    "    theme(axis.text.x = element_text(angle = 45, hjust = 1),\n",
    "          legend.position = \"bottom\") +\n",
    "    scale_y_continuous(limits = c(0, 1), labels = scales::percent)\n",
    "  \n",
    "  print(age_gender_plot)\n",
    "  cat(\"Visualization generated successfully\\n\")\n",
    "  \n",
    "} else {\n",
    "  cat(\"ggplot2 not available - using base R plotting\\n\")\n",
    "  \n",
    "  # Base R fallback plotting\n",
    "  par(mfrow = c(2, 1), mar = c(8, 4, 4, 2))\n",
    "  \n",
    "  # Males plot\n",
    "  male_nass <- nass_proportions[GENDER == \"Male\"]\n",
    "  male_census <- census_proportions[gender == \"Male\"]\n",
    "  \n",
    "  plot(1:nrow(male_nass), male_nass$proportion_white, type = \"b\", col = \"blue\",\n",
    "       xlab = \"\", ylab = \"Proportion White\", main = \"Males: NASS vs Census\",\n",
    "       ylim = c(0, 1), xaxt = \"n\")\n",
    "  lines(1:nrow(male_census), male_census$proportion_white, type = \"b\", col = \"red\")\n",
    "  axis(1, at = 1:nrow(male_nass), labels = male_nass$AGE_GROUP, las = 2, cex.axis = 0.8)\n",
    "  legend(\"topright\", legend = c(\"NASS\", \"Census\"), col = c(\"blue\", \"red\"), lty = 1)\n",
    "  \n",
    "  # Females plot\n",
    "  female_nass <- nass_proportions[GENDER == \"Female\"]\n",
    "  female_census <- census_proportions[gender == \"Female\"]\n",
    "  \n",
    "  plot(1:nrow(female_nass), female_nass$proportion_white, type = \"b\", col = \"blue\",\n",
    "       xlab = \"Age Group\", ylab = \"Proportion White\", main = \"Females: NASS vs Census\",\n",
    "       ylim = c(0, 1), xaxt = \"n\")\n",
    "  lines(1:nrow(female_census), female_census$proportion_white, type = \"b\", col = \"red\")\n",
    "  axis(1, at = 1:nrow(female_nass), labels = female_nass$AGE_GROUP, las = 2, cex.axis = 0.8)\n",
    "  legend(\"topright\", legend = c(\"NASS\", \"Census\"), col = c(\"blue\", \"red\"), lty = 1)\n",
    "  \n",
    "  par(mfrow = c(1, 1))\n",
    "  cat(\"Base R visualization generated\\n\")\n",
    "}\n",
    "\n",
    "cat(\"\\n=== CENSUS AGE-GENDER ANALYSIS COMPLETE ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-level Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Service/Diag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCS Service Agent Key Encryption\n",
    "\n",
    "Requires raw key json file downloaded from Service Agent settings in Google console. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "import getpass\n",
    "from Crypto.Cipher import AES\n",
    "from Crypto.Hash import SHA256\n",
    "from Crypto.Random import get_random_bytes\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def encrypt_and_test_key():\n",
    "    \"\"\"Encrypt a service account key and immediately test decryption\"\"\"\n",
    "    \n",
    "    print(\"üîê SERVICE ACCOUNT KEY ENCRYPTION & TEST\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Load your key file\n",
    "    key_file_path = input(\"Enter path to your service account JSON file: \").strip()\n",
    "    \n",
    "    if not os.path.exists(key_file_path):\n",
    "        print(f\"‚ùå File not found: {key_file_path}\")\n",
    "        return None, None  # Return tuple instead of just None\n",
    "    \n",
    "    try:\n",
    "        with open(key_file_path, 'r') as f:\n",
    "            key_data = json.load(f)\n",
    "        print(\"‚úÖ Key file loaded successfully\")\n",
    "        print(f\"   Project: {key_data.get('project_id')}\")\n",
    "        print(f\"   Email: {key_data.get('client_email')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load key file: {e}\")\n",
    "        return None, None  # Return tuple instead of just None\n",
    "    \n",
    "    # Step 2: Get encryption password\n",
    "    password = getpass.getpass(\"Enter encryption password: \")\n",
    "    if not password:\n",
    "        print(\"‚ùå No password provided\")\n",
    "        return None, None  # Return tuple instead of just None\n",
    "    \n",
    "    print(f\"‚úÖ Password set ({len(password)} characters)\")\n",
    "    \n",
    "    # Step 3: Encrypt the key\n",
    "    print(\"\\nüîí ENCRYPTING KEY...\")\n",
    "    \n",
    "    try:\n",
    "        # Convert to JSON string\n",
    "        key_json = json.dumps(key_data)\n",
    "        \n",
    "        # Create encryption key from password\n",
    "        encryption_key = SHA256.new(password.encode()).digest()\n",
    "        \n",
    "        # Pad data to AES block size (PKCS7 padding)\n",
    "        pad_len = 16 - (len(key_json) % 16)\n",
    "        padded_data = key_json + chr(pad_len) * pad_len\n",
    "        \n",
    "        # Generate random IV\n",
    "        iv = get_random_bytes(16)\n",
    "        \n",
    "        # Encrypt\n",
    "        cipher = AES.new(encryption_key, AES.MODE_CBC, iv)\n",
    "        encrypted_data = cipher.encrypt(padded_data.encode('utf-8'))\n",
    "        \n",
    "        # Combine IV + encrypted data and base64 encode\n",
    "        final_encrypted = base64.b64encode(iv + encrypted_data)\n",
    "        \n",
    "        print(\"‚úÖ Encryption successful\")\n",
    "        print(f\"   Original size: {len(key_json)} bytes\")\n",
    "        print(f\"   Encrypted size: {len(final_encrypted)} bytes\")\n",
    "        \n",
    "        # Save encrypted file\n",
    "        encrypted_file = key_file_path.replace('.json', '.enc')\n",
    "        with open(encrypted_file, 'wb') as f:\n",
    "            f.write(final_encrypted)\n",
    "        print(f\"‚úÖ Encrypted file saved: {encrypted_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Encryption failed: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Step 4: Immediately test decryption\n",
    "    print(\"\\nüîì TESTING DECRYPTION...\")\n",
    "    \n",
    "    try:\n",
    "        # Read the encrypted file we just created\n",
    "        with open(encrypted_file, 'rb') as f:\n",
    "            encrypted_content = f.read()\n",
    "        \n",
    "        # Decode base64\n",
    "        encrypted_data_with_iv = base64.b64decode(encrypted_content)\n",
    "        \n",
    "        # Extract IV and encrypted data\n",
    "        iv = encrypted_data_with_iv[:16]\n",
    "        encrypted_data = encrypted_data_with_iv[16:]\n",
    "        \n",
    "        # Decrypt\n",
    "        cipher = AES.new(encryption_key, AES.MODE_CBC, iv)\n",
    "        decrypted_padded = cipher.decrypt(encrypted_data)\n",
    "        \n",
    "        # Remove PKCS7 padding\n",
    "        pad_len = decrypted_padded[-1]\n",
    "        decrypted_data = decrypted_padded[:-pad_len].decode('utf-8')\n",
    "        \n",
    "        # Parse as JSON\n",
    "        decrypted_key = json.loads(decrypted_data)\n",
    "        \n",
    "        print(\"‚úÖ Decryption successful!\")\n",
    "        print(f\"   Project: {decrypted_key.get('project_id')}\")\n",
    "        print(f\"   Email: {decrypted_key.get('client_email')}\")\n",
    "        \n",
    "        # Verify it matches original\n",
    "        if decrypted_key == key_data:\n",
    "            print(\"‚úÖ Decrypted data matches original perfectly!\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Decrypted data differs from original\")\n",
    "        \n",
    "        return encrypted_file, password\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Decryption test failed: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def test_gcs_access(encrypted_file, password):\n",
    "    \"\"\"Test GCS access using the encrypted key\"\"\"\n",
    "    \n",
    "    print(\"\\n‚òÅÔ∏è  TESTING GCS ACCESS...\")\n",
    "    \n",
    "    try:\n",
    "        from google.cloud import storage\n",
    "        \n",
    "        # Decrypt the key for GCS access\n",
    "        with open(encrypted_file, 'rb') as f:\n",
    "            encrypted_content = f.read()\n",
    "        \n",
    "        # Decode and decrypt\n",
    "        encrypted_data_with_iv = base64.b64decode(encrypted_content)\n",
    "        iv = encrypted_data_with_iv[:16]\n",
    "        encrypted_data = encrypted_data_with_iv[16:]\n",
    "        \n",
    "        encryption_key = SHA256.new(password.encode()).digest()\n",
    "        cipher = AES.new(encryption_key, AES.MODE_CBC, iv)\n",
    "        decrypted_padded = cipher.decrypt(encrypted_data)\n",
    "        \n",
    "        pad_len = decrypted_padded[-1]\n",
    "        decrypted_data = decrypted_padded[:-pad_len].decode('utf-8')\n",
    "        decrypted_key = json.loads(decrypted_data)\n",
    "        \n",
    "        # Create temporary file for GCS client\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as temp_file:\n",
    "            json.dump(decrypted_key, temp_file)\n",
    "            temp_key_path = temp_file.name\n",
    "        \n",
    "        try:\n",
    "            # Test GCS access\n",
    "            client = storage.Client.from_service_account_json(temp_key_path)\n",
    "            \n",
    "            # List buckets\n",
    "            buckets = list(client.list_buckets())\n",
    "            print(f\"‚úÖ GCS authentication successful!\")\n",
    "            print(f\"   Found {len(buckets)} accessible buckets\")\n",
    "            \n",
    "            # Show bucket names\n",
    "            for bucket in buckets[:5]:  # Show first 5\n",
    "                print(f\"   - {bucket.name}\")\n",
    "            \n",
    "            # Test specific bucket access if you have one\n",
    "            bucket_name = input(\"\\nEnter bucket name to test (or press Enter to skip): \").strip()\n",
    "            if bucket_name:\n",
    "                try:\n",
    "                    bucket = client.bucket(bucket_name)\n",
    "                    if bucket.exists():\n",
    "                        print(f\"‚úÖ Bucket '{bucket_name}' exists and is accessible\")\n",
    "                        \n",
    "                        # List some files\n",
    "                        blobs = list(bucket.list_blobs(max_results=5))\n",
    "                        if blobs:\n",
    "                            print(f\"   Found {len(blobs)} files (showing first 5):\")\n",
    "                            for blob in blobs:\n",
    "                                print(f\"   - {blob.name} ({blob.size / (1024*1024):.1f} MB)\")\n",
    "                        else:\n",
    "                            print(\"   Bucket is empty\")\n",
    "                    else:\n",
    "                        print(f\"‚ùå Bucket '{bucket_name}' not found or not accessible\")\n",
    "                except Exception as bucket_error:\n",
    "                    print(f\"‚ùå Bucket access failed: {bucket_error}\")\n",
    "            \n",
    "        finally:\n",
    "            # Clean up temp file\n",
    "            os.unlink(temp_key_path)\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"‚ùå google-cloud-storage not installed\")\n",
    "        print(\"   Install with: pip install google-cloud-storage\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå GCS test failed: {e}\")\n",
    "\n",
    "def create_upload_script(encrypted_file, password):\n",
    "    \"\"\"Create a script to upload the encrypted key to GitHub releases\"\"\"\n",
    "    \n",
    "    script_content = f'''\n",
    "# Upload encrypted key to GitHub releases\n",
    "# 1. Create a new release on GitHub\n",
    "# 2. Upload the encrypted file: {encrypted_file}\n",
    "# 3. Use this URL pattern in your notebook:\n",
    "#    https://github.com/YOUR_USERNAME/YOUR_REPO/releases/download/TAG_NAME/{os.path.basename(encrypted_file)}\n",
    "\n",
    "# Test URL (replace with your actual details):\n",
    "# https://github.com/SeenaKhosravi/NASS/releases/download/v1.0.0.0/{os.path.basename(encrypted_file)}\n",
    "\n",
    "# Password for decryption: {password}\n",
    "# (Keep this secure - don't commit to git!)\n",
    "'''\n",
    "    \n",
    "    script_file = \"upload_instructions.txt\"\n",
    "    with open(script_file, 'w') as f:\n",
    "        f.write(script_content)\n",
    "    \n",
    "    print(f\"\\nüìù Upload instructions saved to: {script_file}\")\n",
    "\n",
    "# Run the complete test with better error handling\n",
    "print(\"üîê Starting GCS service account encryption test...\")\n",
    "\n",
    "# Try the encryption and test\n",
    "encrypted_file, password = encrypt_and_test_key()\n",
    "\n",
    "if encrypted_file and password:\n",
    "    print(f\"\\nüéâ SUCCESS! Your key is encrypted and tested.\")\n",
    "    print(f\"   Encrypted file: {encrypted_file}\")\n",
    "    print(f\"   Password: {password}\")\n",
    "    \n",
    "    # Test GCS access\n",
    "    test_choice = input(\"\\nTest GCS access with encrypted key? (y/n): \").lower()\n",
    "    if test_choice == 'y':\n",
    "        test_gcs_access(encrypted_file, password)\n",
    "    \n",
    "    # Create upload instructions\n",
    "    create_upload_script(encrypted_file, password)\n",
    "    \n",
    "    print(f\"\\nüìã SUMMARY:\")\n",
    "    print(f\"   ‚úÖ Original key: Loaded and validated\")\n",
    "    print(f\"   ‚úÖ Encryption: Working with AES-256-CBC\")\n",
    "    print(f\"   ‚úÖ Decryption: Verified identical to original\")\n",
    "    print(f\"   ‚úÖ File saved: {encrypted_file}\")\n",
    "    print(f\"\\nüîí Keep your password secure: {password}\")\n",
    "    print(f\"\\nüí° Next steps:\")\n",
    "    print(f\"   1. Upload {encrypted_file} to GitHub releases\")\n",
    "    print(f\"   2. Use the password in your notebook\")\n",
    "    print(f\"   3. Delete the original .json file for security\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Encryption/testing failed\")\n",
    "    print(\"üí° Check the file path and try again\")\n",
    "    print(\"   Expected format: C:\\\\path\\\\to\\\\your\\\\file.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud Storage Connection Test\n",
    "\n",
    "Requires public url encrypted key last cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standalone GCS Authentication Test via GitHub Encrypted Key\n",
    "# This cell is completely self-contained and works with a clean kernel\n",
    "\n",
    "print(\"üîë STANDALONE GCS AUTHENTICATION TEST\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Step 1: Get user inputs\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "print(\"Please provide the following information:\")\n",
    "gcs_encrypted_url = input(\"Enter GCS encrypted key URL: \").strip()\n",
    "if not gcs_encrypted_url:\n",
    "    gcs_encrypted_url = \"https://github.com/SeenaKhosravi/NASS/releases/download/v1.0.0.0/gcs_key.enc\"\n",
    "    print(f\"Using default URL: {gcs_encrypted_url}\")\n",
    "\n",
    "gcs_password = getpass.getpass(\"Enter GCS decryption password: \")\n",
    "if not gcs_password:\n",
    "    print(\"‚ùå No password provided - cannot proceed\")\n",
    "    exit()\n",
    "\n",
    "gcs_bucket = input(\"Enter target bucket name (default: nass_2020): \").strip()\n",
    "if not gcs_bucket:\n",
    "    gcs_bucket = \"nass_2020\"\n",
    "\n",
    "gcs_blob = input(\"Enter target file name (default: nass_2020_all.csv): \").strip()\n",
    "if not gcs_blob:\n",
    "    gcs_blob = \"nass_2020_all.csv\"\n",
    "\n",
    "print(f\"\\nüìã Configuration:\")\n",
    "print(f\"   URL: {gcs_encrypted_url}\")\n",
    "print(f\"   Password: {'*' * len(gcs_password)}\")\n",
    "print(f\"   Bucket: {gcs_bucket}\")\n",
    "print(f\"   File: {gcs_blob}\")\n",
    "\n",
    "# Step 2: Install required packages if needed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_if_missing(package, pip_name=None):\n",
    "    \"\"\"Install package if not available\"\"\"\n",
    "    if pip_name is None:\n",
    "        pip_name = package\n",
    "    \n",
    "    try:\n",
    "        __import__(package)\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(f\"Installing {pip_name}...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', pip_name])\n",
    "            return True\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"‚ùå Failed to install {pip_name}\")\n",
    "            return False\n",
    "\n",
    "# Install required packages\n",
    "required_packages = [\n",
    "    ('requests', 'requests'),\n",
    "    ('Crypto', 'pycryptodome'),\n",
    "    ('google.cloud.storage', 'google-cloud-storage')\n",
    "]\n",
    "\n",
    "print(f\"\\nüì¶ Checking required packages...\")\n",
    "all_packages_available = True\n",
    "for package, pip_name in required_packages:\n",
    "    if not install_if_missing(package, pip_name):\n",
    "        all_packages_available = False\n",
    "\n",
    "if not all_packages_available:\n",
    "    print(\"‚ùå Some packages failed to install - cannot proceed\")\n",
    "    exit()\n",
    "\n",
    "print(\"‚úÖ All required packages available\")\n",
    "\n",
    "# Step 3: Define decryption function\n",
    "def decrypt_key(encrypted_url, password):\n",
    "    \"\"\"Decrypt key using pycryptodome\"\"\"\n",
    "    try:\n",
    "        import requests\n",
    "        from Crypto.Cipher import AES\n",
    "        from Crypto.Hash import SHA256\n",
    "        import base64\n",
    "        import json\n",
    "        \n",
    "        print(\"üîì Downloading and decrypting access key...\")\n",
    "        \n",
    "        # Download encrypted data\n",
    "        response = requests.get(encrypted_url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        encrypted_data = base64.b64decode(response.content)\n",
    "        \n",
    "        # Derive key from password\n",
    "        key = SHA256.new(password.encode()).digest()\n",
    "        \n",
    "        # Decrypt (assuming AES-CBC with IV)\n",
    "        cipher = AES.new(key, AES.MODE_CBC, encrypted_data[:16])\n",
    "        decrypted = cipher.decrypt(encrypted_data[16:])\n",
    "        \n",
    "        # Remove PKCS7 padding\n",
    "        pad_len = decrypted[-1]\n",
    "        decrypted_key = decrypted[:-pad_len].decode()\n",
    "        \n",
    "        # Validate it's valid JSON\n",
    "        json.loads(decrypted_key)\n",
    "        \n",
    "        print(\"‚úÖ Successfully decrypted access key\")\n",
    "        return decrypted_key\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Decryption failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Step 4: Test GCS authentication\n",
    "try:\n",
    "    from google.cloud import storage\n",
    "    import json\n",
    "    import tempfile\n",
    "    \n",
    "    print(f\"\\nüîì Testing GCS authentication...\")\n",
    "    \n",
    "    # Decrypt the service account key\n",
    "    decrypted_key = decrypt_key(gcs_encrypted_url, gcs_password)\n",
    "    \n",
    "    if decrypted_key:\n",
    "        print(\"‚úÖ Successfully decrypted service account key\")\n",
    "        \n",
    "        # Parse as JSON and create temporary file\n",
    "        key_data = json.loads(decrypted_key)\n",
    "        print(f\"   Project ID: {key_data.get('project_id')}\")\n",
    "        print(f\"   Service Account: {key_data.get('client_email')}\")\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:\n",
    "            json.dump(key_data, f)\n",
    "            temp_key_file = f.name\n",
    "        \n",
    "        try:\n",
    "            # Test GCS client creation\n",
    "            print(\"\\n‚òÅÔ∏è  Creating GCS client...\")\n",
    "            client = storage.Client.from_service_account_json(temp_key_file)\n",
    "            \n",
    "            # List accessible buckets\n",
    "            print(\"üìä Listing accessible buckets...\")\n",
    "            buckets = list(client.list_buckets())\n",
    "            print(f\"‚úÖ Found {len(buckets)} accessible buckets:\")\n",
    "            \n",
    "            for bucket in buckets:\n",
    "                print(f\"   ‚Ä¢ {bucket.name}\")\n",
    "            \n",
    "            # Test specific bucket access\n",
    "            print(f\"\\nüéØ Testing access to target bucket: {gcs_bucket}\")\n",
    "            target_bucket = client.bucket(gcs_bucket)\n",
    "            \n",
    "            if target_bucket.exists():\n",
    "                print(f\"‚úÖ Bucket '{gcs_bucket}' exists and is accessible\")\n",
    "                \n",
    "                # Check for the target file\n",
    "                target_blob = target_bucket.blob(gcs_blob)\n",
    "                if target_blob.exists():\n",
    "                    target_blob.reload()\n",
    "                    size_gb = target_blob.size / (1024**3)\n",
    "                    print(f\"‚úÖ Target file '{gcs_blob}' found\")\n",
    "                    print(f\"   Size: {size_gb:.2f} GB\")\n",
    "                    print(f\"   Last modified: {target_blob.updated}\")\n",
    "                    print(f\"   Content type: {target_blob.content_type}\")\n",
    "                    print(f\"   MD5 hash: {target_blob.md5_hash}\")\n",
    "                    \n",
    "                    # Test download capability\n",
    "                    print(f\"\\nüì• Testing download capability...\")\n",
    "                    try:\n",
    "                        # Download first 1KB to test\n",
    "                        sample_data = target_blob.download_as_bytes(start=0, end=1023)\n",
    "                        print(f\"‚úÖ Download test successful ({len(sample_data)} bytes)\")\n",
    "                        \n",
    "                        # Show first few characters if it's text\n",
    "                        try:\n",
    "                            preview = sample_data.decode('utf-8')[:100]\n",
    "                            print(f\"   Preview: {preview}...\")\n",
    "                        except:\n",
    "                            print(f\"   Binary file detected\")\n",
    "                            \n",
    "                    except Exception as download_error:\n",
    "                        print(f\"‚ùå Download test failed: {download_error}\")\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"‚ùå Target file '{gcs_blob}' not found in bucket\")\n",
    "                    \n",
    "                    # List some files to see what's available\n",
    "                    print(\"üìÅ Available files in bucket (first 10):\")\n",
    "                    blobs = list(target_bucket.list_blobs(max_results=10))\n",
    "                    if blobs:\n",
    "                        for blob in blobs:\n",
    "                            print(f\"   ‚Ä¢ {blob.name} ({blob.size / (1024*1024):.1f} MB)\")\n",
    "                    else:\n",
    "                        print(\"   (bucket is empty)\")\n",
    "            else:\n",
    "                print(f\"‚ùå Bucket '{gcs_bucket}' not found or not accessible\")\n",
    "            \n",
    "            print(f\"\\nüéâ GCS AUTHENTICATION TEST COMPLETE!\")\n",
    "            print(f\"   ‚Ä¢ Package installation: ‚úÖ Success\")\n",
    "            print(f\"   ‚Ä¢ Key decryption: ‚úÖ Working\")\n",
    "            print(f\"   ‚Ä¢ GCS authentication: ‚úÖ Working\") \n",
    "            print(f\"   ‚Ä¢ Bucket access: ‚úÖ Working\")\n",
    "            print(f\"   ‚Ä¢ File detection: ‚úÖ Working\")\n",
    "            print(f\"   ‚Ä¢ Download capability: ‚úÖ Tested\")\n",
    "            print(f\"\\nüöÄ Ready for full data loading!\")\n",
    "            \n",
    "        finally:\n",
    "            # Clean up temporary file\n",
    "            try:\n",
    "                os.unlink(temp_key_file)\n",
    "                print(\"üßπ Cleaned up temporary key file\")\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        print(\"‚ùå Failed to decrypt service account key\")\n",
    "        print(\"üí° Check your password and encrypted key URL\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Missing required package after installation: {e}\")\n",
    "    print(\"üí° Try manually installing: pip install google-cloud-storage pycryptodome\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Authentication test failed: {e}\")\n",
    "    print(\"üí° Check your configuration and try again\")\n",
    "\n",
    "finally:\n",
    "    # Security cleanup\n",
    "    try:\n",
    "        gcs_password = 'X' * len(gcs_password)\n",
    "        del gcs_password\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        print(\"üîí Security cleanup completed\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Test completed! This cell is fully self-contained.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
