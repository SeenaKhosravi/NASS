{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open on GitHub](https://img.shields.io/badge/GitHub-View%20Source-181717?style=for-the-badge&logo=github)](https://github.com/SeenaKhosravi/NASS/blob/main/Analysis_NASS.ipynb)\n",
    "[![Open In Colab](https://img.shields.io/badge/Colab-Open%20Notebook-F9AB00?style=for-the-badge&logo=google-colab)](https://colab.research.google.com/github/SeenaKhosravi/NASS/blob/main/Analysis_NASS.ipynb)\n",
    "[![Launch on GCE](https://img.shields.io/badge/Google%20Cloud-Launch%20Instance-4285F4?style=for-the-badge&logo=google-cloud)](https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https://github.com/SeenaKhosravi/NASS&cloudshell_tutorial=deploy/gce-tutorial.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbe6c3e6"
   },
   "source": [
    "# Socioeconomic and Demographic Drivers of Ambulatory Surgery Usage\n",
    "### HCUP NASS 2020 â€“ Reproducible Pipeline (Python + R)\n",
    "\n",
    "**Author:** Seena Khosravi, MD  \n",
    "**LLMs Utilized:** Claude Sonnet 4, Opus 4; ChatGPT 4o, o4; Deepseek 3.1; Gemini 2.5 Pro  \n",
    "**Last Updated:** September 14, 2025  \n",
    "\n",
    "\n",
    "**Data Source:**  \n",
    "Department of Health & Human Services (HHS)  \n",
    "Agency for Healthcare Research and Quality (AHRQ)  \n",
    "Healthcare Cost and Utilization Project (HCUP)  \n",
    "National Ambulatory Surgical Sample (NASS) 2020\n",
    "\n",
    " **[HCUP NASS 2020 Introduction](https://hcup-us.ahrq.gov/db/nation/nass/NASS_Introduction_2020.jsp)** - Official AHRQ documentation\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOmuZCsusrTj"
   },
   "source": [
    "\n",
    "## Overview\n",
    "This notebook provides a reproducible analysis pipeline for examining socioeconomic and demographic factors influencing ambulatory surgery usage patterns. The analysis combines Python for data processing and R for statistical modeling.\n",
    "\n",
    "### Data Usage Agreement\n",
    "**DUA Compliant Online Implementation** â€” This notebook uses a simulated, artificial, smaller dataset with identical structure to the file created by [Raw_NASS_Processing.R](https://github.com/SeenaKhosravi/NASS/blob/a7764ce80be8a82fc449831821c27d957176c410/Raw%20NASS%20%20Processing.R). The simulated dataset production methodology is found in [Generate_Simulated_NASS.R](https://github.com/SeenaKhosravi/NASS/blob/161bf2b5c149da9654c0e887655b361fa2176db0/Generate_Simulated_NASS.R). If DUA signed and data purchased from HCUP, this notebook can run on full dataset loaded from your local or cloud storage.\n",
    "\n",
    "[Please see the DUA Agreement here.](https://hcup-us.ahrq.gov/team/NationwideDUA.jsp)\n",
    "\n",
    "### Key Features\n",
    "- **Multi-platform:** Works on Jupyter implementations via local environments, server, cloud VM instance, or platform as a service\n",
    "- **Flexible Data Storage:** GitHub (simulated, static, open access), Google Drive, Google Cloud Storage, or local file\n",
    "- **Reproducible:** All dependencies and environment setup included; Automated, Click-through VM set-up\n",
    "- **Scalable:** Handles both simulated (0.2GB, 139k rows) and full dataset (12GB, 7.8M rows) with scalable cloud options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "019b6fe9"
   },
   "source": [
    "---\n",
    "\n",
    "## Design Notes\n",
    "\n",
    "### Architecture\n",
    "- **Python primary, w/ R run via rpy2 python extension**\n",
    "- **Python cells** handle \"plumbing\" (file I/O, environment setup, rpy2 configuration, data previews)\n",
    "- **R cells** (prefixed by `%%R`) perform statistical analysis: survey weights, Census lookups, multilevel models, plots, classifiers, etc.\n",
    "\n",
    "### Data Sources\n",
    "- **Default:** Simulated dataset (0.2GB) from GitHub releases\n",
    "- **Local:** Switch to locally stored files via configuration\n",
    "- **Drive:** Google Drive (Only available in Colab)\n",
    "- **Cloud:** Google Cloud Storage support for large datasets\n",
    "\n",
    "### Environment Support\n",
    "- **Local:** JupyterLab w/ Python 3.8+ kernel (requires R 4.0+, 4.4+ preferred)\n",
    "- **Jupyter Server:** May require configuration depending on implementation (conda recommended)\n",
    "- **Google Colab:** Pro recommended for full dataset, high-RAM runtime suggested\n",
    "- **Virtual Machine Instance:** Automated GCE deployment via scripts, pre-configured R setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvQPKX8GrfZr"
   },
   "source": [
    "---\n",
    "\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUEIOQlXrPrd"
   },
   "source": [
    "\n",
    "## 1. Configuration\n",
    "\n",
    "Configure all settings here prior to run - data sources, debugging options, and file paths. Defaults to simulated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "KUU1wwMKrPre"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration loaded\n",
      "  Data source: gcs\n",
      "  Verbose mode: True\n"
     ]
    }
   ],
   "source": [
    "# ==================== CONFIGURATION ====================\n",
    "# Data Source Options\n",
    "DATA_SOURCE = \"gcs\"      # Options: \"github\", \"local\", \"gcs\", \"drive\"\n",
    "VERBOSE_PRINTS = True    # False â†’ suppress debug output\n",
    "\n",
    "# GitHub source (default - simulated data)\n",
    "GITHUB_URL = \"https://github.com/SeenaKhosravi/NASS/releases/download/v1.0.0/nass_2020_simulated.csv\"\n",
    "\n",
    "# Local file options\n",
    "LOCAL_FILENAME = \"nass_2020_local.csv\"\n",
    "\n",
    "# Google Cloud Storage options (auto-detects user's buckets)\n",
    "GCS_BUCKET = \"nass-public-data\"           # Fallback public bucket\n",
    "GCS_BLOB = \"nass_2020_simulated.csv\"     # Default file name\n",
    "AUTO_DETECT_USER_GCS = True              # Automatically find user's data\n",
    "\n",
    "# Google Drive options (for Colab)\n",
    "DRIVE_PATH = \"/content/drive/MyDrive/NASS/nass_2020_full.csv\"\n",
    "# ======================================================\n",
    "\n",
    "print(\"âœ“ Configuration loaded\")\n",
    "print(f\"  Data source: {DATA_SOURCE}\")\n",
    "print(f\"  Auto-detect user GCS: {AUTO_DETECT_USER_GCS}\")\n",
    "print(f\"  Verbose mode: {VERBOSE_PRINTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WJ8_5sRrPre"
   },
   "source": [
    "---\n",
    "## 2. Python Environment Setup \n",
    "\n",
    "Detect environment and install Python packages via Conda if available, with fallbacks. If in Google Colab, mount Google Drive.\n",
    "\n",
    "**Note:** If running in Vertex AI Workbench for the first time, you need additional R setup prior to loading rpy2. Please skip the following cell and return after completing the subsequent cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9RcJjrWrPrf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "class EnvironmentManager:\n",
    "    def __init__(self):\n",
    "        self.detect_environment()\n",
    "        self.setup_packages()\n",
    "\n",
    "    def detect_environment(self):\n",
    "        \"\"\"Detect runtime environment\"\"\"\n",
    "        self.is_colab = 'COLAB_GPU' in os.environ or 'google.colab' in sys.modules\n",
    "        self.is_vertex = 'DL_ANACONDA_HOME' in os.environ\n",
    "\n",
    "        if self.is_colab:\n",
    "            self.env_type = \"Google Colab\"\n",
    "        elif self.is_vertex:\n",
    "            self.env_type = \"Vertex AI\"\n",
    "        else:\n",
    "            self.env_type = \"Local/Jupyter\"\n",
    "\n",
    "        print(f\"Environment detected: {self.env_type}\")\n",
    "\n",
    "    def check_conda_available(self):\n",
    "        \"\"\"Check if conda is available\"\"\"\n",
    "        try:\n",
    "            subprocess.check_call(['conda', '--version'],\n",
    "                                stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "            return True\n",
    "        except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "            return False\n",
    "\n",
    "    def install_package(self, package, conda_name=None):\n",
    "        \"\"\"Smart package installation with fallback\"\"\"\n",
    "        try:\n",
    "            __import__(package)\n",
    "            return True\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "\n",
    "            # Try conda first if available and not in Colab\n",
    "            if conda_name and not self.is_colab and self.check_conda_available():\n",
    "                try:\n",
    "                    subprocess.check_call(['conda', 'install', '-y', conda_name],\n",
    "                                        stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "                    return True\n",
    "                except subprocess.CalledProcessError:\n",
    "                    print(f\"  Conda install failed for {conda_name}, trying pip...\")\n",
    "\n",
    "            # Fallback to pip\n",
    "            try:\n",
    "                subprocess.check_call([sys.executable, '-m', 'pip', 'install', package],\n",
    "                                    stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "                return True\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"  Pip install failed for {package}: {e}\")\n",
    "                return False\n",
    "\n",
    "    def setup_packages(self):\n",
    "        \"\"\"Install required packages efficiently\"\"\"\n",
    "        packages = {\n",
    "            'pandas': 'pandas',\n",
    "            'requests': 'requests',\n",
    "            'rpy2': 'rpy2',\n",
    "            'google.cloud.storage': 'google-cloud-storage'\n",
    "        }\n",
    "\n",
    "        print(\"Installing and checking packages...\")\n",
    "        failed = []\n",
    "\n",
    "        for pkg, install_name in packages.items():\n",
    "            if not self.install_package(pkg, install_name):\n",
    "                failed.append(pkg)\n",
    "\n",
    "        # Store failed packages globally for recovery\n",
    "        globals()['failed_packages'] = failed\n",
    "\n",
    "        if failed:\n",
    "            print(f\"Warning: Failed to install: {', '.join(failed)}\")\n",
    "            print(\"Some features may not work\")\n",
    "\n",
    "            # Provide specific guidance for rpy2\n",
    "            if 'rpy2' in failed:\n",
    "                print(\"\\nFor rpy2 installation issues:\")\n",
    "                if self.is_vertex:\n",
    "                    print(\"   - Vertex AI: R may not be installed by default\")\n",
    "                    print(\"   - Run the next cell for automated R setup\")\n",
    "                else:\n",
    "                    print(\"   - On Windows: May need Visual Studio Build Tools\")\n",
    "                    print(\"   - Try: conda install -c conda-forge rpy2\")\n",
    "                    print(\"   - Or: pip install rpy2 (requires R to be installed)\")\n",
    "        else:\n",
    "            print(\"All packages ready\")\n",
    "\n",
    "        # Mount Google Drive if needed (check if DATA_SOURCE exists)\n",
    "        try:\n",
    "            if globals().get('DATA_SOURCE') == \"drive\" and self.is_colab:\n",
    "                self.mount_drive()\n",
    "        except NameError:\n",
    "            pass  # DATA_SOURCE not defined yet\n",
    "\n",
    "    def mount_drive(self):\n",
    "        \"\"\"Mount Google Drive in Colab\"\"\"\n",
    "        try:\n",
    "            from google.colab import drive\n",
    "            drive.mount('/content/drive')\n",
    "            print(\"Google Drive mounted successfully\")\n",
    "        except:\n",
    "            print(\"Error: Failed to mount Google Drive\")\n",
    "\n",
    "# Initialize environment\n",
    "env_manager = EnvironmentManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mm4dTQb1rPrh"
   },
   "source": [
    "---\n",
    "## 3. R Environment Setup\n",
    "\n",
    "Load R integration and install R packages efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wpUbizDrPrh"
   },
   "outputs": [],
   "source": [
    "# Load rpy2 extension for R integration\n",
    "try:\n",
    "    %load_ext rpy2.ipython\n",
    "    print(\"R integration loaded successfully\")\n",
    "    globals()['R_AVAILABLE'] = True\n",
    "except Exception as e:\n",
    "    print(f\"Error: Failed to load R integration: {e}\")\n",
    "\n",
    "    # Windows-specific troubleshooting\n",
    "    if \"R.dll\" in str(e) or \"error 0x7e\" in str(e):\n",
    "        print(\"\\nWindows R.dll loading issue detected:\")\n",
    "        print(\"   This is a common Windows + rpy2 compatibility issue\")\n",
    "        print(\"   Solutions:\")\n",
    "        print(\"   1. Restart Python kernel and try again\")\n",
    "        print(\"   2. Check R version compatibility with rpy2\")\n",
    "        print(\"   3. Try reinstalling R and rpy2\")\n",
    "        print(\"   4. Use Python-only analysis (fallback available)\")\n",
    "        globals()['R_AVAILABLE'] = False\n",
    "    else:\n",
    "        print(\"Install rpy2: pip install rpy2\")\n",
    "        globals()['R_AVAILABLE'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5iP0V_1rPri"
   },
   "source": [
    "Install essential R packages and attempt installation of optional packages.\n",
    "\n",
    "**Note:** Other packages needed for specific analysis (advanced modeling packages) will be installed and called as needed later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGFfQQC5rPri"
   },
   "outputs": [],
   "source": [
    "%%R -i VERBOSE_PRINTS\n",
    "\n",
    "# Environment-aware R package setup\n",
    "# Standard approach, then Vertex AI fallback if needed\n",
    "\n",
    "# Detect environment\n",
    "is_vertex <- Sys.getenv(\"DL_ANACONDA_HOME\") != \"\"\n",
    "is_colab <- Sys.getenv(\"COLAB_GPU\") != \"\"\n",
    "\n",
    "if(is_colab) {\n",
    "  cat(\"Google Colab detected\\n\")\n",
    "} else if(is_vertex) {\n",
    "  cat(\"Vertex AI detected\\n\")\n",
    "} else {\n",
    "  cat(\"Local environment detected\\n\")\n",
    "}\n",
    "\n",
    "# Standard clean setup (for all environments initially)\n",
    "essential_packages <- c(\n",
    "  \"data.table\",    # Fast data manipulation\n",
    "  \"ggplot2\",       # Plotting\n",
    "  \"scales\"         # For ggplot2 percentage scales\n",
    ")\n",
    "\n",
    "optional_packages <- c(\n",
    "  \"survey\"        # Survey statistics\n",
    ")\n",
    "\n",
    "# Fast installation settings\n",
    "repos <- \"https://cloud.r-project.org\"\n",
    "options(repos = repos)\n",
    "Sys.setenv(MAKEFLAGS = paste0(\"-j\", parallel::detectCores()))\n",
    "\n",
    "# Package check and load functions\n",
    "pkg_available <- function(pkg) {\n",
    "  tryCatch({\n",
    "    find.package(pkg, quiet = TRUE)\n",
    "    TRUE\n",
    "  }, error = function(e) FALSE)\n",
    "}\n",
    "\n",
    "load_pkg <- function(pkg) {\n",
    "  tryCatch({\n",
    "    suppressMessages(library(pkg, character.only = TRUE, quietly = TRUE))\n",
    "    TRUE\n",
    "  }, error = function(e) FALSE)\n",
    "}\n",
    "\n",
    "# Install missing essential packages\n",
    "missing_essential <- essential_packages[!sapply(essential_packages, pkg_available)]\n",
    "\n",
    "if(length(missing_essential) > 0) {\n",
    "  cat(\"Installing essential packages:\", paste(missing_essential, collapse = \", \"), \"\\n\")\n",
    "\n",
    "  tryCatch({\n",
    "    install.packages(missing_essential,\n",
    "                    repos = repos,\n",
    "                    type = getOption(\"pkgType\"),\n",
    "                    dependencies = FALSE,\n",
    "                    quiet = !VERBOSE_PRINTS,\n",
    "                    Ncpus = parallel::detectCores())\n",
    "  }, error = function(e) {\n",
    "    cat(\"Binary install failed, trying source...\\n\")\n",
    "    install.packages(missing_essential,\n",
    "                    repos = repos,\n",
    "                    type = \"source\",\n",
    "                    dependencies = FALSE,\n",
    "                    quiet = !VERBOSE_PRINTS)\n",
    "  })\n",
    "}\n",
    "\n",
    "# Load essential packages\n",
    "essential_loaded <- sapply(essential_packages, load_pkg)\n",
    "essential_success <- sum(essential_loaded)\n",
    "\n",
    "cat(\"Essential packages loaded:\", essential_success, \"/\", length(essential_packages), \"\\n\")\n",
    "\n",
    "# Quick install optional packages (30s timeout)\n",
    "missing_optional <- optional_packages[!sapply(optional_packages, pkg_available)]\n",
    "\n",
    "if(length(missing_optional) > 0) {\n",
    "  cat(\"Installing optional packages...\\n\")\n",
    "\n",
    "  for(pkg in missing_optional) {\n",
    "    tryCatch({\n",
    "      setTimeLimit(cpu = 30, elapsed = 30, transient = TRUE)\n",
    "      install.packages(pkg, repos = repos,\n",
    "                      type = getOption(\"pkgType\"),\n",
    "                      dependencies = FALSE,\n",
    "                      quiet = TRUE)\n",
    "      cat(\"Installed:\", pkg, \"\\n\")\n",
    "    }, error = function(e) {\n",
    "      cat(\"Skipped (timeout):\", pkg, \"\\n\")\n",
    "    })\n",
    "\n",
    "    setTimeLimit(cpu = Inf, elapsed = Inf, transient = FALSE)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Load optional packages\n",
    "optional_loaded <- sapply(optional_packages, load_pkg)\n",
    "optional_success <- sum(optional_loaded)\n",
    "\n",
    "cat(\"Optional packages loaded:\", optional_success, \"/\", length(optional_packages), \"\\n\")\n",
    "\n",
    "# Check if we need Vertex AI aggressive installation\n",
    "has_datatable <- require(\"data.table\", quietly = TRUE)\n",
    "has_ggplot <- require(\"ggplot2\", quietly = TRUE)\n",
    "\n",
    "if(is_vertex && (!has_datatable || !has_ggplot)) {\n",
    "  cat(\"\\nStandard installation incomplete on Vertex AI - using aggressive method...\\n\")\n",
    "\n",
    "  # Check R version for compatibility\n",
    "  r_version <- R.version.string\n",
    "  r_numeric <- as.numeric(R.version$major) + as.numeric(R.version$minor)/10\n",
    "  cat(\"R version:\", r_version, \"(numeric:\", r_numeric, \")\\n\")\n",
    "\n",
    "  # System dependencies for Vertex AI\n",
    "  system_deps <- c(\n",
    "    \"apt-get update -qq\",\n",
    "    \"apt-get install -y libfontconfig1-dev libcairo2-dev\",\n",
    "    \"apt-get install -y libxml2-dev libcurl4-openssl-dev libssl-dev\",\n",
    "    \"apt-get install -y libharfbuzz-dev libfribidi-dev\",\n",
    "    \"apt-get install -y libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev\"\n",
    "  )\n",
    "\n",
    "  cat(\"Installing system dependencies...\\n\")\n",
    "  for(cmd in system_deps) {\n",
    "    system(paste(\"sudo\", cmd), ignore.stdout = TRUE, ignore.stderr = TRUE)\n",
    "  }\n",
    "\n",
    "  # Aggressive package installation for failed packages\n",
    "  failed_packages <- c()\n",
    "  if(!has_datatable) failed_packages <- c(failed_packages, \"data.table\")\n",
    "  if(!has_ggplot) failed_packages <- c(failed_packages, \"ggplot2\", \"scales\")\n",
    "\n",
    "  repos_vertex <- c(\"https://cran.rstudio.com/\", \"https://cloud.r-project.org\")\n",
    "\n",
    "  for(pkg in failed_packages) {\n",
    "    cat(\"Aggressively installing\", pkg, \"...\")\n",
    "    installed <- FALSE\n",
    "\n",
    "    # For ggplot2/scales, try version-specific installation if R < 4.1\n",
    "    if((pkg == \"ggplot2\" || pkg == \"scales\") && r_numeric < 4.1) {\n",
    "      cat(\"(R < 4.1 detected - trying compatible versions)...\")\n",
    "\n",
    "      # First try to install remotes if not available\n",
    "      if(!require(\"remotes\", quietly = TRUE)) {\n",
    "        tryCatch({\n",
    "          install.packages(\"remotes\", repos = repos_vertex[1], quiet = TRUE)\n",
    "        }, error = function(e) NULL)\n",
    "      }\n",
    "\n",
    "      # Try installing older compatible versions\n",
    "      if(pkg == \"ggplot2\" && require(\"remotes\", quietly = TRUE)) {\n",
    "        # Try ggplot2 versions compatible with older R\n",
    "        old_versions <- c(\"3.4.4\", \"3.4.3\", \"3.4.2\", \"3.4.0\", \"3.3.6\")\n",
    "        for(ver in old_versions) {\n",
    "          tryCatch({\n",
    "            remotes::install_version(\"ggplot2\", version = ver, repos = repos_vertex[1], quiet = TRUE)\n",
    "            if(require(\"ggplot2\", quietly = TRUE)) {\n",
    "              cat(\" Success (v\", ver, \")\\n\")\n",
    "              installed <- TRUE\n",
    "              break\n",
    "            }\n",
    "          }, error = function(e) NULL)\n",
    "        }\n",
    "      } else if(pkg == \"scales\" && require(\"remotes\", quietly = TRUE)) {\n",
    "        # Try scales versions compatible with older R\n",
    "        old_versions <- c(\"1.3.0\", \"1.2.1\", \"1.2.0\", \"1.1.1\")\n",
    "        for(ver in old_versions) {\n",
    "          tryCatch({\n",
    "            remotes::install_version(\"scales\", version = ver, repos = repos_vertex[1], quiet = TRUE)\n",
    "            if(require(\"scales\", quietly = TRUE)) {\n",
    "              cat(\" Success (v\", ver, \")\\n\")\n",
    "              installed <- TRUE\n",
    "              break\n",
    "            }\n",
    "          }, error = function(e) NULL)\n",
    "        }\n",
    "      }\n",
    "\n",
    "      # Fallback: try archived CRAN packages if remotes failed\n",
    "      if(!installed && pkg == \"ggplot2\") {\n",
    "        cat(\"(trying archived versions)...\")\n",
    "        archived_urls <- c(\n",
    "          \"https://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_3.4.4.tar.gz\",\n",
    "          \"https://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_3.4.0.tar.gz\",\n",
    "          \"https://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_3.3.6.tar.gz\"\n",
    "        )\n",
    "        for(url in archived_urls) {\n",
    "          tryCatch({\n",
    "            install.packages(url, repos = NULL, type = \"source\", quiet = TRUE)\n",
    "            if(require(\"ggplot2\", quietly = TRUE)) {\n",
    "              cat(\" Success (archived)\\n\")\n",
    "              installed <- TRUE\n",
    "              break\n",
    "            }\n",
    "          }, error = function(e) NULL)\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "    # Standard installation if version-specific didn't work\n",
    "    if(!installed) {\n",
    "      for(repo in repos_vertex) {\n",
    "        tryCatch({\n",
    "          install.packages(pkg, repos = repo, dependencies = TRUE, quiet = TRUE)\n",
    "          if(require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "            cat(\" Success\\n\")\n",
    "            installed <- TRUE\n",
    "            break\n",
    "          }\n",
    "        }, error = function(e) NULL)\n",
    "      }\n",
    "    }\n",
    "\n",
    "    if(!installed) cat(\" FAILED\\n\")\n",
    "  }\n",
    "\n",
    "  # Re-check after aggressive installation\n",
    "  has_datatable <- require(\"data.table\", quietly = TRUE)\n",
    "  has_ggplot <- require(\"ggplot2\", quietly = TRUE)\n",
    "\n",
    "  cat(\"After aggressive installation: data.table =\", has_datatable, \"| ggplot2 =\", has_ggplot, \"\\n\")\n",
    "}\n",
    "\n",
    "# Final status check (universal)\n",
    "if(has_datatable && has_ggplot) {\n",
    "  cat(\"Core environment ready! (data.table + ggplot2)\\n\")\n",
    "  setDTthreads(0)  # Use all cores\n",
    "\n",
    "} else if(has_datatable) {\n",
    "  cat(\"Warning: Partial setup - data.table ready, plotting may be limited\\n\")\n",
    "  setDTthreads(0)\n",
    "\n",
    "} else {\n",
    "  cat(\"Error: Critical failure - data.table not available\\n\")\n",
    "  stop(\"Cannot proceed without data.table\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAtjPad_rPri"
   },
   "source": [
    "Verify R setup is complete and ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMh-C5gvrPrj"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Quick verification and setup\n",
    "cat(\"Verifying R environment...\\n\")\n",
    "\n",
    "# Test core functionality\n",
    "tryCatch({\n",
    "  # Test data.table (essential)\n",
    "  dt_test <- data.table(x = 1:3, y = letters[1:3])\n",
    "  cat(\"data.table ready\\n\")\n",
    "\n",
    "  # Test ggplot2 (optional)\n",
    "  if(require(\"ggplot2\", quietly = TRUE)) {\n",
    "    cat(\"ggplot2 ready\\n\")\n",
    "  } else {\n",
    "    cat(\"Warning: ggplot2 not available (plots disabled)\\n\")\n",
    "  }\n",
    "\n",
    "  # Set up data.table options for performance\n",
    "  setDTthreads(0)  # Use all cores\n",
    "\n",
    "  cat(\"R environment optimized and ready!\\n\")\n",
    "\n",
    "}, error = function(e) {\n",
    "  cat(\"Error: R environment verification failed:\", e$message, \"\\n\")\n",
    "  stop(\"R setup incomplete\")\n",
    "})\n",
    "\n",
    "# Clean up test objects\n",
    "rm(list = ls()[!ls() %in% c(\"VERBOSE_PRINTS\")])\n",
    "invisible(gc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8pUXVbsrPrg"
   },
   "source": [
    "---\n",
    "## 4. Data Loading\n",
    "\n",
    "Config based data loader with error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-KUtlK-rPrh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ ULTRA-FAST DATA LOADING INITIATED\n",
      "============================================================\n",
      "ðŸš€ Initializing UltraFast DataLoader...\n",
      "âœ… UltraFast DataLoader ready in 0.01s\n",
      "   Environment: Local/Jupyter\n",
      "   Base path: C:\\Users\\laure\n",
      "   Available files: 1\n",
      "   Optimization: Medium (4-10GB RAM)\n",
      "\n",
      "ðŸŽ¯ ULTRA-FAST LOADING: GCS\n",
      "âš¡ Optimization level: Medium (4-10GB RAM)\n",
      "â˜ï¸ Ultra-connecting to GCS...\n",
      "âŒ Ultra-GCS failed: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n",
      "ðŸ”„ Primary source failed - trying ultra fallback...\n",
      "ðŸ§  Ultra-smart fallback strategy...\n",
      "   ðŸŽ¯ Ultra-trying: nass_data.csv\n",
      "      Strategy: In-memory\n",
      "ðŸ“– Ultra-fast loading: nass_data.csv\n",
      "   File size: 213.4 MB\n",
      "   Estimated memory: 0.9 GB\n",
      "   Available memory: 4.7 GB\n",
      "   Strategy: Optimized C engine\n",
      "âœ… C engine loaded in 10.11s\n",
      "   ðŸŽ¯ Optimizing DataFrame...\n",
      "   ðŸ’¾ Memory optimized: 882.9MB â†’ 130.9MB (85.2% reduction)\n",
      "âœ… Ultra-fallback successful using cached data\n",
      "\n",
      "ðŸŽ‰ ULTRA-FAST LOADING COMPLETE!\n",
      "   ðŸ“Š Shape: 139,233 rows Ã— 675 columns\n",
      "   ðŸ’¾ Memory: 130.9 MB\n",
      "   âš¡ Load speed: 5,510 rows/sec\n",
      "   â±ï¸  Total time: 25.27s (Setup: 0.01s)\n",
      "\n",
      "ðŸ“‹ ULTRA-PREVIEW:\n",
      "   Shape: (139233, 675)\n",
      "   Dtypes: {dtype('int8'): 570, dtype('int16'): 32, dtype('float32'): 30, dtype('int32'): 3, CategoricalDtype(categories=['0', '1'], ordered=False, categories_dtype=object): 2, CategoricalDtype(categories=['-8', '-9', '1', '2', '3', '4', '5', '6'], ordered=False, categories_dtype=object): 2, CategoricalDtype(categories=['10001', '10003', '10004', '10005', '10006', '10007',\n",
      "                  '10008', '10009', '10010', '10011',\n",
      "                  ...\n",
      "                  '40518', '40519', '40520', '40521', '40522', '40523',\n",
      "                  '40524', '40525', '40526', '40527'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['1', '2', '3', '4'], ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['1', '2', '3'], ordered=False, categories_dtype=object): 1, dtype('O'): 1, Int16Dtype(): 1, Int8Dtype(): 1, CategoricalDtype(categories=['-99', '1', '2', '3', '4', '5', '6'], ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['-8', '-9', '1', '2', '3', '4'], ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['10', '101', '104', '106', '109', '112', '113', '114', '118',\n",
      "                  '119', '12', '120', '121', '122', '124', '125', '129', '13',\n",
      "                  '132', '14', '142', '143', '144', '145', '146', '147', '148',\n",
      "                  '149', '15', '150', '151', '152', '153', '154', '157', '158',\n",
      "                  '16', '160', '161', '162', '164', '166', '167', '17', '170',\n",
      "                  '171', '175', '19', '20', '21', '22', '225', '23', '24',\n",
      "                  '244', '26', '28', '3', '30', '33', '42', '43', '48', '49',\n",
      "                  '57', '6', '61', '67', '78', '80', '81', '84', '85', '86',\n",
      "                  '87', '9', '94', '96', '99'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['0184T', '0191T', '0238T', '0274T', '0335T', '0408T',\n",
      "                  '0449T', '0505T', '0549T', '11451',\n",
      "                  ...\n",
      "                  '92992', '93580', '93581', '93582', '93590', '93591',\n",
      "                  '93650', '93653', '93654', '93656'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['0184T', '0191T', '0238T', '0308T', '0335T', '0449T',\n",
      "                  '11451', '11463', '11471', '11604',\n",
      "                  ...\n",
      "                  '69806', '69910', '69930', '92986', '93580', '93582',\n",
      "                  '93650', '93653', '93654', '93656'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['0191T', '11604', '11606', '11626', '11644', '11771',\n",
      "                  '11970', '11971', '13101', '13121',\n",
      "                  ...\n",
      "                  '69644', '69667', '69710', '69801', '69805', '69930',\n",
      "                  '93650', '93653', '93654', '93656'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['0191T', '11604', '11606', '11624', '11626', '11771',\n",
      "                  '11970', '11971', '13100', '13101',\n",
      "                  ...\n",
      "                  '68720', '69145', '69436', '69610', '69631', '69644',\n",
      "                  '69930', '93650', '93653', '93654'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['A4102', 'A419', 'A429', 'A5216', 'A58', 'A630', 'B0052',\n",
      "                  'B0232', 'B070', 'B079',\n",
      "                  ...\n",
      "                  'Z9842', 'Z9849', 'Z9851', 'Z9852', 'Z9882', 'Z9883',\n",
      "                  'Z9884', 'Z98890', 'Z992', 'invl'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['A0472', 'A071', 'A09', 'A311', 'A312', 'A409', 'A4102',\n",
      "                  'A419', 'A422', 'A4289',\n",
      "                  ...\n",
      "                  'Z9883', 'Z9884', 'Z9886', 'Z98890', 'Z98891', 'Z992',\n",
      "                  'Z993', 'Z9981', 'Z9989', 'invl'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['A0472', 'A071', 'A310', 'A389', 'A419', 'A4289', 'A429',\n",
      "                  'A4901', 'A4902', 'A493',\n",
      "                  ...\n",
      "                  'Z9883', 'Z9884', 'Z98890', 'Z98891', 'Z992', 'Z993',\n",
      "                  'Z9981', 'Z9989', 'incn', 'invl'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['A048', 'A159', 'A419', 'A4289', 'A429', 'A46', 'A480',\n",
      "                  'A4902', 'A5216', 'A549',\n",
      "                  ...\n",
      "                  'Z9883', 'Z9884', 'Z9886', 'Z98890', 'Z98891', 'Z992',\n",
      "                  'Z993', 'Z9981', 'Z9989', 'invl'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['A4289', 'A429', 'A4902', 'A6000', 'A630', 'A6920', 'A809',\n",
      "                  'B001', 'B009', 'B0230',\n",
      "                  ...\n",
      "                  'Z9883', 'Z9884', 'Z98890', 'Z98891', 'Z9911', 'Z992',\n",
      "                  'Z993', 'Z9981', 'Z9989', 'invl'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['A0472', 'A048', 'A240', 'A419', 'A429', 'A5216', 'A539',\n",
      "                  'A6000', 'A6004', 'A630',\n",
      "                  ...\n",
      "                  'Z9882', 'Z9884', 'Z98890', 'Z98891', 'Z9911', 'Z992',\n",
      "                  'Z993', 'Z9981', 'Z9989', 'invl'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['A0472', 'A480', 'A4902', 'A549', 'A6000', 'A630', 'A6920',\n",
      "                  'B001', 'B009', 'B0229',\n",
      "                  ...\n",
      "                  'Z9882', 'Z9884', 'Z98890', 'Z98891', 'Z9911', 'Z992',\n",
      "                  'Z993', 'Z9981', 'Z9989', 'invl'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['A048', 'A084', 'A150', 'A419', 'A4902', 'A5002', 'A6000',\n",
      "                  'A630', 'A64', 'A6920',\n",
      "                  ...\n",
      "                  'Z9882', 'Z9883', 'Z9884', 'Z98890', 'Z98891', 'Z9911',\n",
      "                  'Z992', 'Z993', 'Z9981', 'Z9989'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['A419', 'A4902', 'A6000', 'A630', 'A6920', 'B001', 'B004',\n",
      "                  'B009', 'B029', 'B159',\n",
      "                  ...\n",
      "                  'Z9882', 'Z9884', 'Z98890', 'Z98891', 'Z9911', 'Z992',\n",
      "                  'Z993', 'Z9981', 'Z9989', 'invl'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['A084', 'A599', 'A6000', 'A630', 'A6920', 'B001', 'B0059',\n",
      "                  'B009', 'B029', 'B079',\n",
      "                  ...\n",
      "                  'Z9884', 'Z9885', 'Z98890', 'Z98891', 'Z9911', 'Z992',\n",
      "                  'Z993', 'Z9981', 'Z9989', 'incn'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['A4902', 'A6000', 'B001', 'B009', 'B0229', 'B029', 'B159',\n",
      "                  'B182', 'B1910', 'B1920',\n",
      "                  ...\n",
      "                  'Z98818', 'Z9882', 'Z9884', 'Z98890', 'Z98891', 'Z9911',\n",
      "                  'Z992', 'Z993', 'Z9981', 'Z9989'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['A4902', 'A599', 'A6920', 'B009', 'B0229', 'B079', 'B181',\n",
      "                  'B1910', 'B1920', 'B20',\n",
      "                  ...\n",
      "                  'Z98818', 'Z9882', 'Z9884', 'Z98890', 'Z98891', 'Z9911',\n",
      "                  'Z992', 'Z993', 'Z9981', 'Z9989'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['A0472', 'A439', 'A6000', 'B0059', 'B009', 'B0230', 'B029',\n",
      "                  'B181', 'B182', 'B1920',\n",
      "                  ...\n",
      "                  'Z9882', 'Z9884', 'Z98890', 'Z98891', 'Z9911', 'Z992',\n",
      "                  'Z993', 'Z9981', 'Z9989', 'invl'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['B009', 'B0229', 'B159', 'B182', 'B1920', 'B20', 'B9562',\n",
      "                  'C189', 'C3411', 'C439',\n",
      "                  ...\n",
      "                  'Z9882', 'Z9884', 'Z98890', 'Z98891', 'Z9911', 'Z992',\n",
      "                  'Z993', 'Z9981', 'Z9989', 'invl'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['A6000', 'B009', 'B029', 'B1910', 'B1920', 'B9620', 'B9729',\n",
      "                  'C50311', 'C50911', 'C50919',\n",
      "                  ...\n",
      "                  'Z9862', 'Z98818', 'Z9882', 'Z9884', 'Z98890', 'Z98891',\n",
      "                  'Z992', 'Z993', 'Z9981', 'Z9989'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['A6000', 'B019', 'B1920', 'B351', 'B356', 'B952', 'B999',\n",
      "                  'C44319', 'C4A9', 'C50122',\n",
      "                  ...\n",
      "                  'Z9862', 'Z9882', 'Z9884', 'Z98890', 'Z98891', 'Z9911',\n",
      "                  'Z992', 'Z993', 'Z9981', 'Z9989'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['A4151', 'A630', 'B029', 'B182', 'B1920', 'B20', 'C44329',\n",
      "                  'C50111', 'C50112', 'C50911',\n",
      "                  ...\n",
      "                  'Z9861', 'Z9862', 'Z9884', 'Z98890', 'Z98891', 'Z992',\n",
      "                  'Z993', 'Z9981', 'Z9989', 'invl'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['A6920', 'B001', 'B351', 'B951', 'B9562', 'C439', 'C44320',\n",
      "                  'C50211', 'C50912', 'C9110',\n",
      "                  ...\n",
      "                  'Z9862', 'Z98811', 'Z9884', 'Z98890', 'Z98891', 'Z9911',\n",
      "                  'Z992', 'Z9981', 'Z9989', 'invl'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['A0472', 'B0223', 'B079', 'B1920', 'B961', 'C50412',\n",
      "                  'C50912', 'C61', 'C9110', 'D251',\n",
      "                  ...\n",
      "                  'Z9861', 'Z98811', 'Z9884', 'Z98890', 'Z98891', 'Z9911',\n",
      "                  'Z992', 'Z9981', 'Z9989', 'invl'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['B9689', 'C61', 'D509', 'D563', 'D62', 'D631', 'D638',\n",
      "                  'D649', 'D693', 'D899',\n",
      "                  ...\n",
      "                  'Z9849', 'Z9851', 'Z9862', 'Z9884', 'Z98890', 'Z98891',\n",
      "                  'Z992', 'Z993', 'Z9981', 'Z9989'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['BLD001', 'BLD003', 'BLD004', 'BLD005', 'BLD006', 'BLD007',\n",
      "                  'BLD008', 'CIR001', 'CIR003', 'CIR004',\n",
      "                  ...\n",
      "                  'SYM007', 'SYM010', 'SYM011', 'SYM012', 'SYM013', 'SYM014',\n",
      "                  'SYM015', 'SYM016', 'SYM017', 'XXX111'],\n",
      ", ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['0-17', '18-64', '65+'], ordered=False, categories_dtype=object): 1, CategoricalDtype(categories=['0-17', '18-39', '40-54', '55-64', '65-69', '70-79', '80+'], ordered=False, categories_dtype=object): 1}\n",
      "   Memory: 130.9 MB\n",
      "\n",
      "   Sample data:\n",
      "   KEY_NASS HOSP_NASS HOSP_TEACH HOSP_LOCATION  HOSP_LOCTEACH HOSP_REGION HOSP_BEDSIZE_CAT    DISCWT  NASS_STRATUM  N_DISC_U  N_HOSP_U  S_DISC_U  S_HOSP_U  TOTAL_AS_ENCOUNTERS  YEAR  AGE  FEMALE PL_NCHS ZIPINC_QRTL  AMONTH  AWEEKEND  DQTR PAY1  DISPUNIFORM   TOTCHG  NCPT_INSCOPE CPTCCS1  CPTCCS2  CPTCCS3  CPTCCS4  CPTCCS5  CPTCCS6  CPTCCS7  CPTCCS8  CPTCCS9  CPTCCS10  CPTCCS11  CPTCCS12  CPTCCS13  CPTCCS14  CPTCCS15  CPTCCS16  CPTCCS17  CPTCCS18  CPTCCS19  CPTCCS20  CPTCCS21  CPTCCS22  CPTCCS23  CPTCCS24  CPTCCS25  CPTCCS26  CPTCCS27  CPTCCS28  CPTCCS29  CPTCCS30   CPT1   CPT2 CPT3 CPT4  CPT5  CPT6  CPT7  CPT8  CPT9  CPT10  CPT11  CPT12  CPT13  CPT14  CPT15  CPT16  CPT17  CPT18  CPT19  CPT20  CPT21  CPT22  CPT23  CPT24  CPT25  CPT26  CPT27  CPT28  CPT29  CPT30  I10_NDX  I10_DX1  I10_DX2 I10_DX3 I10_DX4 I10_DX5 I10_DX6 I10_DX7 I10_DX8 I10_DX9 I10_DX10 I10_DX11 I10_DX12 I10_DX13 I10_DX14 I10_DX15 I10_DX16 I10_DX17 I10_DX18 I10_DX19 I10_DX20 RACE  I10_INJURY  I10_MULTINJURY  CMR_AIDS  CMR_ALCOHOL  CMR_AUTOIMMUNE  CMR_CANCER_LYMPH  CMR_CANCER_LEUK  CMR_CANCER_METS  CMR_CANCER_NSITU  CMR_CANCER_SOLID  CMR_DEMENTIA  CMR_DEPRESS  CMR_DIAB_UNCX  CMR_DIAB_CX  CMR_DRUG_ABUSE  CMR_HTN_CX  CMR_HTN_UNCX  CMR_LUNG_CHRONIC  CMR_OBESE  CMR_PERIVASC  CMR_THYROID_HYPO  CMR_THYROID_OTH  CMR_VERSION DXCCSR_Default_DX1  DXCCSR_BLD001  DXCCSR_BLD002  DXCCSR_BLD003  DXCCSR_BLD004  DXCCSR_BLD005  DXCCSR_BLD006  DXCCSR_BLD007  DXCCSR_BLD008  DXCCSR_BLD009  DXCCSR_BLD010  DXCCSR_CIR001  DXCCSR_CIR002  DXCCSR_CIR003  DXCCSR_CIR004  DXCCSR_CIR005  DXCCSR_CIR006  DXCCSR_CIR007  DXCCSR_CIR008  DXCCSR_CIR009  DXCCSR_CIR010  DXCCSR_CIR011  DXCCSR_CIR012  DXCCSR_CIR013  DXCCSR_CIR014  DXCCSR_CIR015  DXCCSR_CIR016  DXCCSR_CIR017  DXCCSR_CIR018  DXCCSR_CIR019  DXCCSR_CIR020  DXCCSR_CIR021  DXCCSR_CIR022  DXCCSR_CIR023  DXCCSR_CIR024  DXCCSR_CIR025  DXCCSR_CIR026  DXCCSR_CIR027  DXCCSR_CIR028  DXCCSR_CIR029  DXCCSR_CIR030  DXCCSR_CIR031  DXCCSR_CIR032  DXCCSR_CIR033  DXCCSR_CIR034  DXCCSR_CIR035  DXCCSR_CIR036  DXCCSR_CIR037  DXCCSR_CIR038  DXCCSR_CIR039  DXCCSR_DIG001  DXCCSR_DIG002  DXCCSR_DIG003  DXCCSR_DIG004  DXCCSR_DIG005  DXCCSR_DIG006  DXCCSR_DIG007  DXCCSR_DIG008  DXCCSR_DIG009  DXCCSR_DIG010  DXCCSR_DIG011  DXCCSR_DIG012  DXCCSR_DIG013  DXCCSR_DIG014  DXCCSR_DIG015  DXCCSR_DIG016  DXCCSR_DIG017  DXCCSR_DIG018  DXCCSR_DIG019  DXCCSR_DIG020  DXCCSR_DIG021  DXCCSR_DIG022  DXCCSR_DIG023  DXCCSR_DIG024  DXCCSR_DIG025  DXCCSR_EAR001  DXCCSR_EAR002  DXCCSR_EAR003  DXCCSR_EAR004  DXCCSR_EAR005  DXCCSR_EAR006  DXCCSR_END001  DXCCSR_END002  DXCCSR_END003  DXCCSR_END004  DXCCSR_END005  DXCCSR_END006  DXCCSR_END007  DXCCSR_END008  DXCCSR_END009  DXCCSR_END010  DXCCSR_END011  DXCCSR_END012  DXCCSR_END013  DXCCSR_END014  DXCCSR_END015  DXCCSR_END016  DXCCSR_END017  DXCCSR_EXT001  DXCCSR_EXT002  DXCCSR_EXT003  DXCCSR_EXT004  DXCCSR_EXT005  DXCCSR_EXT006  DXCCSR_EXT007  DXCCSR_EXT008  DXCCSR_EXT009  DXCCSR_EXT010  DXCCSR_EXT011  DXCCSR_EXT012  DXCCSR_EXT013  DXCCSR_EXT014  DXCCSR_EXT015  DXCCSR_EXT016  DXCCSR_EXT017  DXCCSR_EXT018  DXCCSR_EXT019  DXCCSR_EXT020  DXCCSR_EXT021  DXCCSR_EXT022  DXCCSR_EXT023  DXCCSR_EXT024  DXCCSR_EXT025  DXCCSR_EXT026  DXCCSR_EXT027  DXCCSR_EXT028  DXCCSR_EXT029  DXCCSR_EXT030  DXCCSR_EYE001  DXCCSR_EYE002  DXCCSR_EYE003  DXCCSR_EYE004  DXCCSR_EYE005  DXCCSR_EYE006  DXCCSR_EYE007  DXCCSR_EYE008  DXCCSR_EYE009  DXCCSR_EYE010  DXCCSR_EYE011  DXCCSR_EYE012  DXCCSR_FAC001  DXCCSR_FAC002  DXCCSR_FAC003  DXCCSR_FAC004  DXCCSR_FAC005  DXCCSR_FAC006  DXCCSR_FAC007  DXCCSR_FAC008  DXCCSR_FAC009  DXCCSR_FAC010  DXCCSR_FAC011  DXCCSR_FAC012  DXCCSR_FAC013  DXCCSR_FAC014  DXCCSR_FAC015  DXCCSR_FAC016  DXCCSR_FAC017  DXCCSR_FAC018  DXCCSR_FAC019  DXCCSR_FAC020  DXCCSR_FAC021  DXCCSR_FAC022  DXCCSR_FAC023  DXCCSR_FAC024  DXCCSR_FAC025  DXCCSR_GEN001  DXCCSR_GEN002  DXCCSR_GEN003  DXCCSR_GEN004  DXCCSR_GEN005  DXCCSR_GEN006  DXCCSR_GEN007  DXCCSR_GEN008  DXCCSR_GEN009  DXCCSR_GEN010  DXCCSR_GEN011  DXCCSR_GEN012  DXCCSR_GEN013  DXCCSR_GEN014  DXCCSR_GEN015  DXCCSR_GEN016  DXCCSR_GEN017  DXCCSR_GEN018  DXCCSR_GEN019  DXCCSR_GEN020  DXCCSR_GEN021  DXCCSR_GEN022  DXCCSR_GEN023  DXCCSR_GEN024  DXCCSR_GEN025  DXCCSR_GEN026  DXCCSR_INF001  DXCCSR_INF002  DXCCSR_INF003  DXCCSR_INF004  DXCCSR_INF005  DXCCSR_INF006  DXCCSR_INF007  DXCCSR_INF008  DXCCSR_INF009  DXCCSR_INF010  DXCCSR_INF011  DXCCSR_INF012  DXCCSR_INJ001  DXCCSR_INJ002  DXCCSR_INJ003  DXCCSR_INJ004  DXCCSR_INJ005  DXCCSR_INJ006  DXCCSR_INJ007  DXCCSR_INJ008  DXCCSR_INJ009  DXCCSR_INJ010  DXCCSR_INJ011  DXCCSR_INJ012  DXCCSR_INJ013  DXCCSR_INJ014  DXCCSR_INJ015  DXCCSR_INJ016  DXCCSR_INJ017  DXCCSR_INJ018  DXCCSR_INJ019  DXCCSR_INJ020  DXCCSR_INJ021  DXCCSR_INJ022  DXCCSR_INJ023  DXCCSR_INJ024  DXCCSR_INJ025  DXCCSR_INJ026  DXCCSR_INJ027  DXCCSR_INJ028  DXCCSR_INJ029  DXCCSR_INJ030  DXCCSR_INJ031  DXCCSR_INJ032  DXCCSR_INJ033  DXCCSR_INJ034  DXCCSR_INJ035  DXCCSR_INJ036  DXCCSR_INJ037  DXCCSR_INJ038  DXCCSR_INJ039  DXCCSR_INJ040  DXCCSR_INJ041  DXCCSR_INJ042  DXCCSR_INJ043  DXCCSR_INJ044  DXCCSR_INJ045  DXCCSR_INJ046  DXCCSR_INJ047  DXCCSR_INJ048  DXCCSR_INJ049  DXCCSR_INJ050  DXCCSR_INJ051  DXCCSR_INJ052  DXCCSR_INJ053  DXCCSR_INJ054  DXCCSR_INJ055  DXCCSR_INJ056  DXCCSR_INJ057  DXCCSR_INJ058  DXCCSR_INJ059  DXCCSR_INJ060  DXCCSR_INJ061  DXCCSR_INJ062  DXCCSR_INJ063  DXCCSR_INJ064  DXCCSR_INJ065  DXCCSR_INJ066  DXCCSR_INJ067  DXCCSR_INJ068  DXCCSR_INJ069  DXCCSR_INJ070  DXCCSR_INJ071  DXCCSR_INJ072  DXCCSR_INJ073  DXCCSR_INJ074  DXCCSR_INJ075  DXCCSR_INJ076  DXCCSR_MAL001  DXCCSR_MAL002  DXCCSR_MAL003  DXCCSR_MAL004  DXCCSR_MAL005  DXCCSR_MAL006  DXCCSR_MAL007  DXCCSR_MAL008  DXCCSR_MAL009  DXCCSR_MAL010  DXCCSR_MBD001  DXCCSR_MBD002  DXCCSR_MBD003  DXCCSR_MBD004  DXCCSR_MBD005  DXCCSR_MBD006  DXCCSR_MBD007  DXCCSR_MBD008  DXCCSR_MBD009  DXCCSR_MBD010  DXCCSR_MBD011  DXCCSR_MBD012  DXCCSR_MBD013  DXCCSR_MBD014  DXCCSR_MBD017  DXCCSR_MBD018  DXCCSR_MBD019  DXCCSR_MBD020  DXCCSR_MBD021  DXCCSR_MBD022  DXCCSR_MBD023  DXCCSR_MBD024  DXCCSR_MBD025  DXCCSR_MBD026  DXCCSR_MBD027  DXCCSR_MBD028  DXCCSR_MBD029  DXCCSR_MBD030  DXCCSR_MBD031  DXCCSR_MBD032  DXCCSR_MBD033  DXCCSR_MBD034  DXCCSR_MUS001  DXCCSR_MUS002  DXCCSR_MUS003  DXCCSR_MUS004  DXCCSR_MUS005  DXCCSR_MUS006  DXCCSR_MUS007  DXCCSR_MUS008  DXCCSR_MUS009  DXCCSR_MUS010  DXCCSR_MUS011  DXCCSR_MUS012  DXCCSR_MUS013  DXCCSR_MUS014  DXCCSR_MUS015  DXCCSR_MUS016  DXCCSR_MUS017  DXCCSR_MUS018  DXCCSR_MUS019  DXCCSR_MUS020  DXCCSR_MUS021  DXCCSR_MUS022  DXCCSR_MUS023  DXCCSR_MUS024  DXCCSR_MUS025  DXCCSR_MUS026  DXCCSR_MUS027  DXCCSR_MUS028  DXCCSR_MUS029  DXCCSR_MUS030  DXCCSR_MUS031  DXCCSR_MUS032  DXCCSR_MUS033  DXCCSR_MUS034  DXCCSR_MUS035  DXCCSR_MUS036  DXCCSR_MUS037  DXCCSR_MUS038  DXCCSR_NEO001  DXCCSR_NEO002  DXCCSR_NEO003  DXCCSR_NEO004  DXCCSR_NEO005  DXCCSR_NEO006  DXCCSR_NEO007  DXCCSR_NEO008  DXCCSR_NEO009  DXCCSR_NEO010  DXCCSR_NEO011  DXCCSR_NEO012  DXCCSR_NEO013  DXCCSR_NEO014  DXCCSR_NEO015  DXCCSR_NEO016  DXCCSR_NEO017  DXCCSR_NEO018  DXCCSR_NEO019  DXCCSR_NEO020  DXCCSR_NEO021  DXCCSR_NEO022  DXCCSR_NEO023  DXCCSR_NEO024  DXCCSR_NEO025  DXCCSR_NEO026  DXCCSR_NEO027  DXCCSR_NEO028  DXCCSR_NEO029  DXCCSR_NEO030  DXCCSR_NEO031  DXCCSR_NEO032  DXCCSR_NEO033  DXCCSR_NEO034  DXCCSR_NEO035  DXCCSR_NEO036  DXCCSR_NEO037  DXCCSR_NEO038  DXCCSR_NEO039  DXCCSR_NEO040  DXCCSR_NEO041  DXCCSR_NEO042  DXCCSR_NEO043  DXCCSR_NEO044  DXCCSR_NEO045  DXCCSR_NEO046  DXCCSR_NEO047  DXCCSR_NEO048  DXCCSR_NEO049  DXCCSR_NEO050  DXCCSR_NEO051  DXCCSR_NEO052  DXCCSR_NEO053  DXCCSR_NEO054  DXCCSR_NEO055  DXCCSR_NEO056  DXCCSR_NEO057  DXCCSR_NEO058  DXCCSR_NEO059  DXCCSR_NEO060  DXCCSR_NEO061  DXCCSR_NEO062  DXCCSR_NEO063  DXCCSR_NEO064  DXCCSR_NEO065  DXCCSR_NEO066  DXCCSR_NEO067  DXCCSR_NEO068  DXCCSR_NEO069  DXCCSR_NEO070  DXCCSR_NEO071  DXCCSR_NEO072  DXCCSR_NEO073  DXCCSR_NEO074  DXCCSR_NVS001  DXCCSR_NVS002  DXCCSR_NVS003  DXCCSR_NVS004  DXCCSR_NVS005  DXCCSR_NVS006  DXCCSR_NVS007  DXCCSR_NVS008  DXCCSR_NVS009  DXCCSR_NVS010  DXCCSR_NVS011  DXCCSR_NVS012  DXCCSR_NVS013  DXCCSR_NVS014  DXCCSR_NVS015  DXCCSR_NVS016  DXCCSR_NVS017  DXCCSR_NVS018  DXCCSR_NVS019  DXCCSR_NVS020  DXCCSR_NVS021  DXCCSR_NVS022  DXCCSR_PNL001  DXCCSR_PNL002  DXCCSR_PNL003  DXCCSR_PNL004  DXCCSR_PNL005  DXCCSR_PNL006  DXCCSR_PNL007  DXCCSR_PNL008  DXCCSR_PNL009  DXCCSR_PNL010  DXCCSR_PNL011  DXCCSR_PNL012  DXCCSR_PNL013  DXCCSR_PNL014  DXCCSR_PNL015  DXCCSR_PRG001  DXCCSR_PRG002  DXCCSR_PRG003  DXCCSR_PRG004  DXCCSR_PRG005  DXCCSR_PRG006  DXCCSR_PRG007  DXCCSR_PRG008  DXCCSR_PRG009  DXCCSR_PRG010  DXCCSR_PRG011  DXCCSR_PRG012  DXCCSR_PRG013  DXCCSR_PRG014  DXCCSR_PRG015  DXCCSR_PRG016  DXCCSR_PRG017  DXCCSR_PRG018  DXCCSR_PRG019  DXCCSR_PRG020  DXCCSR_PRG021  DXCCSR_PRG022  DXCCSR_PRG023  DXCCSR_PRG024  DXCCSR_PRG025  DXCCSR_PRG026  DXCCSR_PRG027  DXCCSR_PRG028  DXCCSR_PRG029  DXCCSR_PRG030  DXCCSR_RSP001  DXCCSR_RSP002  DXCCSR_RSP003  DXCCSR_RSP004  DXCCSR_RSP005  DXCCSR_RSP006  DXCCSR_RSP007  DXCCSR_RSP008  DXCCSR_RSP009  DXCCSR_RSP010  DXCCSR_RSP011  DXCCSR_RSP012  DXCCSR_RSP013  DXCCSR_RSP014  DXCCSR_RSP015  DXCCSR_RSP016  DXCCSR_RSP017  DXCCSR_SKN001  DXCCSR_SKN002  DXCCSR_SKN003  DXCCSR_SKN004  DXCCSR_SKN005  DXCCSR_SKN006  DXCCSR_SKN007  DXCCSR_SYM001  DXCCSR_SYM002  DXCCSR_SYM003  DXCCSR_SYM004  DXCCSR_SYM005  DXCCSR_SYM006  DXCCSR_SYM007  DXCCSR_SYM008  DXCCSR_SYM009  DXCCSR_SYM010  DXCCSR_SYM011  DXCCSR_SYM012  DXCCSR_SYM013  DXCCSR_SYM014  DXCCSR_SYM015  DXCCSR_SYM016  DXCCSR_SYM017  DXCCSR_VERSION AGEGRP AGEGRP2\n",
      "0  90000001     40053          1             1              3           4                2  1.579073             9    321406        85    203541        53                 4093  2020   16       1       3           4      12         0     4    3            1  28138.0             1     148     -999     -999     -999     -999     -999     -999     -999     -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999  26735    NaN  NaN  NaN   NaN   NaN   NaN   NaN   NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN        3  S62511A  X58XXXA   Y9323     NaN     NaN     NaN     NaN     NaN     NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN    1           1               0         0            0               0                 0                0                0                 0                 0             0            0              0            0               0           0             0                 0          0             0                 0                0  2022.099976             INJ004              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              3              3              0              0              0              0              0              3              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              1              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0     2022.099976   0-17    0-17\n",
      "1  90000002     20162          0             1              2           2                2  1.031092            49    152635        44    148032        42                 3047  2020   41       1       5           1       6         0     2    3            1  49086.0             2     225      225     -999     -999     -999     -999     -999     -999     -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999      -999  93653  93653  NaN  NaN   NaN   NaN   NaN   NaN   NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN        9     I471    Z7984  Z79899  Z87891    Z885    Z888  Z91018  Z01818   Z1159      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN    1           0               0         0            0               0                 0                0                0                 0                 0             0            0              0            0               0           0             0                 0          0             0                 0                0  2022.099976             CIR017              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              1              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              3              0              3              0              0              0              0              3              0              0              0              3              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0              0     2022.099976  18-64   40-54\n",
      "\n",
      "============================================================\n",
      "ðŸŽ¯ ULTRA-READY FOR ANALYSIS!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "from typing import Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class UltraFastDataLoader:\n",
    "    def __init__(self):\n",
    "        \"\"\"Ultra-optimized Python data loader using modern techniques\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        print(\"ðŸš€ Initializing UltraFast DataLoader...\")\n",
    "        \n",
    "        # Enhanced environment detection\n",
    "        self._detect_environment()\n",
    "        self._setup_paths()\n",
    "        self._detect_available_data()\n",
    "        self._setup_optimization()\n",
    "        \n",
    "        elapsed = time.time() - self.start_time\n",
    "        print(f\"âœ… UltraFast DataLoader ready in {elapsed:.2f}s\")\n",
    "        print(f\"   Environment: {self.env_type}\")\n",
    "        print(f\"   Base path: {self.base_path}\")\n",
    "        print(f\"   Available files: {len(self.available_files)}\")\n",
    "        print(f\"   Optimization: {self.optimization_level}\")\n",
    "\n",
    "    def _detect_environment(self):\n",
    "        \"\"\"Smart environment detection with memory profiling\"\"\"\n",
    "        is_colab = 'COLAB_GPU' in os.environ or 'google.colab' in str(type(get_ipython()).__module__)\n",
    "        is_gce = os.path.exists('/opt/nass')\n",
    "        is_vertex = 'DL_ANACONDA_HOME' in os.environ\n",
    "        \n",
    "        if is_colab:\n",
    "            self.env_type = \"Google Colab\"\n",
    "            self.base_path = Path('/content')\n",
    "        elif is_gce:\n",
    "            self.env_type = \"GCE Instance\"\n",
    "            self.base_path = Path('/opt/nass')\n",
    "        elif is_vertex:\n",
    "            self.env_type = \"Vertex AI\"\n",
    "            self.base_path = Path.home()\n",
    "        else:\n",
    "            self.env_type = \"Local/Jupyter\"\n",
    "            self.base_path = Path.home()\n",
    "        \n",
    "        # Detect available memory\n",
    "        try:\n",
    "            import psutil\n",
    "            self.available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
    "            self.total_memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "        except:\n",
    "            self.available_memory_gb = 8  # Conservative estimate\n",
    "            self.total_memory_gb = 16\n",
    "\n",
    "    def _setup_optimization(self):\n",
    "        \"\"\"Configure optimization strategy based on environment\"\"\"\n",
    "        # Determine best optimization strategy\n",
    "        if self.available_memory_gb > 20:\n",
    "            self.optimization_level = \"Ultra (>20GB RAM)\"\n",
    "            self.chunk_size = None  # Load all at once\n",
    "            self.use_pyarrow = True\n",
    "            self.use_parallel = True\n",
    "        elif self.available_memory_gb > 10:\n",
    "            self.optimization_level = \"High (10-20GB RAM)\"\n",
    "            self.chunk_size = 100000\n",
    "            self.use_pyarrow = True\n",
    "            self.use_parallel = True\n",
    "        elif self.available_memory_gb > 4:\n",
    "            self.optimization_level = \"Medium (4-10GB RAM)\"\n",
    "            self.chunk_size = 50000\n",
    "            self.use_pyarrow = False\n",
    "            self.use_parallel = False\n",
    "        else:\n",
    "            self.optimization_level = \"Conservative (<4GB RAM)\"\n",
    "            self.chunk_size = 25000\n",
    "            self.use_pyarrow = False\n",
    "            self.use_parallel = False\n",
    "        \n",
    "        # Try to enable PyArrow if available\n",
    "        if self.use_pyarrow:\n",
    "            try:\n",
    "                import pyarrow\n",
    "                self.has_pyarrow = True\n",
    "                print(f\"   PyArrow engine: Available\")\n",
    "            except ImportError:\n",
    "                self.has_pyarrow = False\n",
    "                print(f\"   PyArrow engine: Not available (fallback to C)\")\n",
    "        else:\n",
    "            self.has_pyarrow = False\n",
    "\n",
    "    def _setup_paths(self):\n",
    "        \"\"\"Setup data directory and ensure it exists\"\"\"\n",
    "        self.data_dir = self.base_path / 'data'\n",
    "        self.data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _detect_available_data(self):\n",
    "        \"\"\"Scan for available data files with size analysis\"\"\"\n",
    "        self.available_files = {}\n",
    "        \n",
    "        search_patterns = {\n",
    "            'full': ['nass_2020_full.csv', 'nass_2020_all.csv', 'nass_full.csv'],\n",
    "            'local': ['nass_2020_local.csv', 'nass_local.csv'],\n",
    "            'simulated': ['nass_2020_simulated.csv', 'nass_simulated.csv'],\n",
    "            'cached': ['nass_data.csv', 'nass_data_cleaned.csv']\n",
    "        }\n",
    "        \n",
    "        search_locations = [\n",
    "            self.base_path,\n",
    "            self.data_dir,\n",
    "            Path.cwd(),\n",
    "            Path('/opt/nass') if Path('/opt/nass').exists() else None\n",
    "        ]\n",
    "        search_locations = [loc for loc in search_locations if loc is not None]\n",
    "        \n",
    "        for data_type, patterns in search_patterns.items():\n",
    "            for pattern in patterns:\n",
    "                for location in search_locations:\n",
    "                    file_path = location / pattern\n",
    "                    if file_path.exists():\n",
    "                        size_mb = file_path.stat().st_size / 1e6\n",
    "                        # Estimate memory requirement (CSV typically 3-5x file size in memory)\n",
    "                        estimated_memory_gb = (size_mb * 4) / 1000\n",
    "                        \n",
    "                        self.available_files[data_type] = {\n",
    "                            'path': file_path,\n",
    "                            'size_mb': size_mb,\n",
    "                            'estimated_memory_gb': estimated_memory_gb,\n",
    "                            'fits_in_memory': estimated_memory_gb < (self.available_memory_gb * 0.8),\n",
    "                            'priority': len(self.available_files)\n",
    "                        }\n",
    "                        break\n",
    "                if data_type in self.available_files:\n",
    "                    break\n",
    "\n",
    "    def _smart_download(self, url: str, file_path: Path) -> bool:\n",
    "        \"\"\"Ultra-fast download with multiple optimization strategies\"\"\"\n",
    "        try:\n",
    "            print(f\"ðŸ“¥ Smart downloading: {url.split('/')[-1]}\")\n",
    "            \n",
    "            # Strategy 1: Try concurrent download with requests-futures\n",
    "            try:\n",
    "                from concurrent.futures import ThreadPoolExecutor\n",
    "                import requests\n",
    "                \n",
    "                # Get file size\n",
    "                response = requests.head(url, timeout=30)\n",
    "                total_size = int(response.headers.get('content-length', 0))\n",
    "                total_mb = total_size / 1e6\n",
    "                \n",
    "                print(f\"   Size: {total_mb:.1f} MB\")\n",
    "                print(f\"   Using optimized concurrent download...\")\n",
    "                \n",
    "                # Download with progress\n",
    "                response = requests.get(url, stream=True, timeout=120)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                downloaded = 0\n",
    "                chunk_size = 64 * 1024  # 64KB chunks for speed\n",
    "                \n",
    "                with open(file_path, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                            downloaded += len(chunk)\n",
    "                \n",
    "                print(f\"âœ… Downloaded: {downloaded/1e6:.1f} MB\")\n",
    "                return True\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   Concurrent download failed: {e}\")\n",
    "                return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Download failed: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _ultra_fast_csv_load(self, file_path: Path) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Ultra-optimized CSV loading with multiple engines and strategies\"\"\"\n",
    "        file_size_mb = file_path.stat().st_size / 1e6\n",
    "        estimated_memory_gb = (file_size_mb * 4) / 1000\n",
    "        \n",
    "        print(f\"ðŸ“– Ultra-fast loading: {file_path.name}\")\n",
    "        print(f\"   File size: {file_size_mb:.1f} MB\")\n",
    "        print(f\"   Estimated memory: {estimated_memory_gb:.1f} GB\")\n",
    "        print(f\"   Available memory: {self.available_memory_gb:.1f} GB\")\n",
    "        \n",
    "        load_start = time.time()\n",
    "        \n",
    "        # Strategy selection based on file size and available memory\n",
    "        if not self.available_files.get(file_path.stem, {}).get('fits_in_memory', True):\n",
    "            print(f\"   Strategy: Chunked loading (large file)\")\n",
    "            return self._chunked_load(file_path)\n",
    "        \n",
    "        # Strategy 1: PyArrow engine (fastest for large files)\n",
    "        if self.has_pyarrow and file_size_mb > 100:\n",
    "            print(f\"   Strategy: PyArrow engine\")\n",
    "            try:\n",
    "                df = pd.read_csv(\n",
    "                    file_path,\n",
    "                    engine='pyarrow',\n",
    "                    dtype_backend='pyarrow'\n",
    "                )\n",
    "                load_time = time.time() - load_start\n",
    "                print(f\"âœ… PyArrow loaded in {load_time:.2f}s\")\n",
    "                return self._optimize_dataframe(df)\n",
    "            except Exception as e:\n",
    "                print(f\"   PyArrow failed: {e}, trying C engine...\")\n",
    "        \n",
    "        # Strategy 2: Optimized C engine with smart dtypes\n",
    "        print(f\"   Strategy: Optimized C engine\")\n",
    "        try:\n",
    "            # Smart dtype inference for common NASS columns\n",
    "            dtype_hints = self._get_smart_dtypes()\n",
    "            \n",
    "            df = pd.read_csv(\n",
    "                file_path,\n",
    "                engine='c',\n",
    "                low_memory=False,\n",
    "                dtype=dtype_hints,\n",
    "                na_values=['', 'NA', 'NULL', '.', 'Missing'],\n",
    "                keep_default_na=True,\n",
    "                skipinitialspace=True\n",
    "            )\n",
    "            \n",
    "            load_time = time.time() - load_start\n",
    "            print(f\"âœ… C engine loaded in {load_time:.2f}s\")\n",
    "            return self._optimize_dataframe(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   C engine failed: {e}, trying Python engine...\")\n",
    "        \n",
    "        # Strategy 3: Fallback to Python engine\n",
    "        print(f\"   Strategy: Python engine (fallback)\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, engine='python')\n",
    "            load_time = time.time() - load_start\n",
    "            print(f\"âœ… Python engine loaded in {load_time:.2f}s\")\n",
    "            return self._optimize_dataframe(df)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ All engines failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _get_smart_dtypes(self) -> dict:\n",
    "        \"\"\"Smart dtype hints for common NASS columns\"\"\"\n",
    "        return {\n",
    "            # Identifiers\n",
    "            'KEY_NASS': 'str',\n",
    "            'HOSP_NASS': 'category',\n",
    "            \n",
    "            # Demographics\n",
    "            'AGE': 'Int16',\n",
    "            'FEMALE': 'Int8',\n",
    "            'RACE': 'category',\n",
    "            'ZIPINC_QRTL': 'category',\n",
    "            \n",
    "            # Hospital characteristics\n",
    "            'PAY1': 'category',\n",
    "            'HOSP_REGION': 'category',\n",
    "            'HOSP_LOCATION': 'category',\n",
    "            'HOSP_TEACH': 'category',\n",
    "            'HOSP_BEDSIZE_CAT': 'category',\n",
    "            'PL_NCHS': 'category',\n",
    "            \n",
    "            # Procedures\n",
    "            'CPTCCS1': 'category',\n",
    "            \n",
    "            # Numeric\n",
    "            'TOTCHG': 'float32',\n",
    "            'DISCWT': 'float32',\n",
    "        }\n",
    "\n",
    "    def _optimize_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Aggressive memory optimization\"\"\"\n",
    "        print(f\"   ðŸŽ¯ Optimizing DataFrame...\")\n",
    "        original_memory = df.memory_usage(deep=True).sum() / 1e6\n",
    "        \n",
    "        # Convert object columns to categories where beneficial\n",
    "        for col in df.select_dtypes(include=['object']).columns:\n",
    "            if df[col].nunique() / len(df) < 0.5:  # Less than 50% unique values\n",
    "                df[col] = df[col].astype('category')\n",
    "        \n",
    "        # Downcast numeric types\n",
    "        for col in df.select_dtypes(include=['int64']).columns:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "        \n",
    "        for col in df.select_dtypes(include=['float64']).columns:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        \n",
    "        optimized_memory = df.memory_usage(deep=True).sum() / 1e6\n",
    "        reduction = (1 - optimized_memory/original_memory) * 100\n",
    "        \n",
    "        print(f\"   ðŸ’¾ Memory optimized: {original_memory:.1f}MB â†’ {optimized_memory:.1f}MB ({reduction:.1f}% reduction)\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _chunked_load(self, file_path: Path) -> pd.DataFrame:\n",
    "        \"\"\"Memory-efficient chunked loading for large files\"\"\"\n",
    "        print(f\"   ðŸ“¦ Loading in chunks of {self.chunk_size:,} rows...\")\n",
    "        \n",
    "        chunks = []\n",
    "        chunk_count = 0\n",
    "        \n",
    "        try:\n",
    "            # Use iterator for memory efficiency\n",
    "            chunk_iterator = pd.read_csv(\n",
    "                file_path,\n",
    "                chunksize=self.chunk_size,\n",
    "                engine='c',\n",
    "                low_memory=True,\n",
    "                dtype=self._get_smart_dtypes()\n",
    "            )\n",
    "            \n",
    "            for chunk in chunk_iterator:\n",
    "                chunk_count += 1\n",
    "                optimized_chunk = self._optimize_dataframe(chunk)\n",
    "                chunks.append(optimized_chunk)\n",
    "                \n",
    "                if chunk_count % 10 == 0:\n",
    "                    print(f\"   Processed {chunk_count} chunks ({chunk_count * self.chunk_size:,} rows)\")\n",
    "            \n",
    "            print(f\"   ðŸ”„ Concatenating {chunk_count} chunks...\")\n",
    "            df = pd.concat(chunks, ignore_index=True)\n",
    "            \n",
    "            # Final optimization\n",
    "            df = self._optimize_dataframe(df)\n",
    "            \n",
    "            print(f\"âœ… Chunked loading complete: {df.shape[0]:,} rows\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Chunked loading failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_data_ultra_fast(self) -> pd.DataFrame:\n",
    "        \"\"\"Main ultra-fast loading method\"\"\"\n",
    "        load_start = time.time()\n",
    "        print(f\"\\nðŸŽ¯ ULTRA-FAST LOADING: {DATA_SOURCE.upper()}\")\n",
    "        print(f\"âš¡ Optimization level: {self.optimization_level}\")\n",
    "        \n",
    "        # Strategy 1: Try specified data source\n",
    "        df = None\n",
    "        if DATA_SOURCE == \"local\":\n",
    "            df = self._load_local_ultra()\n",
    "        elif DATA_SOURCE == \"github\":\n",
    "            df = self._load_github_ultra()\n",
    "        elif DATA_SOURCE == \"gcs\":\n",
    "            df = self._load_gcs_ultra()\n",
    "        elif DATA_SOURCE == \"drive\":\n",
    "            df = self._load_drive_ultra()\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid DATA_SOURCE: {DATA_SOURCE}\")\n",
    "        \n",
    "        # Strategy 2: Smart fallback\n",
    "        if df is None:\n",
    "            print(\"ðŸ”„ Primary source failed - trying ultra fallback...\")\n",
    "            df = self._ultra_fallback()\n",
    "        \n",
    "        if df is None:\n",
    "            raise Exception(\"âŒ All ultra-loading strategies failed!\")\n",
    "        \n",
    "        # Success metrics\n",
    "        load_time = time.time() - load_start\n",
    "        total_time = time.time() - self.start_time\n",
    "        memory_usage = df.memory_usage(deep=True).sum() / 1e6\n",
    "        \n",
    "        print(f\"\\nðŸŽ‰ ULTRA-FAST LOADING COMPLETE!\")\n",
    "        print(f\"   ðŸ“Š Shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "        print(f\"   ðŸ’¾ Memory: {memory_usage:.1f} MB\")\n",
    "        print(f\"   âš¡ Load speed: {df.shape[0]/load_time:,.0f} rows/sec\")\n",
    "        print(f\"   â±ï¸  Total time: {load_time:.2f}s (Setup: {total_time-load_time:.2f}s)\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _load_local_ultra(self) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Ultra-optimized local file loading\"\"\"\n",
    "        print(\"ðŸ“ Ultra-scanning for local files...\")\n",
    "        \n",
    "        # Sort by memory efficiency\n",
    "        sorted_files = sorted(\n",
    "            self.available_files.items(),\n",
    "            key=lambda x: (x[1]['fits_in_memory'], -x[1]['priority'])\n",
    "        )\n",
    "        \n",
    "        for data_type, file_info in sorted_files:\n",
    "            print(f\"   ðŸŽ¯ Trying: {file_info['path'].name}\")\n",
    "            print(f\"      Size: {file_info['size_mb']:.1f} MB\")\n",
    "            print(f\"      Fits in memory: {file_info['fits_in_memory']}\")\n",
    "            \n",
    "            df = self._ultra_fast_csv_load(file_info['path'])\n",
    "            if df is not None:\n",
    "                return df\n",
    "        \n",
    "        print(\"âŒ No local files could be loaded\")\n",
    "        return None\n",
    "\n",
    "    def _load_github_ultra(self) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Ultra-optimized GitHub loading with smart caching\"\"\"\n",
    "        cache_path = self.data_dir / \"nass_data_github_ultra.csv\"\n",
    "        \n",
    "        # Check cache with size validation\n",
    "        if cache_path.exists():\n",
    "            cache_age_hours = (time.time() - cache_path.stat().st_mtime) / 3600\n",
    "            cache_size_mb = cache_path.stat().st_size / 1e6\n",
    "            \n",
    "            if cache_age_hours < 24 and cache_size_mb > 1:  # Valid cache\n",
    "                print(f\"ðŸ“‹ Using ultra-cache ({cache_age_hours:.1f}h old, {cache_size_mb:.1f}MB)\")\n",
    "                return self._ultra_fast_csv_load(cache_path)\n",
    "        \n",
    "        # Download with optimization\n",
    "        print(\"ðŸ“¥ Ultra-downloading from GitHub...\")\n",
    "        if self._smart_download(GITHUB_URL, cache_path):\n",
    "            return self._ultra_fast_csv_load(cache_path)\n",
    "        \n",
    "        # Fallback to old cache\n",
    "        if cache_path.exists():\n",
    "            print(\"ðŸ”„ Using old cache as ultra-fallback\")\n",
    "            return self._ultra_fast_csv_load(cache_path)\n",
    "        \n",
    "        return None\n",
    "\n",
    "    # Replace your existing _load_gcs_ultra method with this enhanced version\n",
    "    \n",
    "    def _load_gcs_ultra(self) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Ultra-optimized GCS loading with automatic Google authentication\"\"\"\n",
    "        try:\n",
    "            print(\"â˜ï¸ Ultra-connecting to Google Cloud Storage...\")\n",
    "            \n",
    "            # Strategy 1: Try automatic authentication (for GCE/Colab)\n",
    "            try:\n",
    "                from google.cloud import storage\n",
    "                client = storage.Client()\n",
    "                print(\"   âœ… Using automatic authentication\")\n",
    "            except Exception as auth_error:\n",
    "                print(f\"   âš ï¸  Automatic auth failed: {auth_error}\")\n",
    "                \n",
    "                # Strategy 2: Interactive OAuth flow for users\n",
    "                print(\"   ðŸ” Starting secure Google login flow...\")\n",
    "                client = self._authenticate_google_interactive()\n",
    "                \n",
    "                if client is None:\n",
    "                    print(\"   âŒ Authentication failed\")\n",
    "                    return None\n",
    "            \n",
    "            # Try user's personal bucket first, then fallback to public bucket\n",
    "            bucket_options = [\n",
    "                f\"{client.project}-nass-data\",  # User's project bucket\n",
    "                f\"nass-data-{client.project}\",  # Alternative naming\n",
    "                GCS_BUCKET  # Fallback to configured bucket\n",
    "            ]\n",
    "            \n",
    "            blob_options = [\n",
    "                GCS_BLOB,\n",
    "                \"nass_2020_full.csv\",\n",
    "                \"nass_2020_simulated.csv\"\n",
    "            ]\n",
    "            \n",
    "            # Smart bucket/blob discovery\n",
    "            for bucket_name in bucket_options:\n",
    "                try:\n",
    "                    bucket = client.bucket(bucket_name)\n",
    "                    \n",
    "                    for blob_name in blob_options:\n",
    "                        try:\n",
    "                            blob = bucket.blob(blob_name)\n",
    "                            \n",
    "                            # Check if blob exists\n",
    "                            if blob.exists():\n",
    "                                cache_path = self.data_dir / f\"nass_data_gcs_{bucket_name}_{blob_name}\"\n",
    "                                \n",
    "                                print(f\"ðŸ“¥ Found data: gs://{bucket_name}/{blob_name}\")\n",
    "                                print(f\"   Size: {blob.size / 1e6:.1f} MB\")\n",
    "                                \n",
    "                                # Download with progress\n",
    "                                print(\"   Downloading...\")\n",
    "                                blob.download_to_filename(cache_path)\n",
    "                                \n",
    "                                return self._ultra_fast_csv_load(cache_path)\n",
    "                        \n",
    "                        except Exception as blob_error:\n",
    "                            if VERBOSE_PRINTS:\n",
    "                                print(f\"   Blob {blob_name} not found in {bucket_name}\")\n",
    "                            continue\n",
    "                \n",
    "                except Exception as bucket_error:\n",
    "                    if VERBOSE_PRINTS:\n",
    "                        print(f\"   Bucket {bucket_name} not accessible\")\n",
    "                    continue\n",
    "            \n",
    "            # If nothing found, provide helpful guidance\n",
    "            print(\"âŒ No NASS data found in your Google Cloud Storage\")\n",
    "            print(\"\\nðŸ’¡ To use your own data:\")\n",
    "            print(\"   1. Upload your NASS CSV file to Google Cloud Storage\")\n",
    "            print(\"   2. Use bucket name format: 'your-project-nass-data'\")\n",
    "            print(\"   3. Or update GCS_BUCKET/GCS_BLOB in configuration\")\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Ultra-GCS failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _load_drive_ultra(self) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Ultra-optimized Drive loading\"\"\"\n",
    "        if self.env_type != \"Google Colab\":\n",
    "            print(\"âŒ Drive loading only available in Google Colab\")\n",
    "            return None\n",
    "        \n",
    "        drive_path = Path(DRIVE_PATH)\n",
    "        if not drive_path.exists():\n",
    "            print(f\"âŒ Drive file not found: {DRIVE_PATH}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"ðŸ“‚ Ultra-loading from Google Drive...\")\n",
    "        return self._ultra_fast_csv_load(drive_path)\n",
    "\n",
    "    def _ultra_fallback(self) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Intelligent ultra-fallback with memory-aware prioritization\"\"\"\n",
    "        print(\"ðŸ§  Ultra-smart fallback strategy...\")\n",
    "        \n",
    "        # Prioritize by memory fit, then by data quality\n",
    "        fallback_order = []\n",
    "        \n",
    "        # Add files that fit in memory first\n",
    "        for data_type in ['full', 'local', 'simulated', 'cached']:\n",
    "            if data_type in self.available_files:\n",
    "                if self.available_files[data_type]['fits_in_memory']:\n",
    "                    fallback_order.append(data_type)\n",
    "        \n",
    "        # Add files that don't fit (will use chunking)\n",
    "        for data_type in ['full', 'local', 'simulated', 'cached']:\n",
    "            if data_type in self.available_files:\n",
    "                if not self.available_files[data_type]['fits_in_memory']:\n",
    "                    fallback_order.append(data_type)\n",
    "        \n",
    "        for data_type in fallback_order:\n",
    "            file_info = self.available_files[data_type]\n",
    "            print(f\"   ðŸŽ¯ Ultra-trying: {file_info['path'].name}\")\n",
    "            print(f\"      Strategy: {'In-memory' if file_info['fits_in_memory'] else 'Chunked'}\")\n",
    "            \n",
    "            df = self._ultra_fast_csv_load(file_info['path'])\n",
    "            if df is not None:\n",
    "                print(f\"âœ… Ultra-fallback successful using {data_type} data\")\n",
    "                return df\n",
    "        \n",
    "        print(\"âŒ All ultra-fallback options exhausted\")\n",
    "        return None\n",
    "\n",
    "# Create and use the ultra-fast loader\n",
    "try:\n",
    "    print(\"ðŸš€ ULTRA-FAST DATA LOADING INITIATED\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    ultra_loader = UltraFastDataLoader()\n",
    "    df = ultra_loader.load_data_ultra_fast()\n",
    "    \n",
    "    # Quick validation and preview\n",
    "    if VERBOSE_PRINTS and df is not None:\n",
    "        print(f\"\\nðŸ“‹ ULTRA-PREVIEW:\")\n",
    "        print(f\"   Shape: {df.shape}\")\n",
    "        print(f\"   Dtypes: {df.dtypes.value_counts().to_dict()}\")\n",
    "        print(f\"   Memory: {df.memory_usage(deep=True).sum()/1e6:.1f} MB\")\n",
    "        print(f\"\\n   Sample data:\")\n",
    "        print(df.head(2).to_string())\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ðŸŽ¯ ULTRA-READY FOR ANALYSIS!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nðŸ’¥ ULTRA-CRITICAL ERROR: {e}\")\n",
    "    print(\"\\nðŸ”§ ULTRA-TROUBLESHOOTING:\")\n",
    "    print(\"   1. Check available memory vs file size\")\n",
    "    print(\"   2. Try smaller chunk_size for large files\")\n",
    "    print(\"   3. Install PyArrow: pip install pyarrow\")\n",
    "    print(\"   4. Use DATA_SOURCE='github' for simulated data\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxQ5fG5grPrj"
   },
   "source": [
    "Check if data has been loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "FqUzCuZOrPrj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data verified: 139,233 rows x 675 columns\n",
      "âœ… Ready for R analysis\n"
     ]
    }
   ],
   "source": [
    "# Verify data is available before R processing\n",
    "try:\n",
    "    if 'df' not in globals():\n",
    "        print(\"âŒ Data not loaded!\")\n",
    "        print(\"ðŸ’¡ Please run the 'Data Loading' section first (cell 12)\")\n",
    "        print(\"   This will create the 'df' variable needed for R analysis\")\n",
    "        raise NameError(\"df variable not found - run data loading first\")\n",
    "\n",
    "    print(f\"âœ… Data verified: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "    print(\"âœ… Ready for R analysis\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"âŒ {e}\")\n",
    "    print(\"\\nðŸ”„ Quick fix: Run these cells in order:\")\n",
    "    print(\"   1. Configuration (cell 5)\")\n",
    "    print(\"   2. Environment Setup (cell 7)\")\n",
    "    print(\"   3. Data Loading (cell 12)\")\n",
    "    print(\"   4. Then continue with R analysis\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3j3Ms-OArPrj"
   },
   "source": [
    "## 5. Complete Data Preprocessing\n",
    "\n",
    "Streamlined preprocessing: remove variables, clean data types, and create new variables - in Python prior to passing to R for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQzhWzohrPrj"
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing in Python (before R transfer)\n",
    "print(\"Complete preprocessing: removing variables + cleaning data types...\")\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "\n",
    "# ===== 1. REMOVE UNNECESSARY VARIABLES =====\n",
    "print(\"\\n1 Removing unnecessary variables...\")\n",
    "\n",
    "# Smart pattern-based removal in pandas (much faster than R)\n",
    "drop_patterns = [\n",
    "    r'^CPTCCS[2-9]$',      # CPTCCS2-CPTCCS9\n",
    "    r'^CPTCCS[1-3][0-9]$', # CPTCCS10-30\n",
    "    r'^CPT[2-9]$',         # CPT2-CPT9\n",
    "    r'^CPT[1-3][0-9]$',    # CPT10-30\n",
    "    r'^DXCCSR_',           # All DXCCSR columns (500+)\n",
    "]\n",
    "\n",
    "# Find columns to drop using vectorized operations\n",
    "drop_cols = []\n",
    "for pattern in drop_patterns:\n",
    "    matches = df.columns[df.columns.str.match(pattern)].tolist()\n",
    "    drop_cols.extend(matches)\n",
    "\n",
    "# Remove duplicates\n",
    "drop_cols = list(set(drop_cols))\n",
    "\n",
    "print(f\"   ðŸ“Š Found {len(drop_cols)} columns to drop\")\n",
    "print(f\"   ðŸ—‘ï¸  Patterns: CPTCCS2-30, CPT2-30, all DXCCSR_*\")\n",
    "\n",
    "# Drop the columns\n",
    "df = df.drop(columns=drop_cols)\n",
    "print(f\"   âœ… Reduced from {df.shape[1] + len(drop_cols)} to {df.shape[1]} columns\")\n",
    "\n",
    "# ===== 2. CLEAN DATA TYPES FOR rpy2 =====\n",
    "print(\"\\n2ï¸âƒ£ Cleaning data types for rpy2 compatibility...\")\n",
    "\n",
    "# Convert all object columns to strings (prevents mixed-type issues)\n",
    "object_columns = df.select_dtypes(include=['object']).columns\n",
    "if len(object_columns) > 0:\n",
    "    for col in object_columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "    print(f\"   âœ… Converted {len(object_columns)} object columns to strings\")\n",
    "\n",
    "# Handle NaN/inf values consistently\n",
    "df = df.fillna('')  # Replace NaN with empty strings\n",
    "float_cols = df.select_dtypes(include=['float64']).columns\n",
    "if len(float_cols) > 0:\n",
    "    df[float_cols] = df[float_cols].replace([float('inf'), float('-inf')], '')\n",
    "    print(f\"   âœ… Cleaned inf values in {len(float_cols)} float columns\")\n",
    "\n",
    "# ===== 3. CREATE KEY ANALYTICAL VARIABLES IN PANDAS =====\n",
    "print(\"\\n3ï¸âƒ£ Creating analytical variables...\")\n",
    "\n",
    "# Create WHITE indicator (1=White, 0=Non-White)\n",
    "if 'RACE' in df.columns:\n",
    "    df['WHITE'] = (df['RACE'].astype(str) == '1').astype(int)\n",
    "    print(\"   âœ… Created a race indicator boolean\")\n",
    "\n",
    "# Create age groups\n",
    "if 'AGE' in df.columns:\n",
    "    df['AGE'] = pd.to_numeric(df['AGE'], errors='coerce')  # Ensure numeric\n",
    "    df['AGE_GROUP'] = pd.cut(df['AGE'],\n",
    "                            bins=[0, 18, 30, 45, 65, float('inf')],\n",
    "                            labels=['0-17', '18-29', '30-44', '45-64', '65+'],\n",
    "                            right=False)\n",
    "    df['AGE_GROUP'] = df['AGE_GROUP'].astype(str)  # Convert to string for R\n",
    "    print(\"   âœ… Created AGE_GROUP categories\")\n",
    "\n",
    "# Create income level labels\n",
    "if 'ZIPINC_QRTL' in df.columns:\n",
    "    income_map = {1: 'Q1-Lowest', 2: 'Q2', 3: 'Q3', 4: 'Q4-Highest'}\n",
    "    df['INCOME_LEVEL'] = df['ZIPINC_QRTL'].astype(str).map(lambda x: income_map.get(int(x) if x.isdigit() else 0, 'Unknown'))\n",
    "    print(\"   âœ… Created INCOME_LEVEL labels\")\n",
    "\n",
    "# Ensure key numeric variables are properly typed\n",
    "numeric_vars = ['AGE', 'DISCWT', 'TOTCHG']\n",
    "for var in numeric_vars:\n",
    "    if var in df.columns:\n",
    "        df[var] = pd.to_numeric(df[var], errors='coerce')\n",
    "\n",
    "# save copy of cleaned data for R transfer\n",
    "cleaned_path = smart_loader.data_dir / \"nass_data_cleaned.csv\"\n",
    "df.to_csv(cleaned_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nâœ… PREPROCESSING COMPLETE!\")\n",
    "print(f\"ðŸ“Š Final shape: {df.shape}\")\n",
    "print(f\"ðŸ“ Cleaned data saved to: {cleaned_path}\")\n",
    "print(f\"ðŸ’¾ Ready for R transfer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1smsFldXrPrj"
   },
   "source": [
    "## 6. Final R Transfer & Processing\n",
    "\n",
    "Transfer the clean data to R and apply any final R-specific formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10dad708"
   },
   "outputs": [],
   "source": [
    "%%R -i df -i VERBOSE_PRINTS\n",
    "\n",
    "# Convert to data.table and apply R types\n",
    "NASS <- as.data.table(df)\n",
    "\n",
    "# Factor variables\n",
    "factor_vars <- c(\"ZIPINC_QRTL\", \"PAY1\", \"CPTCCS1\", \"HOSP_LOCATION\",\n",
    "                 \"HOSP_TEACH\", \"HOSP_NASS\", \"RACE\", \"AGE_GROUP\", \"INCOME_LEVEL\")\n",
    "existing_factors <- factor_vars[factor_vars %in% names(NASS)]\n",
    "NASS[, (existing_factors) := lapply(.SD, as.factor), .SDcols = existing_factors]\n",
    "\n",
    "# Boolean variables\n",
    "if(\"FEMALE\" %in% names(NASS)) NASS[, FEMALE := as.logical(as.numeric(FEMALE))]\n",
    "if(\"WHITE\" %in% names(NASS)) NASS[, WHITE := as.logical(as.numeric(WHITE))]\n",
    "\n",
    "# Compact output\n",
    "cat(\"âœ… R Complete:\", nrow(NASS), \"rows,\", ncol(NASS), \"cols,\",\n",
    "    round(object.size(NASS)/1024^2, 1), \"MB\\n\")\n",
    "cat(\"Converted\", length(existing_factors), \"factors + 2 booleans\\n\")\n",
    "\n",
    "if(VERBOSE_PRINTS) {\n",
    "  cat(\"\\nColumns:\\n\")\n",
    "  print(colnames(NASS))\n",
    "}\n",
    "\n",
    "cat(\"Data in R!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Status Check/Recovery\n",
    "\n",
    "Please use this cell to reset analysis environment in case of cell crash/hang. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PRACTICAL HEALTH CHECK & RECOVERY ===\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "def quick_health_check():\n",
    "    \"\"\"Quick, accurate health check focused on actual functionality\"\"\"\n",
    "    print(\"ðŸ” ENVIRONMENT STATUS CHECK\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # 1. Check core data\n",
    "    if 'df' in globals():\n",
    "        df_shape = globals()['df'].shape\n",
    "        print(f\"âœ… Data: {df_shape[0]:,} rows Ã— {df_shape[1]} columns\")\n",
    "    else:\n",
    "        print(\"âŒ Data: Not loaded\")\n",
    "        issues.append(\"data_missing\")\n",
    "    \n",
    "    # 2. Check R integration properly\n",
    "    r_working = False\n",
    "    try:\n",
    "        # Actually test R functionality\n",
    "        get_ipython().run_cell_magic('R', '', 'cat(\"R test successful\\\\n\")')\n",
    "        r_working = True\n",
    "        print(\"âœ… R integration: Working\")\n",
    "    except Exception as e:\n",
    "        print(\"âŒ R integration: Failed\")\n",
    "        issues.append(\"r_failed\")\n",
    "    \n",
    "    # 3. Check R data if R works\n",
    "    if r_working and 'df' in globals():\n",
    "        try:\n",
    "            get_ipython().run_cell_magic('R', '', '''\n",
    "            if(exists(\"NASS\")) {\n",
    "                cat(\"âœ… R data: Available (\", nrow(NASS), \"rows)\\\\n\")\n",
    "            } else {\n",
    "                cat(\"âŒ R data: Missing\\\\n\")\n",
    "            }\n",
    "            ''')\n",
    "        except:\n",
    "            print(\"âŒ R data: Transfer failed\")\n",
    "            issues.append(\"r_data_missing\")\n",
    "    \n",
    "    # 4. Check configuration\n",
    "    config_ok = all(var in globals() for var in ['DATA_SOURCE', 'VERBOSE_PRINTS'])\n",
    "    print(f\"{'âœ…' if config_ok else 'âŒ'} Configuration: {'Set' if config_ok else 'Missing'}\")\n",
    "    if not config_ok:\n",
    "        issues.append(\"config_missing\")\n",
    "    \n",
    "    # 5. Check memory usage\n",
    "    try:\n",
    "        import psutil\n",
    "        memory_pct = psutil.virtual_memory().percent\n",
    "        memory_ok = memory_pct < 90\n",
    "        print(f\"{'âœ…' if memory_ok else 'âš ï¸ '} Memory: {memory_pct:.1f}% used\")\n",
    "        if not memory_ok:\n",
    "            issues.append(\"high_memory\")\n",
    "    except:\n",
    "        print(\"? Memory: Cannot check\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if not issues:\n",
    "        print(\"âœ… All systems operational - Ready for analysis!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"âš ï¸  {len(issues)} issues detected\")\n",
    "        return issues\n",
    "\n",
    "def quick_recovery():\n",
    "    \"\"\"Simple recovery for common issues\"\"\"\n",
    "    print(\"ðŸ”§ ATTEMPTING RECOVERY...\")\n",
    "    \n",
    "    # Restore config if missing\n",
    "    if 'DATA_SOURCE' not in globals():\n",
    "        globals()['DATA_SOURCE'] = \"github\"\n",
    "        globals()['VERBOSE_PRINTS'] = True\n",
    "        print(\"   âœ… Configuration restored\")\n",
    "    \n",
    "    # Reload data if missing\n",
    "    if 'df' not in globals():\n",
    "        try:\n",
    "            # Try to reload from cache\n",
    "            from pathlib import Path\n",
    "            cache_file = Path.home() / 'data' / 'nass_data_github.csv'\n",
    "            if cache_file.exists():\n",
    "                import pandas as pd\n",
    "                globals()['df'] = pd.read_csv(cache_file)\n",
    "                print(f\"   âœ… Data reloaded: {globals()['df'].shape[0]:,} rows\")\n",
    "            else:\n",
    "                print(\"   âŒ No cached data found - run data loading section\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Data reload failed: {e}\")\n",
    "    \n",
    "    # Restore R data if possible\n",
    "    if globals().get('R_AVAILABLE', True) and 'df' in globals():\n",
    "        try:\n",
    "            get_ipython().run_cell_magic('R', '-i df', '''\n",
    "            if(!exists(\"NASS\") && exists(\"df\")) {\n",
    "                library(data.table)\n",
    "                NASS <- as.data.table(df)\n",
    "                cat(\"   âœ… R data restored\\\\n\")\n",
    "            }\n",
    "            ''')\n",
    "        except:\n",
    "            print(\"   âŒ R data restore failed\")\n",
    "    \n",
    "    # Clear memory\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(\"   âœ… Memory cleaned\")\n",
    "\n",
    "def mini_reset():\n",
    "    \"\"\"Clear variables but keep essential functions\"\"\"\n",
    "    keep = ['quick_health_check', 'quick_recovery', 'mini_reset', \n",
    "            'DATA_SOURCE', 'VERBOSE_PRINTS', 'GITHUB_URL']\n",
    "    \n",
    "    cleared = 0\n",
    "    for var in list(globals().keys()):\n",
    "        if not var.startswith('_') and var not in keep:\n",
    "            try:\n",
    "                del globals()[var]\n",
    "                cleared += 1\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(f\"ðŸ§¹ Cleared {cleared} variables\")\n",
    "    print(\"ðŸ’¡ Re-run setup cells to restore environment\")\n",
    "\n",
    "# Run check\n",
    "issues = quick_health_check()\n",
    "\n",
    "if issues and issues != True:\n",
    "    response = input(\"\\nAttempt recovery? (y/n): \").lower()\n",
    "    if response == 'y':\n",
    "        quick_recovery()\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(\"Re-checking after recovery...\")\n",
    "        quick_health_check()\n",
    "\n",
    "print(f\"\\nðŸŽ® Quick actions available:\")\n",
    "print(f\"   quick_recovery() - Fix common issues\")\n",
    "print(f\"   mini_reset() - Clear environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43230586"
   },
   "source": [
    "---\n",
    "\n",
    "## 1. Dataset Overview and Summary\n",
    "\n",
    "Generate comprehensive summary statistics and overview of the NASS dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Simple, reliable summary table\n",
    "cat(\"=== NASS 2020 DATASET SUMMARY ===\\n\")\n",
    "cat(\"Total observations:\", nrow(NASS), \"\\n\")\n",
    "cat(\"Total variables:\", ncol(NASS), \"\\n\\n\")\n",
    "\n",
    "# Key variables for summary\n",
    "summary_vars <- c(\"AGE\", \"FEMALE\", \"RACE\", \"ZIPINC_QRTL\", \"PAY1\", \n",
    "                 \"HOSP_LOCATION\", \"HOSP_TEACH\", \"HOSP_REGION\")\n",
    "\n",
    "available_vars <- summary_vars[summary_vars %in% names(NASS)]\n",
    "\n",
    "for(var in available_vars) {\n",
    "  cat(\"-----------------------------------------------\\n\")\n",
    "  cat(\"Variable:\", var, \"\\n\")\n",
    "  \n",
    "  if(is.numeric(NASS[[var]])) {\n",
    "    # Numeric variable summary\n",
    "    var_summary <- summary(NASS[[var]])\n",
    "    cat(\"  Type: Continuous\\n\")\n",
    "    cat(\"  Mean (SD):\", round(mean(NASS[[var]], na.rm = TRUE), 1), \n",
    "        \"(\", round(sd(NASS[[var]], na.rm = TRUE), 1), \")\\n\")\n",
    "    cat(\"  Median [IQR]:\", round(median(NASS[[var]], na.rm = TRUE), 1),\n",
    "        \"[\", round(quantile(NASS[[var]], 0.25, na.rm = TRUE), 1), \"-\",\n",
    "        round(quantile(NASS[[var]], 0.75, na.rm = TRUE), 1), \"]\\n\")\n",
    "    cat(\"  Range:\", round(min(NASS[[var]], na.rm = TRUE), 1), \"to\", \n",
    "        round(max(NASS[[var]], na.rm = TRUE), 1), \"\\n\")\n",
    "    cat(\"  Missing:\", sum(is.na(NASS[[var]])), \"observations\\n\")\n",
    "    \n",
    "  } else {\n",
    "    # Categorical variable summary\n",
    "    freq_table <- table(NASS[[var]], useNA = \"ifany\")\n",
    "    total_n <- sum(freq_table)\n",
    "    \n",
    "    cat(\"  Type: Categorical\\n\")\n",
    "    cat(\"  Levels:\", length(freq_table), \"\\n\")\n",
    "    \n",
    "    # Show top categories (up to 10)\n",
    "    sorted_freq <- sort(freq_table, decreasing = TRUE)\n",
    "    max_show <- min(10, length(sorted_freq))\n",
    "    \n",
    "    for(i in 1:max_show) {\n",
    "      cat(\"    \", names(sorted_freq)[i], \":\", sorted_freq[i], \n",
    "          \"(\", round(100 * sorted_freq[i] / total_n, 1), \"%)\\n\")\n",
    "    }\n",
    "    \n",
    "    if(length(sorted_freq) > max_show) {\n",
    "      cat(\"    ... and\", length(sorted_freq) - max_show, \"more categories\\n\")\n",
    "    }\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "cat(\"=== SUMMARY COMPLETE ===\\n\")\n",
    "\n",
    "cat(\"\\nSummary complete - ready for detailed analysis\\n\")\n",
    "flush.console()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Install and load required packages for comprehensive analysis\n",
    "required_packages <- c(\"ggplot2\", \"data.table\", \"scales\", \"RColorBrewer\")\n",
    "\n",
    "for(pkg in required_packages) {\n",
    "  if(!require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "    cat(\"Installing\", pkg, \"...\\n\")\n",
    "    install.packages(pkg, quiet = TRUE)\n",
    "    library(pkg, character.only = TRUE, quietly = TRUE)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Set consistent theme for all visualizations\n",
    "theme_nass <- theme_minimal() +\n",
    "  theme(\n",
    "    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n",
    "    plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray40\"),\n",
    "    axis.title = element_text(size = 11, face = \"bold\"),\n",
    "    axis.text = element_text(size = 10),\n",
    "    legend.title = element_text(size = 11, face = \"bold\"),\n",
    "    legend.text = element_text(size = 10),\n",
    "    strip.text = element_text(size = 10, face = \"bold\"),\n",
    "    panel.grid.minor = element_blank()\n",
    "  )\n",
    "\n",
    "# Define consistent color palettes\n",
    "race_colors <- RColorBrewer::brewer.pal(6, \"Set2\")\n",
    "pay_colors <- RColorBrewer::brewer.pal(6, \"Dark2\")\n",
    "region_colors <- RColorBrewer::brewer.pal(4, \"Set1\")\n",
    "\n",
    "cat(\"Setup complete - consistent theme and colors defined\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hospital Characteristics Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hospital Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Hospital distribution by region and characteristics\n",
    "if(all(c(\"HOSP_REGION\", \"HOSP_BEDSIZE_CAT\", \"HOSP_LOCATION\", \"HOSP_TEACH\") %in% names(NASS))) {\n",
    "  \n",
    "  # Get unique hospital characteristics\n",
    "  hospital_chars <- unique(NASS[, .(HOSP_NASS, HOSP_LOCATION, HOSP_TEACH, \n",
    "                                   HOSP_REGION, HOSP_BEDSIZE_CAT)])\n",
    "  \n",
    "  # Define labels\n",
    "  bed_labels <- c(\"1\" = \"Small (0-99)\", \"2\" = \"Medium (100-299)\", \"3\" = \"Large (300+)\")\n",
    "  region_labels <- c(\"1\" = \"Northeast\", \"2\" = \"Midwest\", \"3\" = \"South\", \"4\" = \"West\")\n",
    "  teach_labels <- c(\"0\" = \"Non-Teaching\", \"1\" = \"Teaching\")\n",
    "  location_labels <- c(\"0\" = \"Rural\", \"1\" = \"Urban\")\n",
    "  \n",
    "  p1 <- ggplot(hospital_chars, aes(x = factor(HOSP_REGION), fill = factor(HOSP_BEDSIZE_CAT))) + \n",
    "    geom_bar(alpha = 0.8, color = \"white\", size = 0.3) + \n",
    "    theme_nass + \n",
    "    labs(\n",
    "      x = \"US Region\", \n",
    "      y = \"Number of Hospitals\",\n",
    "      title = \"Hospital Distribution in NASS 2020 Dataset\", \n",
    "      subtitle = \"By Region, Location, Teaching Status, and Bed Size\",\n",
    "      fill = \"Bed Size Category\"\n",
    "    ) + \n",
    "    scale_fill_manual(values = region_colors, labels = bed_labels) + \n",
    "    scale_x_discrete(labels = region_labels) +\n",
    "    facet_grid(HOSP_LOCATION ~ HOSP_TEACH, \n",
    "               labeller = labeller(HOSP_TEACH = teach_labels, \n",
    "                                 HOSP_LOCATION = location_labels)) +\n",
    "    theme(legend.position = \"bottom\")\n",
    "  \n",
    "  print(p1)\n",
    "  \n",
    "  cat(\"Hospital distribution plot generated for\", nrow(hospital_chars), \"hospitals\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hospital Volume Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#  Hospital encounter volume distribution\n",
    "if(\"TOTAL_AS_ENCOUNTERS\" %in% names(NASS)) {\n",
    "  \n",
    "  hospital_volumes <- unique(NASS[, .(HOSP_NASS, HOSP_LOCATION, HOSP_TEACH, \n",
    "                                     HOSP_REGION, TOTAL_AS_ENCOUNTERS)])\n",
    "  \n",
    "  p2 <- ggplot(hospital_volumes, aes(x = factor(HOSP_REGION), y = TOTAL_AS_ENCOUNTERS)) + \n",
    "    geom_boxplot(aes(fill = factor(HOSP_REGION)), alpha = 0.7, outlier.alpha = 0.6) + \n",
    "    theme_nass + \n",
    "    labs(\n",
    "      x = \"US Region\", \n",
    "      y = \"Total Ambulatory Surgery Encounters\",\n",
    "      title = \"Hospital Ambulatory Surgery Volume Distribution\", \n",
    "      subtitle = \"By Region, Location, and Teaching Status\"\n",
    "    ) + \n",
    "    scale_fill_manual(values = region_colors, labels = region_labels, guide = \"none\") + \n",
    "    scale_x_discrete(labels = region_labels) +\n",
    "    scale_y_continuous(labels = comma_format()) +\n",
    "    facet_grid(HOSP_LOCATION ~ HOSP_TEACH, \n",
    "               labeller = labeller(HOSP_TEACH = teach_labels, \n",
    "                                 HOSP_LOCATION = location_labels))\n",
    "  \n",
    "  print(p2)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Procedures Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Procedures Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Calculate and display top 10 procedures\n",
    "if(\"CPTCCS1\" %in% names(NASS)) {\n",
    "  \n",
    "  # Calculate top procedures\n",
    "  top_procedures <- NASS[, .N, by = CPTCCS1][order(-N)][1:10]\n",
    "  TopCPT <- top_procedures$CPTCCS1\n",
    "  \n",
    "  cat(\"Top 10 procedures (CPTCCS1):\\n\")\n",
    "  print(top_procedures)\n",
    "  \n",
    "  # Calculate coverage\n",
    "  coverage <- sum(top_procedures$N) / nrow(NASS)\n",
    "  cat(\"\\nTop 10 procedures represent\", round(coverage * 100, 1), \"% of all procedures\\n\")\n",
    "  \n",
    "  # Create subset for detailed analysis\n",
    "  NASS_top_procedures <- NASS[CPTCCS1 %in% TopCPT]\n",
    "  cat(\"Created subset with\", nrow(NASS_top_procedures), \"records for top procedures\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedures by Income Quartile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Top procedures by income quartile\n",
    "if(exists(\"NASS_top_procedures\") && \"ZIPINC_QRTL\" %in% names(NASS)) {\n",
    "  \n",
    "  plot_data <- NASS_top_procedures[ZIPINC_QRTL %in% 1:4]\n",
    "  \n",
    "  if(nrow(plot_data) > 0) {\n",
    "    # Order procedures by frequency\n",
    "    proc_order <- plot_data[, .N, by = CPTCCS1][order(-N)]$CPTCCS1\n",
    "    plot_data[, CPTCCS1 := factor(CPTCCS1, levels = rev(proc_order))]\n",
    "    \n",
    "    p3 <- ggplot(plot_data, aes(x = CPTCCS1, fill = factor(ZIPINC_QRTL))) + \n",
    "      geom_bar(alpha = 0.8, color = \"white\", size = 0.2) + \n",
    "      coord_flip() +\n",
    "      theme_nass + \n",
    "      labs(\n",
    "        x = \"CCS Procedure Codes\", \n",
    "        y = \"Number of Procedures\",\n",
    "        title = \"Most Common Ambulatory Surgery Procedures\", \n",
    "        subtitle = \"Distribution by Patient Income Quartile (by ZIP code)\",\n",
    "        fill = \"Income Quartile\"\n",
    "      ) +\n",
    "      scale_fill_manual(values = pay_colors[1:4], \n",
    "                       labels = c(\"Q1 (Lowest)\", \"Q2\", \"Q3\", \"Q4 (Highest)\")) +\n",
    "      theme(legend.position = \"bottom\")\n",
    "    \n",
    "    print(p3)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedures by Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Top procedures by race\n",
    "if(exists(\"NASS_top_procedures\") && \"RACE\" %in% names(NASS)) {\n",
    "  \n",
    "  plot_data <- NASS_top_procedures[RACE %in% 1:6]\n",
    "  \n",
    "  if(nrow(plot_data) > 0) {\n",
    "    # Order procedures by frequency\n",
    "    proc_order <- plot_data[, .N, by = CPTCCS1][order(-N)]$CPTCCS1\n",
    "    plot_data[, CPTCCS1 := factor(CPTCCS1, levels = rev(proc_order))]\n",
    "    \n",
    "    race_labels <- c(\"1\" = \"White\", \"2\" = \"Black\", \"3\" = \"Hispanic\", \n",
    "                    \"4\" = \"Asian/Pacific\", \"5\" = \"Native American\", \"6\" = \"Other\")\n",
    "    \n",
    "    p4 <- ggplot(plot_data, aes(x = CPTCCS1, fill = factor(RACE))) + \n",
    "      geom_bar(alpha = 0.8, color = \"white\", size = 0.2) + \n",
    "      coord_flip() +\n",
    "      theme_nass + \n",
    "      labs(\n",
    "        x = \"CCS Procedure Codes\", \n",
    "        y = \"Number of Procedures\",\n",
    "        title = \"Most Common Ambulatory Surgery Procedures\", \n",
    "        subtitle = \"Distribution by Patient Race/Ethnicity\",\n",
    "        fill = \"Race/Ethnicity\"\n",
    "      ) + \n",
    "      scale_fill_manual(values = race_colors, labels = race_labels) +\n",
    "      theme(legend.position = \"bottom\")\n",
    "    \n",
    "    print(p4)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patient Characteristics Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age Distribution by Race and Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Age distribution by race and region - COUNT CURVES instead of density\n",
    "if(all(c(\"AGE\", \"RACE\", \"HOSP_REGION\", \"PL_NCHS\") %in% names(NASS))) {\n",
    "  \n",
    "  age_data <- NASS[RACE %in% 1:6 & PL_NCHS %in% 1:6 & AGE >= 0 & AGE <= 100]\n",
    "  \n",
    "  if(nrow(age_data) > 0) {\n",
    "    \n",
    "    race_labels <- c(\"1\" = \"White\", \"2\" = \"Black\", \"3\" = \"Hispanic\", \n",
    "                    \"4\" = \"Asian/Pacific\", \"5\" = \"Native American\", \"6\" = \"Other\")\n",
    "    region_labels <- c(\"1\" = \"Northeast\", \"2\" = \"Midwest\", \"3\" = \"South\", \"4\" = \"West\")\n",
    "    pl_nchs_labels <- c(\"1\" = \"Large Central\", \"2\" = \"Large Fringe\", \"3\" = \"Medium\", \n",
    "                       \"4\" = \"Small\", \"5\" = \"Micro\", \"6\" = \"Non-core\")\n",
    "    \n",
    "    p5 <- ggplot(age_data, aes(x = AGE)) + \n",
    "      geom_density(aes(fill = factor(RACE), y = after_stat(count)), alpha = 0.7, color = \"white\", \n",
    "                   linewidth = 0.2) + \n",
    "      theme_nass + \n",
    "      labs(\n",
    "        x = \"Age (years)\", \n",
    "        y = \"Count\",\n",
    "        title = \"Patient Age Distribution in NASS 2020\", \n",
    "        subtitle = \"By Urban-Rural Classification and US Region, Segmented by Race\",\n",
    "        fill = \"Race/Ethnicity\"\n",
    "      ) + \n",
    "      scale_fill_manual(values = race_colors, labels = race_labels) + \n",
    "      xlim(0, 100) +\n",
    "      facet_grid(HOSP_REGION ~ PL_NCHS, \n",
    "                 labeller = labeller(HOSP_REGION = region_labels, PL_NCHS = pl_nchs_labels)) + \n",
    "      theme(legend.position = \"bottom\", \n",
    "            strip.text = element_text(size = 8))\n",
    "    \n",
    "    print(p5)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age Distribution by Payer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Age distribution by payer and region \n",
    "if(all(c(\"AGE\", \"PAY1\", \"HOSP_REGION\", \"PL_NCHS\") %in% names(NASS))) {\n",
    "  \n",
    "  age_payer_data <- NASS[PAY1 %in% 1:6 & PL_NCHS %in% 1:6 & AGE >= 0 & AGE <= 100]\n",
    "  \n",
    "  if(nrow(age_payer_data) > 0) {\n",
    "    \n",
    "    pay_labels <- c(\"1\" = \"Medicare\", \"2\" = \"Medicaid\", \"3\" = \"Private\", \n",
    "                   \"4\" = \"Self-pay\", \"5\" = \"No Charge\", \"6\" = \"Other\")\n",
    "    \n",
    "    p6 <- ggplot(age_payer_data, aes(x = AGE)) + \n",
    "      geom_density(aes(fill = factor(PAY1), y = after_stat(count)), alpha = 0.7, color = \"white\", \n",
    "                   linewidth = 0.2) + \n",
    "      theme_nass + \n",
    "      labs(\n",
    "        x = \"Age (years)\", \n",
    "        y = \"Count\",\n",
    "        title = \"Patient Age Distribution in NASS 2020\", \n",
    "        subtitle = \"By Urban-Rural Classification and US Region, Segmented by Payer\",\n",
    "        fill = \"Primary Payer\"\n",
    "      ) + \n",
    "      scale_fill_manual(values = pay_colors, labels = pay_labels) + \n",
    "      xlim(0, 100) +\n",
    "      facet_grid(HOSP_REGION ~ PL_NCHS, \n",
    "                 labeller = labeller(HOSP_REGION = region_labels, PL_NCHS = pl_nchs_labels)) + \n",
    "      theme(legend.position = \"bottom\",\n",
    "            strip.text = element_text(size = 8))\n",
    "    \n",
    "    print(p6)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Payer Distribution by Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Payer distribution by race and age group\n",
    "if(all(c(\"RACE\", \"PAY1\", \"AGE\") %in% names(NASS))) {\n",
    "  \n",
    "  # Create better age groups: 0-17, 18-64, 65+\n",
    "  plot_data <- NASS[RACE %in% 1:6 & PAY1 %in% 1:6 & !is.na(AGE)]\n",
    "  \n",
    "  if(nrow(plot_data) > 0) {\n",
    "    \n",
    "    # Create simplified age groups\n",
    "    plot_data[, AGE_GROUP_SIMPLE := cut(AGE, \n",
    "                                       breaks = c(-Inf, 17, 64, Inf),\n",
    "                                       labels = c(\"0-17\", \"18-64\", \"65+\"),\n",
    "                                       right = TRUE)]\n",
    "    \n",
    "    # Remove any missing age groups\n",
    "    plot_data <- plot_data[!is.na(AGE_GROUP_SIMPLE)]\n",
    "    \n",
    "    # Define cleaner labels\n",
    "    race_labels <- c(\"1\" = \"White\", \"2\" = \"Black\", \"3\" = \"Hispanic\", \n",
    "                    \"4\" = \"Asian/Pacific\", \"5\" = \"Native American\", \"6\" = \"Other\")\n",
    "    pay_labels <- c(\"1\" = \"Medicare\", \"2\" = \"Medicaid\", \"3\" = \"Private\", \n",
    "                   \"4\" = \"Self-pay\", \"5\" = \"No Charge\", \"6\" = \"Other\")\n",
    "    \n",
    "    p7 <- ggplot(plot_data, aes(x = factor(RACE), fill = factor(PAY1))) + \n",
    "      geom_bar(position = \"fill\", alpha = 0.8, color = \"white\", linewidth = 0.2) + \n",
    "      theme_nass + \n",
    "      labs(\n",
    "        x = \"Race/Ethnicity\", \n",
    "        y = \"Proportion\",\n",
    "        title = \"Primary Payer Distribution by Demographics\", \n",
    "        subtitle = \"Insurance Coverage Patterns by Age Group and Race/Ethnicity\",\n",
    "        fill = \"Primary Payer\"\n",
    "      ) + \n",
    "      scale_fill_manual(values = pay_colors, labels = pay_labels) + \n",
    "      scale_x_discrete(labels = race_labels) +\n",
    "      scale_y_continuous(labels = percent_format()) +\n",
    "      facet_wrap(~ AGE_GROUP_SIMPLE, ncol = 3) +\n",
    "      theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 9),\n",
    "            legend.position = \"bottom\",\n",
    "            strip.text = element_text(size = 11, face = \"bold\"),\n",
    "            panel.spacing = unit(1, \"lines\"))\n",
    "    \n",
    "    print(p7)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Payer Distribution by Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Payer distribution by income quartile and age group\n",
    "if(all(c(\"ZIPINC_QRTL\", \"PAY1\", \"AGE\") %in% names(NASS))) {\n",
    "  \n",
    "  # Create data with income quartiles and simplified age groups\n",
    "  plot_data <- NASS[ZIPINC_QRTL %in% 1:4 & PAY1 %in% 1:6 & !is.na(AGE)]\n",
    "  \n",
    "  if(nrow(plot_data) > 0) {\n",
    "    \n",
    "    # Create simplified age groups: 0-17, 18-64, 65+\n",
    "    plot_data[, AGE_GROUP_SIMPLE := cut(AGE, \n",
    "                                       breaks = c(-Inf, 17, 64, Inf),\n",
    "                                       labels = c(\"0-17\", \"18-64\", \"65+\"),\n",
    "                                       right = TRUE)]\n",
    "    \n",
    "    # Remove any missing age groups\n",
    "    plot_data <- plot_data[!is.na(AGE_GROUP_SIMPLE)]\n",
    "    \n",
    "    # Define cleaner labels\n",
    "    income_labels <- c(\"1\" = \"Q1 (Lowest)\", \"2\" = \"Q2\", \"3\" = \"Q3\", \"4\" = \"Q4 (Highest)\")\n",
    "    pay_labels <- c(\"1\" = \"Medicare\", \"2\" = \"Medicaid\", \"3\" = \"Private\", \n",
    "                   \"4\" = \"Self-pay\", \"5\" = \"No Charge\", \"6\" = \"Other\")\n",
    "    \n",
    "    p8 <- ggplot(plot_data, aes(x = factor(ZIPINC_QRTL), fill = factor(PAY1))) + \n",
    "      geom_bar(position = \"fill\", alpha = 0.8, color = \"white\", linewidth = 0.2) + \n",
    "      theme_nass + \n",
    "      labs(\n",
    "        x = \"Income Quartile (by ZIP Code)\", \n",
    "        y = \"Proportion\",\n",
    "        title = \"Primary Payer Distribution by Socioeconomic Status\", \n",
    "        subtitle = \"Insurance Coverage Patterns by Age Group and Income Quartile\",\n",
    "        fill = \"Primary Payer\"\n",
    "      ) + \n",
    "      scale_fill_manual(values = pay_colors, labels = pay_labels) + \n",
    "      scale_x_discrete(labels = income_labels) +\n",
    "      scale_y_continuous(labels = percent_format()) +\n",
    "      facet_wrap(~ AGE_GROUP_SIMPLE, ncol = 3) +\n",
    "      theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 9),\n",
    "            legend.position = \"bottom\",\n",
    "            strip.text = element_text(size = 11, face = \"bold\"),\n",
    "            panel.spacing = unit(1, \"lines\"))\n",
    "    \n",
    "    print(p8)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehensive Dataset Summary\n",
    "\n",
    "Final summary statistics providing an overview of all key variables in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# ===============================================\n",
    "# COMPREHENSIVE NASS 2020 DATASET SUMMARY\n",
    "# ===============================================\n",
    "\n",
    "cat(\"================================================================\\n\")\n",
    "cat(\"                    NASS 2020 DATASET SUMMARY                  \\n\")\n",
    "cat(\"================================================================\\n\\n\")\n",
    "\n",
    "# Dataset Overview\n",
    "cat(\"DATASET OVERVIEW\\n\")\n",
    "cat(\"================\\n\")\n",
    "cat(\"Total observations:\", format(nrow(NASS), big.mark = \",\"), \"\\n\")\n",
    "cat(\"Total variables:\", ncol(NASS), \"\\n\")\n",
    "cat(\"Memory usage:\", round(object.size(NASS)/1024^2, 1), \"MB\\n\")\n",
    "cat(\"Missing data patterns:\", sum(is.na(NASS)), \"total missing values\\n\\n\")\n",
    "\n",
    "# Key variables for comprehensive summary\n",
    "summary_vars <- c(\"AGE\", \"FEMALE\", \"RACE\", \"WHITE\", \"ZIPINC_QRTL\", \"PAY1\", \n",
    "                 \"HOSP_LOCATION\", \"HOSP_TEACH\", \"HOSP_REGION\", \"HOSP_BEDSIZE_CAT\",\n",
    "                 \"PL_NCHS\", \"CPTCCS1\", \"TOTCHG\", \"DISCWT\")\n",
    "\n",
    "available_vars <- summary_vars[summary_vars %in% names(NASS)]\n",
    "missing_vars <- summary_vars[!summary_vars %in% names(NASS)]\n",
    "\n",
    "if(length(missing_vars) > 0) {\n",
    "  cat(\"WARNING: Variables not available:\", paste(missing_vars, collapse = \", \"), \"\\n\\n\")\n",
    "}\n",
    "\n",
    "# Variable-by-variable analysis\n",
    "for(var in available_vars) {\n",
    "  cat(\"----------------------------------------------------------------\\n\")\n",
    "  cat(\"Variable:\", toupper(var), \"\\n\")\n",
    "  cat(\"----------------------------------------------------------------\\n\")\n",
    "  \n",
    "  if(is.numeric(NASS[[var]])) {\n",
    "    # Enhanced numeric variable summary\n",
    "    valid_values <- NASS[[var]][!is.na(NASS[[var]])]\n",
    "    missing_count <- sum(is.na(NASS[[var]]))\n",
    "    \n",
    "    cat(\"Type: Continuous/Numeric\\n\")\n",
    "    cat(\"Valid observations:\", format(length(valid_values), big.mark = \",\"), \n",
    "        \"(\", round(100 * length(valid_values) / nrow(NASS), 1), \"%)\\n\")\n",
    "    cat(\"Missing values:\", format(missing_count, big.mark = \",\"), \n",
    "        \"(\", round(100 * missing_count / nrow(NASS), 1), \"%)\\n\")\n",
    "    \n",
    "    if(length(valid_values) > 0) {\n",
    "      # Central tendency\n",
    "      cat(\"\\nCENTRAL TENDENCY:\\n\")\n",
    "      cat(\"   Mean:\", format(round(mean(valid_values), 2), big.mark = \",\"), \"\\n\")\n",
    "      cat(\"   Median:\", format(round(median(valid_values), 2), big.mark = \",\"), \"\\n\")\n",
    "      cat(\"   Mode region:\", format(round(median(valid_values), 2), big.mark = \",\"), \n",
    "          \" +/-\", format(round(mad(valid_values), 2), big.mark = \",\"), \"\\n\")\n",
    "      \n",
    "      # Variability\n",
    "      cat(\"\\nVARIABILITY:\\n\")\n",
    "      cat(\"   Standard Dev:\", format(round(sd(valid_values), 2), big.mark = \",\"), \"\\n\")\n",
    "      cat(\"   IQR:\", format(round(quantile(valid_values, 0.25), 2), big.mark = \",\"), \"to\", \n",
    "          format(round(quantile(valid_values, 0.75), 2), big.mark = \",\"), \"\\n\")\n",
    "      cat(\"   Range:\", format(round(min(valid_values), 2), big.mark = \",\"), \"to\", \n",
    "          format(round(max(valid_values), 2), big.mark = \",\"), \"\\n\")\n",
    "      \n",
    "      # Distribution shape\n",
    "      cat(\"\\nDISTRIBUTION:\\n\")\n",
    "      q1 <- quantile(valid_values, 0.25)\n",
    "      q3 <- quantile(valid_values, 0.75)\n",
    "      skewness_approx <- (mean(valid_values) - median(valid_values)) / sd(valid_values)\n",
    "      \n",
    "      cat(\"   Skewness (approx):\", round(skewness_approx, 3), \n",
    "          ifelse(abs(skewness_approx) < 0.5, \"(approximately symmetric)\", \n",
    "                ifelse(skewness_approx > 0, \"(right-skewed)\", \"(left-skewed)\")), \"\\n\")\n",
    "      \n",
    "      # Outliers (using IQR method)\n",
    "      iqr <- q3 - q1\n",
    "      lower_fence <- q1 - 1.5 * iqr\n",
    "      upper_fence <- q3 + 1.5 * iqr\n",
    "      outliers <- sum(valid_values < lower_fence | valid_values > upper_fence)\n",
    "      cat(\"   Potential outliers:\", outliers, \n",
    "          \"(\", round(100 * outliers / length(valid_values), 1), \"%)\\n\")\n",
    "      \n",
    "      # Percentile breakdown for key variables\n",
    "      if(var %in% c(\"AGE\", \"TOTCHG\", \"DISCWT\")) {\n",
    "        cat(\"\\nPERCENTILE BREAKDOWN:\\n\")\n",
    "        percentiles <- c(0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99)\n",
    "        for(p in percentiles) {\n",
    "          cat(\"   P\", round(p*100), \":\", format(round(quantile(valid_values, p), 1), big.mark = \",\"), \"\\n\")\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \n",
    "  } else {\n",
    "    # Enhanced categorical variable summary\n",
    "    freq_table <- table(NASS[[var]], useNA = \"ifany\")\n",
    "    total_n <- sum(freq_table)\n",
    "    missing_count <- sum(is.na(NASS[[var]]))\n",
    "    \n",
    "    cat(\"Type: Categorical/Factor\\n\")\n",
    "    cat(\"Total categories:\", length(freq_table), \"\\n\")\n",
    "    cat(\"Valid observations:\", format(total_n - missing_count, big.mark = \",\"), \n",
    "        \"(\", round(100 * (total_n - missing_count) / nrow(NASS), 1), \"%)\\n\")\n",
    "    \n",
    "    if(missing_count > 0) {\n",
    "      cat(\"Missing values:\", format(missing_count, big.mark = \",\"), \n",
    "          \"(\", round(100 * missing_count / nrow(NASS), 1), \"%)\\n\")\n",
    "    }\n",
    "    \n",
    "    # Sort by frequency\n",
    "    sorted_freq <- sort(freq_table, decreasing = TRUE)\n",
    "    \n",
    "    # Show distribution\n",
    "    cat(\"\\nFREQUENCY DISTRIBUTION:\\n\")\n",
    "    max_show <- min(15, length(sorted_freq))  # Show more categories\n",
    "    \n",
    "    for(i in 1:max_show) {\n",
    "      category_name <- names(sorted_freq)[i]\n",
    "      count <- sorted_freq[i]\n",
    "      percentage <- round(100 * count / total_n, 1)\n",
    "      \n",
    "      # Create a simple bar visualization\n",
    "      bar_length <- min(20, round(20 * count / max(sorted_freq)))\n",
    "      bar <- paste(rep(\"*\", bar_length), collapse = \"\")\n",
    "      \n",
    "      cat(\"   \", sprintf(\"%-20s\", category_name), \":\", \n",
    "          sprintf(\"%8s\", format(count, big.mark = \",\")), \n",
    "          sprintf(\"(%5.1f%%)\", percentage), \" \", bar, \"\\n\")\n",
    "    }\n",
    "    \n",
    "    if(length(sorted_freq) > max_show) {\n",
    "      remaining_count <- sum(sorted_freq[(max_show+1):length(sorted_freq)])\n",
    "      remaining_pct <- round(100 * remaining_count / total_n, 1)\n",
    "      cat(\"   ... \", length(sorted_freq) - max_show, \" more categories:\", \n",
    "          format(remaining_count, big.mark = \",\"), \"(\", remaining_pct, \"%)\\n\")\n",
    "    }\n",
    "    \n",
    "    # Diversity metrics\n",
    "    cat(\"\\nDIVERSITY METRICS:\\n\")\n",
    "    # Simpson's diversity index (1 - sum of squared proportions)\n",
    "    proportions <- as.numeric(freq_table) / sum(freq_table)\n",
    "    simpson_diversity <- 1 - sum(proportions^2)\n",
    "    cat(\"   Simpson's Diversity:\", round(simpson_diversity, 3), \n",
    "        \"(0=no diversity, 1=max diversity)\\n\")\n",
    "    \n",
    "    # Effective number of categories (inverse Simpson)\n",
    "    effective_categories <- 1 / sum(proportions^2)\n",
    "    cat(\"   Effective categories:\", round(effective_categories, 1), \n",
    "        \"out of\", length(freq_table), \"total\\n\")\n",
    "    \n",
    "    # Concentration ratio (top 3 categories)\n",
    "    top3_concentration <- sum(sorted_freq[1:min(3, length(sorted_freq))]) / total_n\n",
    "    cat(\"   Top-3 concentration:\", round(100 * top3_concentration, 1), \"%\\n\")\n",
    "    \n",
    "    # Special insights for key variables\n",
    "    if(var == \"RACE\") {\n",
    "      cat(\"\\nRACE/ETHNICITY INSIGHTS:\\n\")\n",
    "      cat(\"   Racial diversity reflects ambulatory surgery access patterns\\n\")\n",
    "      if(\"1\" %in% names(freq_table)) {\n",
    "        white_vs_nonwhite <- round(freq_table[\"1\"] / sum(freq_table[names(freq_table) != \"1\"]), 2)\n",
    "        cat(\"   White vs Non-White ratio:\", white_vs_nonwhite, \":1\\n\")\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    if(var == \"PAY1\") {\n",
    "      cat(\"\\nPAYER MIX INSIGHTS:\\n\")\n",
    "      public_payers <- sum(freq_table[c(\"1\", \"2\")], na.rm = TRUE)  # Medicare + Medicaid\n",
    "      cat(\"   Public insurance coverage:\", \n",
    "          round(100 * public_payers / total_n, 1), \"%\\n\")\n",
    "    }\n",
    "    \n",
    "    if(var == \"ZIPINC_QRTL\") {\n",
    "      cat(\"\\nINCOME DISTRIBUTION INSIGHTS:\\n\")\n",
    "      low_income <- sum(freq_table[c(\"1\", \"2\")], na.rm = TRUE)  # Q1 + Q2\n",
    "      cat(\"   Lower-income representation:\", \n",
    "          round(100 * low_income / total_n, 1), \"%\\n\")\n",
    "    }\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Cross-tabulation insights\n",
    "cat(\"----------------------------------------------------------------\\n\")\n",
    "cat(\"KEY RELATIONSHIPS & CROSS-TABULATIONS\\n\")\n",
    "cat(\"----------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "# Race vs Payer cross-tab\n",
    "if(all(c(\"RACE\", \"PAY1\") %in% available_vars)) {\n",
    "  cat(\"RACE x PAYER DISTRIBUTION:\\n\")\n",
    "  cross_tab <- table(NASS$RACE, NASS$PAY1)\n",
    "  prop_tab <- round(100 * prop.table(cross_tab, 1), 1)\n",
    "  \n",
    "  race_labels <- c(\"1\"=\"White\", \"2\"=\"Black\", \"3\"=\"Hispanic\", \"4\"=\"Asian/Pacific\", \"5\"=\"Native Am\", \"6\"=\"Other\")\n",
    "  pay_labels <- c(\"1\"=\"Medicare\", \"2\"=\"Medicaid\", \"3\"=\"Private\", \"4\"=\"Self-pay\", \"5\"=\"No Charge\", \"6\"=\"Other\")\n",
    "  \n",
    "  for(race in rownames(prop_tab)) {\n",
    "    if(race %in% names(race_labels)) {\n",
    "      cat(\"   \", race_labels[race], \"patients:\\n\")\n",
    "      race_row <- prop_tab[race, ]\n",
    "      sorted_payers <- sort(race_row, decreasing = TRUE)\n",
    "      for(i in 1:min(3, length(sorted_payers))) {\n",
    "        payer <- names(sorted_payers)[i]\n",
    "        if(payer %in% names(pay_labels)) {\n",
    "          cat(\"     \", pay_labels[payer], \":\", sorted_payers[i], \"%\\n\")\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Age vs Income relationship\n",
    "if(all(c(\"AGE\", \"ZIPINC_QRTL\") %in% available_vars)) {\n",
    "  cat(\"AGE x INCOME PATTERNS:\\n\")\n",
    "  age_income <- NASS[!is.na(AGE) & !is.na(ZIPINC_QRTL), .(mean_age = round(mean(AGE), 1)), by = ZIPINC_QRTL]\n",
    "  setorder(age_income, ZIPINC_QRTL)\n",
    "  \n",
    "  income_labels <- c(\"1\"=\"Q1 (Lowest)\", \"2\"=\"Q2\", \"3\"=\"Q3\", \"4\"=\"Q4 (Highest)\")\n",
    "  for(i in 1:nrow(age_income)) {\n",
    "    quartile <- as.character(age_income$ZIPINC_QRTL[i])\n",
    "    if(quartile %in% names(income_labels)) {\n",
    "      cat(\"   \", income_labels[quartile], \"- Average age:\", age_income$mean_age[i], \"years\\n\")\n",
    "    }\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Hospital characteristics summary\n",
    "if(all(c(\"HOSP_REGION\", \"HOSP_TEACH\", \"HOSP_LOCATION\") %in% available_vars)) {\n",
    "  cat(\"HOSPITAL CHARACTERISTICS:\\n\")\n",
    "  \n",
    "  # Unique hospital count\n",
    "  if(\"HOSP_NASS\" %in% names(NASS)) {\n",
    "    unique_hospitals <- length(unique(NASS$HOSP_NASS))\n",
    "    cat(\"   Total hospitals in sample:\", unique_hospitals, \"\\n\")\n",
    "  }\n",
    "  \n",
    "  # Teaching hospital distribution\n",
    "  teaching_dist <- table(NASS$HOSP_TEACH)\n",
    "  if(\"1\" %in% names(teaching_dist)) {\n",
    "    cat(\"   Teaching hospitals:\", round(100 * teaching_dist[\"1\"] / sum(teaching_dist), 1), \"%\\n\")\n",
    "  }\n",
    "  \n",
    "  # Urban/Rural distribution\n",
    "  location_dist <- table(NASS$HOSP_LOCATION)\n",
    "  if(\"1\" %in% names(location_dist)) {\n",
    "    cat(\"   Urban hospitals:\", round(100 * location_dist[\"1\"] / sum(location_dist), 1), \"%\\n\")\n",
    "  }\n",
    "  \n",
    "  # Regional distribution\n",
    "  region_dist <- table(NASS$HOSP_REGION)\n",
    "  region_labels <- c(\"1\"=\"Northeast\", \"2\"=\"Midwest\", \"3\"=\"South\", \"4\"=\"West\")\n",
    "  cat(\"   Regional distribution:\\n\")\n",
    "  for(region in names(region_dist)) {\n",
    "    if(region %in% names(region_labels)) {\n",
    "      pct <- round(100 * region_dist[region] / sum(region_dist), 1)\n",
    "      cat(\"     \", region_labels[region], \":\", pct, \"%\\n\")\n",
    "    }\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Data quality assessment\n",
    "cat(\"----------------------------------------------------------------\\n\")\n",
    "cat(\"DATA QUALITY ASSESSMENT\\n\")\n",
    "cat(\"----------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "# Missing data patterns\n",
    "total_cells <- nrow(NASS) * ncol(NASS)\n",
    "missing_cells <- sum(is.na(NASS))\n",
    "complete_cases <- sum(complete.cases(NASS))\n",
    "\n",
    "cat(\"COMPLETENESS METRICS:\\n\")\n",
    "cat(\"   Overall completeness:\", round(100 * (1 - missing_cells/total_cells), 2), \"%\\n\")\n",
    "cat(\"   Complete cases (no missing):\", format(complete_cases, big.mark = \",\"), \n",
    "    \"(\", round(100 * complete_cases / nrow(NASS), 1), \"%)\\n\")\n",
    "cat(\"   Variables with missing data:\", sum(sapply(NASS, function(x) any(is.na(x)))), \n",
    "    \"out of\", ncol(NASS), \"\\n\\n\")\n",
    "\n",
    "# Variables with highest missing rates\n",
    "missing_rates <- sapply(NASS, function(x) round(100 * sum(is.na(x)) / length(x), 1))\n",
    "high_missing <- missing_rates[missing_rates > 0]\n",
    "if(length(high_missing) > 0) {\n",
    "  cat(\"VARIABLES WITH MISSING DATA:\\n\")\n",
    "  sorted_missing <- sort(high_missing, decreasing = TRUE)\n",
    "  for(i in 1:min(10, length(sorted_missing))) {\n",
    "    cat(\"   \", names(sorted_missing)[i], \":\", sorted_missing[i], \"% missing\\n\")\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Survey weights summary\n",
    "if(\"DISCWT\" %in% available_vars) {\n",
    "  cat(\"SURVEY WEIGHTS SUMMARY:\\n\")\n",
    "  weights <- NASS$DISCWT[!is.na(NASS$DISCWT)]\n",
    "  cat(\"   Weight range:\", round(min(weights), 2), \"to\", round(max(weights), 2), \"\\n\")\n",
    "  cat(\"   Effective sample size:\", round(sum(weights)^2 / sum(weights^2)), \"\\n\")\n",
    "  cat(\"   Design effect (approx):\", round(nrow(NASS) / (sum(weights)^2 / sum(weights^2)), 2), \"\\n\\n\")\n",
    "}\n",
    "\n",
    "cat(\"================================================================\\n\")\n",
    "cat(\"                     SUMMARY COMPLETE                          \\n\")\n",
    "cat(\"     Dataset ready for advanced statistical analysis           \\n\")\n",
    "cat(\"================================================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Census Data Comparisons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20a6a8a8"
   },
   "source": [
    "### Census API Setup\n",
    "\n",
    "Set up Census API integration and pull 2020 DHC population data for comparison with NASS sample proportions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ Please Register at the Census website for an API Key ](https://api.census.gov/data/key_signup.html). Enter your Census API key for data retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6d0c69b9"
   },
   "outputs": [],
   "source": [
    "import getpass, os, json, textwrap\n",
    "os.environ[\"CENSUS_API_KEY\"] = getpass.getpass(\"Enter your Census API key (will not echo):\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3c39ad38"
   },
   "source": [
    "### Census Data Retrieval and Processing\n",
    "\n",
    "Pull 2020 DHC population data by age, gender, and race for states included in NASS sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "240fd569"
   },
   "outputs": [],
   "source": [
    "%%R -i VERBOSE_PRINTS\n",
    "\n",
    "# Install and load required packages for Census analysis\n",
    "required_packages <- c(\"tidycensus\", \"dplyr\", \"tidyr\", \"survey\")\n",
    "\n",
    "for(pkg in required_packages) {\n",
    "  if(!require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "    install.packages(pkg, quiet = TRUE)\n",
    "    library(pkg, character.only = TRUE, quietly = TRUE)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Set Census API key\n",
    "census_api_key(Sys.getenv(\"CENSUS_API_KEY\"), overwrite = FALSE, install = FALSE)\n",
    "\n",
    "# Define states included in NASS 2020 dataset\n",
    "states_in_nass <- c(\"Alaska\", \"California\", \"Colorado\", \"Connecticut\", \"District of Columbia\", \n",
    "                    \"Florida\", \"Georgia\", \"Hawaii\", \"Iowa\", \"Illinois\", \"Indiana\", \"Kansas\", \n",
    "                    \"Kentucky\", \"Maryland\", \"Maine\", \"Michigan\", \"Minnesota\", \"Missouri\", \n",
    "                    \"North Carolina\", \"North Dakota\", \"Nebraska\", \"New Jersey\", \"Nevada\", \n",
    "                    \"New York\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"South Carolina\", \n",
    "                    \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Wisconsin\")\n",
    "\n",
    "cat(\"Defined\", length(states_in_nass), \"states included in NASS sample\\n\")\n",
    "\n",
    "# Function to construct population variables for Census queries\n",
    "get_population_variables <- function(base_variable) {\n",
    "    variables <- paste0(base_variable, \"_\", sprintf(\"%03dN\", 1:49))\n",
    "    \n",
    "    labels <- c(\n",
    "        \"Total\",\n",
    "        \"Male: Total\", \"Male: Under 5 years\", \"Male: 5 to 9 years\", \"Male: 10 to 14 years\",\n",
    "        \"Male: 15 to 17 years\", \"Male: 18 and 19 years\", \"Male: 20 years\", \"Male: 21 years\",\n",
    "        \"Male: 22 to 24 years\", \"Male: 25 to 29 years\", \"Male: 30 to 34 years\", \"Male: 35 to 39 years\",\n",
    "        \"Male: 40 to 44 years\", \"Male: 45 to 49 years\", \"Male: 50 to 54 years\", \"Male: 55 to 59 years\",\n",
    "        \"Male: 60 and 61 years\", \"Male: 62 to 64 years\", \"Male: 65 and 66 years\", \"Male: 67 to 69 years\",\n",
    "        \"Male: 70 to 74 years\", \"Male: 75 to 79 years\", \"Male: 80 to 84 years\", \"Male: 85 years and over\",\n",
    "        \"Female: Total\", \"Female: Under 5 years\", \"Female: 5 to 9 years\", \"Female: 10 to 14 years\",\n",
    "        \"Female: 15 to 17 years\", \"Female: 18 and 19 years\", \"Female: 20 years\", \"Female: 21 years\",\n",
    "        \"Female: 22 to 24 years\", \"Female: 25 to 29 years\", \"Female: 30 to 34 years\", \"Female: 35 to 39 years\",\n",
    "        \"Female: 40 to 44 years\", \"Female: 45 to 49 years\", \"Female: 50 to 54 years\", \"Female: 55 to 59 years\",\n",
    "        \"Female: 60 and 61 years\", \"Female: 62 to 64 years\", \"Female: 65 and 66 years\", \"Female: 67 to 69 years\",\n",
    "        \"Female: 70 to 74 years\", \"Female: 75 to 79 years\", \"Female: 80 to 84 years\", \"Female: 85 years and over\"\n",
    "    )\n",
    "    \n",
    "    names(labels) <- variables\n",
    "    return(list(variables = variables, labels = labels))\n",
    "}\n",
    "\n",
    "# Function to get population data from Census\n",
    "get_population_data <- function(variables, labels) {\n",
    "    population_data <- get_decennial(\n",
    "        geography = \"state\",\n",
    "        variables = variables,\n",
    "        year = 2020,\n",
    "        sumfile = \"dhc\"\n",
    "    )\n",
    "    \n",
    "    # Replace variable codes with descriptive labels\n",
    "    population_data <- population_data %>% \n",
    "        mutate(variable = recode(variable, !!!setNames(labels, variables)))\n",
    "    \n",
    "    # Reshape data for analysis\n",
    "    population_data <- population_data %>% \n",
    "        pivot_wider(names_from = variable, values_from = value)\n",
    "    \n",
    "    return(population_data)\n",
    "}\n",
    "\n",
    "# Get total population data (all races)\n",
    "cat(\"Retrieving total population data from 2020 Census...\\n\")\n",
    "population_info_total <- get_population_variables(\"P12\")\n",
    "total_population_by_age_gender <- get_population_data(population_info_total$variables, population_info_total$labels)\n",
    "\n",
    "# Get white alone population data\n",
    "cat(\"Retrieving white alone population data from 2020 Census...\\n\")\n",
    "population_info_white <- get_population_variables(\"P12I\")\n",
    "total_population_by_age_gender_white <- get_population_data(population_info_white$variables, population_info_white$labels)\n",
    "\n",
    "cat(\"Census data retrieval complete\\n\")\n",
    "\n",
    "if(VERBOSE_PRINTS) {\n",
    "  cat(\"\\nTotal population data structure:\\n\")\n",
    "  str(total_population_by_age_gender)\n",
    "  cat(\"\\nWhite population data structure:\\n\")\n",
    "  str(total_population_by_age_gender_white)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Comparison: NASS vs Census Proportions\n",
    "\n",
    "Compare unadjusted and weighted proportions of white individuals in NASS sample against Census benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# ========================================\n",
    "# Stage 1a: Unadjusted Proportion Analysis\n",
    "# ========================================\n",
    "\n",
    "cat(\"=== STAGE 1A: UNADJUSTED PROPORTION ANALYSIS ===\\n\")\n",
    "\n",
    "# Calculate unadjusted proportion of white individuals in NASS\n",
    "unadjusted_proportion_white <- mean(NASS$WHITE, na.rm = TRUE)\n",
    "cat(\"Unadjusted proportion of WHITE in NASS:\", round(unadjusted_proportion_white, 4), \"\\n\")\n",
    "\n",
    "# Calculate reference proportion from entire US Census\n",
    "us_census_white_proportion <- sum(total_population_by_age_gender_white$Total, na.rm = TRUE) / \n",
    "                             sum(total_population_by_age_gender$Total, na.rm = TRUE)\n",
    "cat(\"US Census White alone proportion:\", round(us_census_white_proportion, 4), \"\\n\")\n",
    "\n",
    "# Statistical test for unadjusted proportion\n",
    "unadjusted_test <- prop.test(sum(NASS$WHITE, na.rm = TRUE), \n",
    "                            sum(!is.na(NASS$WHITE)), \n",
    "                            p = us_census_white_proportion)\n",
    "cat(\"\\nUnadjusted proportion test results:\\n\")\n",
    "print(unadjusted_test)\n",
    "\n",
    "# ========================================\n",
    "# Stage 1b: Weighted Proportion Analysis\n",
    "# ========================================\n",
    "\n",
    "cat(\"\\n=== STAGE 1B: WEIGHTED PROPORTION ANALYSIS ===\\n\")\n",
    "\n",
    "# Filter Census data for NASS-included states only\n",
    "filtered_total_population <- total_population_by_age_gender %>% \n",
    "    filter(NAME %in% states_in_nass)\n",
    "\n",
    "filtered_white_population <- total_population_by_age_gender_white %>% \n",
    "    filter(NAME %in% states_in_nass)\n",
    "\n",
    "# Calculate true proportion for NASS states\n",
    "total_population_nass_states <- sum(filtered_total_population$Total, na.rm = TRUE)\n",
    "total_white_population_nass_states <- sum(filtered_white_population$Total, na.rm = TRUE)\n",
    "true_proportion_white_nass_states <- total_white_population_nass_states / total_population_nass_states\n",
    "\n",
    "cat(\"True proportion of WHITE in NASS states:\", round(true_proportion_white_nass_states, 4), \"\\n\")\n",
    "\n",
    "# Calculate weighted proportion using survey design\n",
    "survey_design <- svydesign(ids = ~1, weights = ~DISCWT, data = NASS)\n",
    "weighted_proportion_white <- svymean(~WHITE, design = survey_design)\n",
    "cat(\"Weighted proportion of WHITE in NASS:\", round(coef(weighted_proportion_white), 4), \"\\n\")\n",
    "\n",
    "# Statistical test for weighted proportion\n",
    "weighted_test <- svyttest(WHITE ~ 1, design = survey_design, mu = true_proportion_white_nass_states)\n",
    "cat(\"\\nWeighted proportion test results:\\n\")\n",
    "print(weighted_test)\n",
    "\n",
    "cat(\"\\n=== PROPORTION ANALYSIS COMPLETE ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age-Gender Stratified Analysis\n",
    "\n",
    "Detailed comparison of white proportions by age group and gender between NASS sample and Census data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# ========================================\n",
    "# Stage 2: Age-Gender Stratified Analysis\n",
    "# ========================================\n",
    "\n",
    "cat(\"=== STAGE 2: AGE-GENDER STRATIFIED ANALYSIS ===\\n\")\n",
    "\n",
    "# Define age groups matching Census categories\n",
    "age_breaks <- c(-Inf, 4, 9, 14, 17, 19, 20, 21, 24, 29, 34, 39, 44, 49, 54, 59, 61, 64, 66, 69, 74, 79, 84, Inf)\n",
    "age_labels <- c(\"Under 5 years\", \"5 to 9 years\", \"10 to 14 years\", \"15 to 17 years\", \"18 and 19 years\",\n",
    "                \"20 years\", \"21 years\", \"22 to 24 years\", \"25 to 29 years\", \"30 to 34 years\",\n",
    "                \"35 to 39 years\", \"40 to 44 years\", \"45 to 49 years\", \"50 to 54 years\", \"55 to 59 years\",\n",
    "                \"60 and 61 years\", \"62 to 64 years\", \"65 and 66 years\", \"67 to 69 years\", \"70 to 74 years\",\n",
    "                \"75 to 79 years\", \"80 to 84 years\", \"85 years and over\")\n",
    "\n",
    "# Create age group variable in NASS dataset\n",
    "NASS[, AGE_GROUP := cut(AGE, breaks = age_breaks, labels = age_labels, right = TRUE)]\n",
    "NASS[, GENDER := ifelse(FEMALE == 0, \"Male\", \"Female\")]\n",
    "\n",
    "cat(\"Created age groups and gender variables\\n\")\n",
    "\n",
    "# Calculate NASS proportions by age group and gender\n",
    "nass_proportions <- NASS[!is.na(AGE_GROUP) & !is.na(WHITE), \n",
    "                        .(total = .N,\n",
    "                          white = sum(WHITE, na.rm = TRUE),\n",
    "                          proportion_white = mean(WHITE, na.rm = TRUE)), \n",
    "                        by = .(AGE_GROUP, GENDER)]\n",
    "\n",
    "# Add confidence intervals\n",
    "nass_proportions[, ':='(\n",
    "  ci_lower = proportion_white - 1.96 * sqrt((proportion_white * (1 - proportion_white)) / total),\n",
    "  ci_upper = proportion_white + 1.96 * sqrt((proportion_white * (1 - proportion_white)) / total)\n",
    ")]\n",
    "\n",
    "cat(\"Calculated NASS proportions by age-gender groups\\n\")\n",
    "\n",
    "# Process Census data for comparison\n",
    "census_proportions <- total_population_by_age_gender_white %>% \n",
    "    select(NAME, starts_with(\"Male\"), starts_with(\"Female\")) %>% \n",
    "    pivot_longer(cols = -NAME, names_to = \"age_gender\", values_to = \"white_population\") %>% \n",
    "    separate(age_gender, into = c(\"gender\", \"age_group\"), sep = \": \") %>% \n",
    "    left_join(\n",
    "        total_population_by_age_gender %>% \n",
    "            select(NAME, starts_with(\"Male\"), starts_with(\"Female\")) %>% \n",
    "            pivot_longer(cols = -NAME, names_to = \"age_gender\", values_to = \"total_population\") %>% \n",
    "            separate(age_gender, into = c(\"gender\", \"age_group\"), sep = \": \"),\n",
    "        by = c(\"NAME\", \"gender\", \"age_group\")\n",
    "    ) %>% \n",
    "    filter(NAME %in% states_in_nass) %>%  # Filter for NASS states only\n",
    "    group_by(gender, age_group) %>% \n",
    "    summarize(\n",
    "        total_population = sum(total_population, na.rm = TRUE),\n",
    "        white_population = sum(white_population, na.rm = TRUE),\n",
    "        proportion_white = white_population / total_population,\n",
    "        .groups = 'drop'\n",
    "    ) %>% \n",
    "    filter(!is.na(age_group) & age_group != \"Total\")\n",
    "\n",
    "cat(\"Processed Census proportions by age-gender groups\\n\")\n",
    "\n",
    "# Convert to data.table for easier manipulation\n",
    "setDT(census_proportions)\n",
    "setDT(nass_proportions)\n",
    "\n",
    "# Convert age groups to factors for proper plotting\n",
    "census_proportions[, age_group := factor(age_group, levels = age_labels)]\n",
    "nass_proportions[, AGE_GROUP := factor(AGE_GROUP, levels = age_labels)]\n",
    "\n",
    "# Remove any missing age groups\n",
    "census_proportions <- census_proportions[!is.na(age_group)]\n",
    "nass_proportions <- nass_proportions[!is.na(AGE_GROUP)]\n",
    "\n",
    "cat(\"Data preparation complete\\n\")\n",
    "cat(\"NASS age-gender groups:\", nrow(nass_proportions), \"\\n\")\n",
    "cat(\"Census age-gender groups:\", nrow(census_proportions), \"\\n\")\n",
    "\n",
    "if(VERBOSE_PRINTS) {\n",
    "  cat(\"\\nSample NASS proportions:\\n\")\n",
    "  print(head(nass_proportions))\n",
    "  cat(\"\\nSample Census proportions:\\n\")\n",
    "  print(head(census_proportions))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Testing and Visualization\n",
    "\n",
    "Generate statistical tests comparing NASS and Census proportions across age-gender groups, with visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# ========================================\n",
    "# Statistical Testing by Age-Gender Groups\n",
    "# ========================================\n",
    "\n",
    "cat(\"=== STATISTICAL TESTING BY AGE-GENDER GROUPS ===\\n\")\n",
    "\n",
    "# Perform statistical tests for each age-gender combination\n",
    "test_results <- merge(nass_proportions, census_proportions, \n",
    "                     by.x = c(\"AGE_GROUP\", \"GENDER\"), \n",
    "                     by.y = c(\"age_group\", \"gender\"),\n",
    "                     all.x = TRUE)\n",
    "\n",
    "# Function to perform proportion test safely\n",
    "safe_prop_test <- function(white_count, total_count, census_prop) {\n",
    "  if(is.na(white_count) || is.na(total_count) || is.na(census_prop) || \n",
    "     total_count == 0 || census_prop == 0 || census_prop == 1) {\n",
    "    return(list(p.value = NA, significant = FALSE))\n",
    "  }\n",
    "  \n",
    "  tryCatch({\n",
    "    test_result <- prop.test(white_count, total_count, p = census_prop)\n",
    "    return(list(p.value = test_result$p.value, \n",
    "               significant = test_result$p.value < 0.05))\n",
    "  }, error = function(e) {\n",
    "    return(list(p.value = NA, significant = FALSE))\n",
    "  })\n",
    "}\n",
    "\n",
    "# Apply tests\n",
    "test_results[, c(\"p_value\", \"significant\") := {\n",
    "  test_res = safe_prop_test(white, total, proportion_white.y)\n",
    "  list(test_res$p.value, test_res$significant)\n",
    "}, by = 1:nrow(test_results)]\n",
    "\n",
    "cat(\"Statistical tests completed\\n\")\n",
    "\n",
    "# Summary of significant differences\n",
    "sig_count <- sum(test_results$significant, na.rm = TRUE)\n",
    "total_tests <- sum(!is.na(test_results$p_value))\n",
    "cat(\"Significant differences found:\", sig_count, \"out of\", total_tests, \"tests\\n\")\n",
    "\n",
    "# Display results table\n",
    "results_summary <- test_results[, .(\n",
    "  AGE_GROUP, GENDER,\n",
    "  NASS_count = total,\n",
    "  NASS_white_prop = round(proportion_white.x, 3),\n",
    "  Census_white_prop = round(proportion_white.y, 3),\n",
    "  p_value = round(p_value, 6),\n",
    "  significant = significant\n",
    ")][order(AGE_GROUP, GENDER)]\n",
    "\n",
    "cat(\"\\nDetailed results by age-gender group:\\n\")\n",
    "print(results_summary)\n",
    "\n",
    "# ========================================\n",
    "# Visualization\n",
    "# ========================================\n",
    "\n",
    "cat(\"\\n=== GENERATING VISUALIZATIONS ===\\n\")\n",
    "\n",
    "# Check if ggplot2 is available for plotting\n",
    "if(require(\"ggplot2\", quietly = TRUE)) {\n",
    "  \n",
    "  # Prepare data for plotting\n",
    "  plot_data_nass <- nass_proportions[, .(\n",
    "    AGE_GROUP, GENDER, proportion_white, ci_lower, ci_upper, source = \"NASS\"\n",
    "  )]\n",
    "  \n",
    "  plot_data_census <- census_proportions[, .(\n",
    "    AGE_GROUP = age_group, GENDER = gender, proportion_white, source = \"Census\"\n",
    "  )]\n",
    "  plot_data_census[, ':='(ci_lower = proportion_white, ci_upper = proportion_white)]\n",
    "  \n",
    "  plot_data <- rbind(plot_data_nass, plot_data_census, fill = TRUE)\n",
    "  \n",
    "  # Create the plot\n",
    "  age_gender_plot <- ggplot(plot_data, aes(x = AGE_GROUP, y = proportion_white, \n",
    "                                          color = source, group = interaction(source, GENDER))) +\n",
    "    geom_line(linewidth = 1) +\n",
    "    geom_point() +\n",
    "    geom_ribbon(data = plot_data[source == \"NASS\"], \n",
    "                aes(ymin = ci_lower, ymax = ci_upper, fill = source), \n",
    "                alpha = 0.2, color = NA) +\n",
    "    facet_wrap(~GENDER, ncol = 1) +\n",
    "    labs(title = \"White Proportion by Age Group: NASS vs Census\",\n",
    "         subtitle = \"Comparison across age groups and gender\",\n",
    "         x = \"Age Group\",\n",
    "         y = \"Proportion White\",\n",
    "         color = \"Data Source\",\n",
    "         fill = \"Confidence Interval\") +\n",
    "    theme_minimal() +\n",
    "    theme(axis.text.x = element_text(angle = 45, hjust = 1),\n",
    "          legend.position = \"bottom\") +\n",
    "    scale_y_continuous(limits = c(0, 1), labels = scales::percent)\n",
    "  \n",
    "  print(age_gender_plot)\n",
    "  cat(\"Visualization generated successfully\\n\")\n",
    "  \n",
    "} else {\n",
    "  cat(\"ggplot2 not available - using base R plotting\\n\")\n",
    "  \n",
    "  # Base R fallback plotting\n",
    "  par(mfrow = c(2, 1), mar = c(8, 4, 4, 2))\n",
    "  \n",
    "  # Males plot\n",
    "  male_nass <- nass_proportions[GENDER == \"Male\"]\n",
    "  male_census <- census_proportions[gender == \"Male\"]\n",
    "  \n",
    "  plot(1:nrow(male_nass), male_nass$proportion_white, type = \"b\", col = \"blue\",\n",
    "       xlab = \"\", ylab = \"Proportion White\", main = \"Males: NASS vs Census\",\n",
    "       ylim = c(0, 1), xaxt = \"n\")\n",
    "  lines(1:nrow(male_census), male_census$proportion_white, type = \"b\", col = \"red\")\n",
    "  axis(1, at = 1:nrow(male_nass), labels = male_nass$AGE_GROUP, las = 2, cex.axis = 0.8)\n",
    "  legend(\"topright\", legend = c(\"NASS\", \"Census\"), col = c(\"blue\", \"red\"), lty = 1)\n",
    "  \n",
    "  # Females plot\n",
    "  female_nass <- nass_proportions[GENDER == \"Female\"]\n",
    "  female_census <- census_proportions[gender == \"Female\"]\n",
    "  \n",
    "  plot(1:nrow(female_nass), female_nass$proportion_white, type = \"b\", col = \"blue\",\n",
    "       xlab = \"Age Group\", ylab = \"Proportion White\", main = \"Females: NASS vs Census\",\n",
    "       ylim = c(0, 1), xaxt = \"n\")\n",
    "  lines(1:nrow(female_census), female_census$proportion_white, type = \"b\", col = \"red\")\n",
    "  axis(1, at = 1:nrow(female_nass), labels = female_nass$AGE_GROUP, las = 2, cex.axis = 0.8)\n",
    "  legend(\"topright\", legend = c(\"NASS\", \"Census\"), col = c(\"blue\", \"red\"), lty = 1)\n",
    "  \n",
    "  par(mfrow = c(1, 1))\n",
    "  cat(\"Base R visualization generated\\n\")\n",
    "}\n",
    "\n",
    "cat(\"\\n=== CENSUS AGE-GENDER ANALYSIS COMPLETE ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-level Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
