{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d779aa9",
   "metadata": {},
   "source": [
    "# NASS Analysis Sandbox - Development & Testing Environment\n",
    "\n",
    "**Purpose:** Sandbox environment for developing and testing extensions to the main Analysis_NASS.ipynb pipeline before integration.\n",
    "\n",
    "**Main Notebook Reference:** `Analysis_NASS.ipynb` (same directory)\n",
    "\n",
    "**Author:** Development testing for Seena Khosravi, MD  \n",
    "**Created:** For extension development and testing  \n",
    "\n",
    "---\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "- This sandbox references the main `Analysis_NASS.ipynb` notebook structure\n",
    "- All new code developed here should be tested before integration\n",
    "- Environment setup mirrors the main notebook for compatibility\n",
    "- Data processing assumes the main notebook preprocessing pipeline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e6064",
   "metadata": {},
   "source": [
    "# 1. Environment Verification\n",
    "\n",
    "Verify R and Python environments match the main notebook setup, load required packages, and test rpy2 integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07203b1",
   "metadata": {},
   "source": [
    "## Environment Setup Verification\n",
    "\n",
    "Check that our environment matches the main Analysis_NASS.ipynb setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f1bac77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SANDBOX ENVIRONMENT VERIFICATION ===\n",
      "Python version: 3.11.5 (tags/v3.11.5:cce6ba9, Aug 24 2023, 14:38:34) [MSC v.1936 64 bit (AMD64)]\n",
      "Working directory: c:\\Users\\laure\\Documents\\GitHub\\NASS\n",
      "✅ Main notebook found: c:\\Users\\laure\\Documents\\GitHub\\NASS\\Analysis_NASS.ipynb\n",
      "✅ pandas available\n",
      "✅ requests available\n",
      "✅ rpy2 available\n",
      "\n",
      "✅ All required packages available\n"
     ]
    }
   ],
   "source": [
    "# Verify environment matches main notebook setup\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== SANDBOX ENVIRONMENT VERIFICATION ===\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check if we're in the same environment as main notebook\n",
    "main_notebook_path = Path(\"Analysis_NASS.ipynb\")\n",
    "if main_notebook_path.exists():\n",
    "    print(f\"✅ Main notebook found: {main_notebook_path.absolute()}\")\n",
    "else:\n",
    "    print(f\"⚠️  Main notebook not found in current directory\")\n",
    "    print(f\"   Expected: {main_notebook_path.absolute()}\")\n",
    "\n",
    "# Verify key packages from main notebook\n",
    "required_packages = ['pandas', 'requests', 'rpy2']\n",
    "missing_packages = []\n",
    "\n",
    "for pkg in required_packages:\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "        print(f\"✅ {pkg} available\")\n",
    "    except ImportError:\n",
    "        missing_packages.append(pkg)\n",
    "        print(f\"❌ {pkg} missing\")\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\n⚠️  Missing packages: {missing_packages}\")\n",
    "    print(\"   Run the environment setup from main notebook first\")\n",
    "else:\n",
    "    print(\"\\n✅ All required packages available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb7dde1",
   "metadata": {},
   "source": [
    "## R Integration Testing\n",
    "\n",
    "Test rpy2 integration and R package availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6713dbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error importing in API mode: ImportError('On Windows, cffi mode \"ANY\" is only \"ABI\".')\n",
      "Trying to import in ABI mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ R integration loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Test R integration\n",
    "try:\n",
    "    %load_ext rpy2.ipython\n",
    "    print(\"✅ R integration loaded successfully\")\n",
    "    R_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(f\"❌ R integration failed: {e}\")\n",
    "    R_AVAILABLE = False\n",
    "\n",
    "# Store R availability for later cells\n",
    "globals()['R_AVAILABLE'] = R_AVAILABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3a46892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== R ENVIRONMENT VERIFICATION ===\n",
      "R version: R version 4.4.0 (2024-04-24 ucrt) \n",
      "<U+2705> data.table loaded\n",
      "<U+2705> ggplot2 loaded\n",
      "<U+2705> scales loaded\n",
      "\n",
      "<U+2705> All essential R packages available\n",
      "data.table threads: 8 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data.table 1.17.8 using 4 threads (see ?getDTthreads).  Latest news: r-datatable.com\n",
       "In addition: Warning messages:\n",
       "1: package 'data.table' was built under R version 4.4.3 \n",
       "2: package 'ggplot2' was built under R version 4.4.2 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "# Test R environment and core packages\n",
    "cat(\"=== R ENVIRONMENT VERIFICATION ===\\n\")\n",
    "cat(\"R version:\", R.version.string, \"\\n\")\n",
    "\n",
    "# Test essential packages from main notebook\n",
    "essential_packages <- c(\"data.table\", \"ggplot2\", \"scales\")\n",
    "missing_r_packages <- c()\n",
    "\n",
    "for(pkg in essential_packages) {\n",
    "  if(require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "    cat(\"✅\", pkg, \"loaded\\n\")\n",
    "  } else {\n",
    "    cat(\"❌\", pkg, \"missing\\n\")\n",
    "    missing_r_packages <- c(missing_r_packages, pkg)\n",
    "  }\n",
    "}\n",
    "\n",
    "if(length(missing_r_packages) > 0) {\n",
    "  cat(\"\\n⚠️  Missing R packages:\", paste(missing_r_packages, collapse = \", \"), \"\\n\")\n",
    "  cat(\"   Install these packages before proceeding\\n\")\n",
    "} else {\n",
    "  cat(\"\\n✅ All essential R packages available\\n\")\n",
    "}\n",
    "\n",
    "# Test data.table threading (performance check)\n",
    "if(require(\"data.table\", quietly = TRUE)) {\n",
    "  setDTthreads(0)  # Use all cores\n",
    "  cat(\"data.table threads:\", getDTthreads(), \"\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa953ae1",
   "metadata": {},
   "source": [
    "# 2. Data Import from Main Notebook\n",
    "\n",
    "Import the processed NASS dataset and verify data structure matches expectations from the main analysis pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1134359",
   "metadata": {},
   "source": [
    "## Data Loading Strategy\n",
    "\n",
    "Since this is a sandbox, we'll implement multiple data loading strategies to work with the main notebook's processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "004e8484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying memory loading...\n",
      "❌ memory loading failed\n",
      "Trying pickle loading...\n",
      "❌ pickle loading failed\n",
      "Trying csv loading...\n",
      "❌ csv loading failed\n",
      "Trying fresh loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laure\\AppData\\Local\\Temp\\ipykernel_17524\\1759960300.py:61: DtypeWarning: Columns (56,57,58,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.nass_data = pd.read_csv(temp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data loaded via fresh\n",
      "   Shape: (139233, 675)\n",
      "\n",
      "✅ Sandbox data ready: (139233, 675)\n"
     ]
    }
   ],
   "source": [
    "# Data loading for sandbox - multiple strategies\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "class SandboxDataLoader:\n",
    "    def __init__(self):\n",
    "        self.data_sources = {\n",
    "            'memory': 'From Python globals (if main notebook ran)',\n",
    "            'pickle': 'From saved pickle file',\n",
    "            'csv': 'From processed CSV file',\n",
    "            'fresh': 'Fresh download (GitHub simulated data)'\n",
    "        }\n",
    "        self.nass_data = None\n",
    "        \n",
    "    def load_from_memory(self):\n",
    "        \"\"\"Try to load from Python globals if main notebook was run\"\"\"\n",
    "        try:\n",
    "            if 'df' in globals():\n",
    "                self.nass_data = globals()['df'].copy()\n",
    "                return True\n",
    "            return False\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def load_from_pickle(self):\n",
    "        \"\"\"Load from pickle file if main notebook saved it\"\"\"\n",
    "        pickle_path = Path('nass_processed.pkl')\n",
    "        try:\n",
    "            if pickle_path.exists():\n",
    "                with open(pickle_path, 'rb') as f:\n",
    "                    self.nass_data = pickle.load(f)\n",
    "                return True\n",
    "            return False\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def load_from_csv(self):\n",
    "        \"\"\"Load from processed CSV if available\"\"\"\n",
    "        csv_path = Path('data/nass_processed.csv')\n",
    "        try:\n",
    "            if csv_path.exists():\n",
    "                self.nass_data = pd.read_csv(csv_path)\n",
    "                return True\n",
    "            return False\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def load_fresh_data(self):\n",
    "        \"\"\"Load fresh simulated data from GitHub\"\"\"\n",
    "        try:\n",
    "            import requests\n",
    "            github_url = \"https://github.com/SeenaKhosravi/NASS/releases/download/v1.0.0/nass_2020_simulated.csv\"\n",
    "            response = requests.get(github_url, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Save temporarily\n",
    "            temp_path = Path('temp_nass_data.csv')\n",
    "            temp_path.write_bytes(response.content)\n",
    "            \n",
    "            self.nass_data = pd.read_csv(temp_path)\n",
    "            temp_path.unlink()  # Clean up\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def auto_load(self):\n",
    "        \"\"\"Try loading strategies in order of preference\"\"\"\n",
    "        strategies = [\n",
    "            ('memory', self.load_from_memory),\n",
    "            ('pickle', self.load_from_pickle),\n",
    "            ('csv', self.load_from_csv),\n",
    "            ('fresh', self.load_fresh_data)\n",
    "        ]\n",
    "        \n",
    "        for strategy_name, strategy_func in strategies:\n",
    "            print(f\"Trying {strategy_name} loading...\")\n",
    "            if strategy_func():\n",
    "                print(f\"✅ Data loaded via {strategy_name}\")\n",
    "                print(f\"   Shape: {self.nass_data.shape}\")\n",
    "                return strategy_name\n",
    "            else:\n",
    "                print(f\"❌ {strategy_name} loading failed\")\n",
    "        \n",
    "        raise RuntimeError(\"All data loading strategies failed\")\n",
    "\n",
    "# Load data\n",
    "loader = SandboxDataLoader()\n",
    "try:\n",
    "    load_method = loader.auto_load()\n",
    "    df_sandbox = loader.nass_data\n",
    "    print(f\"\\n✅ Sandbox data ready: {df_sandbox.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Data loading failed: {e}\")\n",
    "    df_sandbox = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fff0d26",
   "metadata": {},
   "source": [
    "## Data Structure Verification\n",
    "\n",
    "Verify the loaded data matches the expected structure from the main notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "407783e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA STRUCTURE VERIFICATION ===\n",
      "Present columns: 10/13\n",
      "✅ Found: ['AGE', 'FEMALE', 'RACE', 'ZIPINC_QRTL', 'PAY1', 'CPTCCS1', 'HOSP_LOCATION', 'HOSP_TEACH', 'HOSP_REGION', 'DISCWT']\n",
      "❌ Missing: ['WHITE', 'AGE_GROUP', 'INCOME_LEVEL']\n",
      "   May need to create these variables\n",
      "\n",
      "Data Quality:\n",
      "  Total rows: 139,233\n",
      "  Total columns: 675\n",
      "  Memory usage: 885.2 MB\n",
      "  Age range: 0 - 104\n"
     ]
    }
   ],
   "source": [
    "# Verify data structure matches main notebook expectations\n",
    "if df_sandbox is not None:\n",
    "    print(\"=== DATA STRUCTURE VERIFICATION ===\")\n",
    "    \n",
    "    # Expected columns from main notebook processing\n",
    "    expected_columns = [\n",
    "        'AGE', 'FEMALE', 'RACE', 'WHITE', 'ZIPINC_QRTL', 'PAY1',\n",
    "        'CPTCCS1', 'HOSP_LOCATION', 'HOSP_TEACH', 'HOSP_REGION',\n",
    "        'DISCWT', 'AGE_GROUP', 'INCOME_LEVEL'\n",
    "    ]\n",
    "    \n",
    "    # Check which expected columns are present\n",
    "    present_columns = [col for col in expected_columns if col in df_sandbox.columns]\n",
    "    missing_columns = [col for col in expected_columns if col not in df_sandbox.columns]\n",
    "    \n",
    "    print(f\"Present columns: {len(present_columns)}/{len(expected_columns)}\")\n",
    "    print(f\"✅ Found: {present_columns}\")\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"❌ Missing: {missing_columns}\")\n",
    "        print(\"   May need to create these variables\")\n",
    "    \n",
    "    # Basic data quality checks\n",
    "    print(f\"\\nData Quality:\")\n",
    "    print(f\"  Total rows: {len(df_sandbox):,}\")\n",
    "    print(f\"  Total columns: {len(df_sandbox.columns)}\")\n",
    "    print(f\"  Memory usage: {df_sandbox.memory_usage(deep=True).sum() / 1e6:.1f} MB\")\n",
    "    \n",
    "    # Check for key variables\n",
    "    if 'WHITE' in df_sandbox.columns:\n",
    "        white_prop = df_sandbox['WHITE'].mean()\n",
    "        print(f\"  White proportion: {white_prop:.3f}\")\n",
    "    \n",
    "    if 'AGE' in df_sandbox.columns:\n",
    "        age_stats = df_sandbox['AGE'].describe()\n",
    "        print(f\"  Age range: {age_stats['min']:.0f} - {age_stats['max']:.0f}\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No data available for verification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7309b993",
   "metadata": {},
   "source": [
    "## Transfer to R Environment\n",
    "\n",
    "Transfer the verified data to R for advanced analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f003fa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferring data to R environment...\n",
      "✅ Data prepared for R transfer: (139233, 675)\n"
     ]
    }
   ],
   "source": [
    "# Transfer data to R environment\n",
    "if df_sandbox is not None and R_AVAILABLE:\n",
    "    print(\"Transferring data to R environment...\")\n",
    "    \n",
    "    # Ensure data types are R-compatible (same as main notebook)\n",
    "    df_r_ready = df_sandbox.copy()\n",
    "    \n",
    "    # Convert object columns to strings\n",
    "    object_columns = df_r_ready.select_dtypes(include=['object']).columns\n",
    "    for col in object_columns:\n",
    "        df_r_ready[col] = df_r_ready[col].astype(str)\n",
    "    \n",
    "    # Handle NaN values\n",
    "    df_r_ready = df_r_ready.fillna('')\n",
    "    \n",
    "    print(f\"✅ Data prepared for R transfer: {df_r_ready.shape}\")\n",
    "else:\n",
    "    print(\"❌ Cannot transfer to R - data or R not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d29c0df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT5\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT6\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT7\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT8\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT9\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT10\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT11\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT12\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT13\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT14\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT15\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT16\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT17\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT18\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT19\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT20\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT21\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT22\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT23\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT24\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT25\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT26\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT27\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT28\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT29\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n",
      "c:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:65: UserWarning: Error while trying to convert the column \"CPT30\". Fall back to string conversion. The error is: Series can only be of one type, or None (and here we have <class 'str'> and <class 'float'>). If happening with a pandas DataFrame the method infer_objects() will normalize data types before conversion.\n",
      "  warnings.warn('Error while trying to convert '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\rinterface_lib\\sexp.py:634\u001b[39m, in \u001b[36mSexpVectorAbstract.from_object\u001b[39m\u001b[34m(cls, obj)\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m     mv = \u001b[38;5;28;43mmemoryview\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    635\u001b[39m     res = \u001b[38;5;28mcls\u001b[39m.from_memoryview(mv)\n",
      "\u001b[31mTypeError\u001b[39m: memoryview: a bytes-like object is required, not 'tuple'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mR\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-i df_r_ready\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m# Convert to data.table for R analysis\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mif(exists(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdf_r_ready\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)) \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  NASS_SANDBOX <- as.data.table(df_r_ready)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  # Apply R data types (matching main notebook)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  factor_vars <- c(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mZIPINC_QRTL\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m, \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPAY1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m, \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCPTCCS1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m, \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHOSP_LOCATION\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m                   \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHOSP_TEACH\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m, \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHOSP_REGION\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m, \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRACE\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  existing_factors <- factor_vars[factor_vars \u001b[39;49m\u001b[38;5;132;43;01m%i\u001b[39;49;00m\u001b[33;43mn\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43m names(NASS_SANDBOX)]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  if(length(existing_factors) > 0) \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    NASS_SANDBOX[, (existing_factors) := lapply(.SD, as.factor), .SDcols = existing_factors]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  }\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  # Boolean variables\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  if(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFEMALE\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[38;5;132;43;01m%i\u001b[39;49;00m\u001b[33;43mn\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43m names(NASS_SANDBOX)) \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    NASS_SANDBOX[, FEMALE := as.logical(as.numeric(FEMALE))]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  }\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  if(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWHITE\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[38;5;132;43;01m%i\u001b[39;49;00m\u001b[33;43mn\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43m names(NASS_SANDBOX)) \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    NASS_SANDBOX[, WHITE := as.logical(as.numeric(WHITE))]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  }\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  cat(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m✅ R data ready: NASS_SANDBOX\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  cat(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m   Rows:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m, nrow(NASS_SANDBOX), \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  cat(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m   Cols:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m, ncol(NASS_SANDBOX), \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  cat(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m   Factors:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m, length(existing_factors), \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m} else \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  cat(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m❌ df_r_ready not found - cannot create NASS_SANDBOX\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m}\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:2565\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2564\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2565\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2568\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2569\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\ipython\\rmagic.py:1246\u001b[39m, in \u001b[36mRMagics.R\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1244\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m converter.context() \u001b[38;5;28;01mas\u001b[39;00m cv:\n\u001b[32m   1245\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m.join(args.input).split(\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1246\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_import_name_into_r\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mro\u001b[49m\u001b[43m.\u001b[49m\u001b[43mglobalenv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.display:\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\ipython\\rmagic.py:811\u001b[39m, in \u001b[36mRMagics._import_name_into_r\u001b[39m\u001b[34m(self, arg, env, local_ns)\u001b[39m\n\u001b[32m    809\u001b[39m         val = _find(rhs, \u001b[38;5;28mself\u001b[39m.shell.user_ns)\n\u001b[32m    810\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m811\u001b[39m     \u001b[43menv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlhs\u001b[49m\u001b[43m]\u001b[49m = val\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\environments.py:35\u001b[39m, in \u001b[36mEnvironment.__setitem__\u001b[39m\u001b[34m(self, item, value)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__setitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item: \u001b[38;5;28mstr\u001b[39m, value: typing.Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     robj = \u001b[43mconversion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_conversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpy2rpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28msuper\u001b[39m(Environment, \u001b[38;5;28mself\u001b[39m).\u001b[34m__setitem__\u001b[39m(item, robj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\functools.py:909\u001b[39m, in \u001b[36msingledispatch.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires at least \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    907\u001b[39m                     \u001b[33m'\u001b[39m\u001b[33m1 positional argument\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m909\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:63\u001b[39m, in \u001b[36mpy2rpy_pandasdataframe\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, values \u001b[38;5;129;01min\u001b[39;00m obj.items():\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         od[name] = \u001b[43mconversion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverter_ctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpy2rpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     65\u001b[39m         warnings.warn(\u001b[33m'\u001b[39m\u001b[33mError while trying to convert \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     66\u001b[39m                       \u001b[33m'\u001b[39m\u001b[33mthe column \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m. Fall back to string conversion. \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     67\u001b[39m                       \u001b[33m'\u001b[39m\u001b[33mThe error is: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     68\u001b[39m                       % (name, \u001b[38;5;28mstr\u001b[39m(e)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\functools.py:909\u001b[39m, in \u001b[36msingledispatch.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires at least \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    907\u001b[39m                     \u001b[33m'\u001b[39m\u001b[33m1 positional argument\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m909\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:244\u001b[39m, in \u001b[36mpy2rpy_pandasseries\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# \"index\" is equivalent to \"names\" in R\u001b[39;00m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obj.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    243\u001b[39m     res.do_slot_assign(\u001b[33m'\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m                        \u001b[43mStrVector\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    246\u001b[39m     res.do_slot_assign(\u001b[33m'\u001b[39m\u001b[33mdimnames\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    247\u001b[39m                        SexpVector(conversion.converter_ctx\n\u001b[32m    248\u001b[39m                                   .get().py2rpy(obj.index)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\robjects\\vectors.py:418\u001b[39m, in \u001b[36mStrVector.__init__\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj):\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    419\u001b[39m     \u001b[38;5;28mself\u001b[39m._add_rops()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\rinterface_lib\\sexp.py:769\u001b[39m, in \u001b[36mSexpVector.__init__\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    767\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(obj)\n\u001b[32m    768\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, collections.abc.Sized):\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m     robj: Sexp = \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    770\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(robj)\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\rinterface_lib\\sexp.py:638\u001b[39m, in \u001b[36mSexpVectorAbstract.from_object\u001b[39m\u001b[34m(cls, obj)\u001b[39m\n\u001b[32m    636\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m    637\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m         res = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m    640\u001b[39m         msg = (\u001b[33m'\u001b[39m\u001b[33mThe class methods from_memoryview() and \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    641\u001b[39m                \u001b[33m'\u001b[39m\u001b[33mfrom_iterable() both failed to make a \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    642\u001b[39m                \u001b[33m'\u001b[39m\u001b[33mfrom an object of class \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    643\u001b[39m                .format(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m(obj)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\rinterface_lib\\conversion.py:46\u001b[39m, in \u001b[36m_cdata_res_to_rinterface.<locals>._\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_\u001b[39m(*args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     cdata = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     \u001b[38;5;66;03m# TODO: test cdata is of the expected CType\u001b[39;00m\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _cdata_to_rinterface(cdata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\rinterface_lib\\sexp.py:572\u001b[39m, in \u001b[36mSexpVectorAbstract.from_iterable\u001b[39m\u001b[34m(cls, iterable, populate_func, set_elt, cast_value)\u001b[39m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m memorymanagement.rmemory() \u001b[38;5;28;01mas\u001b[39;00m rmemory:\n\u001b[32m    568\u001b[39m     r_vector = rmemory.protect(\n\u001b[32m    569\u001b[39m         openrlib.rlib.Rf_allocVector(\n\u001b[32m    570\u001b[39m             \u001b[38;5;28mcls\u001b[39m._R_TYPE, n)\n\u001b[32m    571\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m572\u001b[39m     \u001b[43mpopulate_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_elt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcast_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r_vector\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\rinterface_lib\\sexp.py:520\u001b[39m, in \u001b[36m_populate_r_vector\u001b[39m\u001b[34m(iterable, r_vector, set_elt, cast_value)\u001b[39m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_populate_r_vector\u001b[39m(iterable, r_vector, set_elt, cast_value) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    519\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(iterable):\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m         set_elt(r_vector, i, \u001b[43mcast_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\rinterface_lib\\sexp.py:784\u001b[39m, in \u001b[36m_as_charsxp_cdata\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    782\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x.__sexp__._cdata\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconversion\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_str_to_charsxp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rpy2\\rinterface_lib\\conversion.py:165\u001b[39m, in \u001b[36m_str_to_charsxp\u001b[39m\u001b[34m(val)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    164\u001b[39m     cchar = _str_to_cchar(val, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     s = \u001b[43mrlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mRf_mkCharCE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcchar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopenrlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCE_UTF8\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m s\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "%%R -i df_r_ready\n",
    "\n",
    "# Convert to data.table for R analysis\n",
    "if(exists(\"df_r_ready\")) {\n",
    "  NASS_SANDBOX <- as.data.table(df_r_ready)\n",
    "  \n",
    "  # Apply R data types (matching main notebook)\n",
    "  factor_vars <- c(\"ZIPINC_QRTL\", \"PAY1\", \"CPTCCS1\", \"HOSP_LOCATION\",\n",
    "                   \"HOSP_TEACH\", \"HOSP_REGION\", \"RACE\")\n",
    "  existing_factors <- factor_vars[factor_vars %in% names(NASS_SANDBOX)]\n",
    "  \n",
    "  if(length(existing_factors) > 0) {\n",
    "    NASS_SANDBOX[, (existing_factors) := lapply(.SD, as.factor), .SDcols = existing_factors]\n",
    "  }\n",
    "  \n",
    "  # Boolean variables\n",
    "  if(\"FEMALE\" %in% names(NASS_SANDBOX)) {\n",
    "    NASS_SANDBOX[, FEMALE := as.logical(as.numeric(FEMALE))]\n",
    "  }\n",
    "  if(\"WHITE\" %in% names(NASS_SANDBOX)) {\n",
    "    NASS_SANDBOX[, WHITE := as.logical(as.numeric(WHITE))]\n",
    "  }\n",
    "  \n",
    "  cat(\"✅ R data ready: NASS_SANDBOX\\n\")\n",
    "  cat(\"   Rows:\", nrow(NASS_SANDBOX), \"\\n\")\n",
    "  cat(\"   Cols:\", ncol(NASS_SANDBOX), \"\\n\")\n",
    "  cat(\"   Factors:\", length(existing_factors), \"\\n\")\n",
    "  \n",
    "} else {\n",
    "  cat(\"❌ df_r_ready not found - cannot create NASS_SANDBOX\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062c1974",
   "metadata": {},
   "source": [
    "# 3. Extended Statistical Analysis\n",
    "\n",
    "Develop advanced statistical models including multilevel regression, survey-weighted analyses, and demographic stratification methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4acd6f",
   "metadata": {},
   "source": [
    "## Advanced Statistical Packages Installation\n",
    "\n",
    "Install and load packages for advanced statistical modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa941380",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Install advanced statistical packages\n",
    "advanced_packages <- c(\n",
    "  \"lme4\",          # Mixed-effects models\n",
    "  \"survey\",        # Survey statistics\n",
    "  \"broom\",         # Model tidying\n",
    "  \"broom.mixed\",   # Mixed model tidying\n",
    "  \"ggeffects\",     # Effect plots\n",
    "  \"sjPlot\",        # Statistical plots\n",
    "  \"performance\",   # Model performance\n",
    "  \"marginaleffects\" # Marginal effects\n",
    ")\n",
    "\n",
    "cat(\"Installing advanced statistical packages...\\n\")\n",
    "\n",
    "for(pkg in advanced_packages) {\n",
    "  if(!require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "    cat(\"Installing\", pkg, \"...\\n\")\n",
    "    tryCatch({\n",
    "      install.packages(pkg, quiet = TRUE, dependencies = TRUE)\n",
    "      library(pkg, character.only = TRUE, quietly = TRUE)\n",
    "      cat(\"✅\", pkg, \"installed and loaded\\n\")\n",
    "    }, error = function(e) {\n",
    "      cat(\"❌\", pkg, \"installation failed:\", e$message, \"\\n\")\n",
    "    })\n",
    "  } else {\n",
    "    cat(\"✅\", pkg, \"already available\\n\")\n",
    "  }\n",
    "}\n",
    "\n",
    "cat(\"\\nAdvanced packages setup complete\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d657a1b",
   "metadata": {},
   "source": [
    "## Multilevel Modeling Framework\n",
    "\n",
    "Develop hierarchical models accounting for hospital-level clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e4bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Advanced Statistical Analysis - Multilevel Models\n",
    "cat(\"=== MULTILEVEL MODELING ANALYSIS ===\\n\")\n",
    "\n",
    "if(exists(\"NASS_SANDBOX\") && require(\"lme4\", quietly = TRUE) && require(\"survey\", quietly = TRUE)) {\n",
    "  \n",
    "  # Create analysis subset with complete cases\n",
    "  analysis_vars <- c(\"WHITE\", \"AGE\", \"FEMALE\", \"ZIPINC_QRTL\", \"PAY1\", \n",
    "                    \"HOSP_LOCATION\", \"HOSP_TEACH\", \"HOSP_REGION\", \"HOSP_NASS\", \"DISCWT\")\n",
    "  available_vars <- analysis_vars[analysis_vars %in% names(NASS_SANDBOX)]\n",
    "  \n",
    "  cat(\"Analysis variables available:\", length(available_vars), \"of\", length(analysis_vars), \"\\n\")\n",
    "  cat(\"Variables:\", paste(available_vars, collapse = \", \"), \"\\n\")\n",
    "  \n",
    "  # Create complete case dataset\n",
    "  analysis_data <- NASS_SANDBOX[, ..available_vars]\n",
    "  analysis_data <- analysis_data[complete.cases(analysis_data)]\n",
    "  \n",
    "  cat(\"Complete cases for analysis:\", nrow(analysis_data), \"\\n\")\n",
    "  \n",
    "  if(nrow(analysis_data) > 100) {  # Minimum sample size check\n",
    "    \n",
    "    # ===== MODEL 1: Basic Logistic Regression =====\n",
    "    cat(\"\\n--- Model 1: Basic Logistic Regression ---\\n\")\n",
    "    \n",
    "    if(all(c(\"WHITE\", \"AGE\", \"FEMALE\", \"ZIPINC_QRTL\") %in% names(analysis_data))) {\n",
    "      model1 <- glm(WHITE ~ AGE + FEMALE + ZIPINC_QRTL, \n",
    "                   data = analysis_data, \n",
    "                   family = binomial())\n",
    "      \n",
    "      cat(\"Basic model fitted successfully\\n\")\n",
    "      print(summary(model1))\n",
    "      \n",
    "      # Model performance\n",
    "      if(require(\"performance\", quietly = TRUE)) {\n",
    "        cat(\"\\nModel Performance:\\n\")\n",
    "        print(model_performance(model1))\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    # ===== MODEL 2: Multilevel Model (if hospital ID available) =====\n",
    "    if(\"HOSP_NASS\" %in% names(analysis_data) && require(\"lme4\", quietly = TRUE)) {\n",
    "      cat(\"\\n--- Model 2: Multilevel Logistic Regression ---\\n\")\n",
    "      \n",
    "      # Check hospital variation\n",
    "      hospital_counts <- analysis_data[, .N, by = HOSP_NASS][order(-N)]\n",
    "      cat(\"Number of hospitals:\", nrow(hospital_counts), \"\\n\")\n",
    "      cat(\"Patients per hospital (top 5):\\n\")\n",
    "      print(head(hospital_counts))\n",
    "      \n",
    "      # Fit multilevel model if sufficient clustering\n",
    "      if(nrow(hospital_counts) >= 5) {\n",
    "        tryCatch({\n",
    "          model2 <- glmer(WHITE ~ AGE + FEMALE + ZIPINC_QRTL + \n",
    "                         HOSP_LOCATION + HOSP_TEACH + \n",
    "                         (1 | HOSP_NASS), \n",
    "                         data = analysis_data, \n",
    "                         family = binomial(),\n",
    "                         control = glmerControl(optimizer = \"bobyqa\"))\n",
    "          \n",
    "          cat(\"Multilevel model fitted successfully\\n\")\n",
    "          print(summary(model2))\n",
    "          \n",
    "          # Random effects\n",
    "          cat(\"\\nRandom Effects Summary:\\n\")\n",
    "          print(VarCorr(model2))\n",
    "          \n",
    "          # Model comparison\n",
    "          if(exists(\"model1\")) {\n",
    "            cat(\"\\nModel Comparison (AIC):\\n\")\n",
    "            cat(\"Basic model AIC:\", AIC(model1), \"\\n\")\n",
    "            cat(\"Multilevel model AIC:\", AIC(model2), \"\\n\")\n",
    "          }\n",
    "          \n",
    "        }, error = function(e) {\n",
    "          cat(\"Multilevel model failed:\", e$message, \"\\n\")\n",
    "        })\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    # ===== MODEL 3: Survey-Weighted Analysis =====\n",
    "    if(\"DISCWT\" %in% names(analysis_data) && require(\"survey\", quietly = TRUE)) {\n",
    "      cat(\"\\n--- Model 3: Survey-Weighted Analysis ---\\n\")\n",
    "      \n",
    "      # Create survey design\n",
    "      survey_design <- svydesign(ids = ~1, \n",
    "                                weights = ~DISCWT, \n",
    "                                data = analysis_data)\n",
    "      \n",
    "      cat(\"Survey design created with\", nrow(analysis_data), \"observations\\n\")\n",
    "      \n",
    "      # Weighted descriptive statistics\n",
    "      cat(\"\\nWeighted Descriptive Statistics:\\n\")\n",
    "      if(\"WHITE\" %in% names(analysis_data)) {\n",
    "        white_mean <- svymean(~WHITE, design = survey_design)\n",
    "        cat(\"Weighted WHITE proportion:\", round(coef(white_mean), 3), \n",
    "            \"±\", round(SE(white_mean), 3), \"\\n\")\n",
    "      }\n",
    "      \n",
    "      if(\"AGE\" %in% names(analysis_data)) {\n",
    "        age_mean <- svymean(~AGE, design = survey_design)\n",
    "        cat(\"Weighted AGE mean:\", round(coef(age_mean), 1), \n",
    "            \"±\", round(SE(age_mean), 1), \"\\n\")\n",
    "      }\n",
    "      \n",
    "      # Weighted regression\n",
    "      if(all(c(\"WHITE\", \"AGE\", \"FEMALE\", \"ZIPINC_QRTL\") %in% names(analysis_data))) {\n",
    "        tryCatch({\n",
    "          model3 <- svyglm(WHITE ~ AGE + FEMALE + ZIPINC_QRTL, \n",
    "                          design = survey_design, \n",
    "                          family = binomial())\n",
    "          \n",
    "          cat(\"\\nWeighted logistic regression:\\n\")\n",
    "          print(summary(model3))\n",
    "          \n",
    "        }, error = function(e) {\n",
    "          cat(\"Survey-weighted model failed:\", e$message, \"\\n\")\n",
    "        })\n",
    "      }\n",
    "    }\n",
    "    \n",
    "  } else {\n",
    "    cat(\"❌ Insufficient complete cases for analysis\\n\")\n",
    "  }\n",
    "  \n",
    "} else {\n",
    "  cat(\"❌ Required data or packages not available\\n\")\n",
    "}\n",
    "\n",
    "cat(\"\\n=== MULTILEVEL ANALYSIS COMPLETE ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c8db04",
   "metadata": {},
   "source": [
    "## Advanced Demographic Stratification\n",
    "\n",
    "Perform detailed stratified analyses by key demographic variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d91075",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Advanced Demographic Stratification Analysis\n",
    "cat(\"=== DEMOGRAPHIC STRATIFICATION ANALYSIS ===\\n\")\n",
    "\n",
    "if(exists(\"NASS_SANDBOX\")) {\n",
    "  \n",
    "  # ===== STRATIFIED ANALYSIS BY REGION =====\n",
    "  if(all(c(\"WHITE\", \"HOSP_REGION\", \"AGE\", \"FEMALE\", \"DISCWT\") %in% names(NASS_SANDBOX))) {\n",
    "    cat(\"\\n--- Regional Stratification ---\\n\")\n",
    "    \n",
    "    # Basic regional statistics\n",
    "    regional_stats <- NASS_SANDBOX[!is.na(WHITE) & !is.na(HOSP_REGION), \n",
    "                                  .(n = .N,\n",
    "                                    white_prop = mean(WHITE, na.rm = TRUE),\n",
    "                                    age_mean = mean(AGE, na.rm = TRUE),\n",
    "                                    female_prop = mean(as.numeric(FEMALE), na.rm = TRUE)),\n",
    "                                  by = HOSP_REGION]\n",
    "    \n",
    "    cat(\"Regional Demographics:\\n\")\n",
    "    print(regional_stats)\n",
    "    \n",
    "    # Survey-weighted regional analysis\n",
    "    if(require(\"survey\", quietly = TRUE)) {\n",
    "      survey_design <- svydesign(ids = ~1, weights = ~DISCWT, data = NASS_SANDBOX)\n",
    "      \n",
    "      cat(\"\\nWeighted Regional Analysis:\\n\")\n",
    "      regional_weighted <- svyby(~WHITE, ~HOSP_REGION, design = survey_design, FUN = svymean, na.rm = TRUE)\n",
    "      print(regional_weighted)\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # ===== INTERSECTIONAL ANALYSIS =====\n",
    "  if(all(c(\"WHITE\", \"FEMALE\", \"ZIPINC_QRTL\", \"AGE\") %in% names(NASS_SANDBOX))) {\n",
    "    cat(\"\\n--- Intersectional Analysis: Gender × Income ---\\n\")\n",
    "    \n",
    "    # Create income groups\n",
    "    NASS_SANDBOX[, INCOME_GROUP := case_when(\n",
    "      ZIPINC_QRTL == 1 ~ \"Low\",\n",
    "      ZIPINC_QRTL == 2 ~ \"Low-Mid\", \n",
    "      ZIPINC_QRTL == 3 ~ \"High-Mid\",\n",
    "      ZIPINC_QRTL == 4 ~ \"High\",\n",
    "      TRUE ~ \"Unknown\"\n",
    "    )]\n",
    "    \n",
    "    # Intersectional statistics\n",
    "    intersectional_stats <- NASS_SANDBOX[!is.na(WHITE) & !is.na(FEMALE) & !is.na(INCOME_GROUP), \n",
    "                                        .(n = .N,\n",
    "                                          white_prop = mean(WHITE, na.rm = TRUE),\n",
    "                                          age_mean = mean(AGE, na.rm = TRUE)),\n",
    "                                        by = .(FEMALE, INCOME_GROUP)]\n",
    "    \n",
    "    cat(\"Gender × Income Demographics:\\n\")\n",
    "    print(intersectional_stats)\n",
    "    \n",
    "    # Statistical test for interaction\n",
    "    if(nrow(intersectional_stats) > 4) {\n",
    "      interaction_test <- glm(WHITE ~ FEMALE * INCOME_GROUP, \n",
    "                             data = NASS_SANDBOX[!is.na(WHITE) & !is.na(FEMALE) & INCOME_GROUP != \"Unknown\"], \n",
    "                             family = binomial())\n",
    "      \n",
    "      cat(\"\\nInteraction Test (Female × Income):\\n\")\n",
    "      print(summary(interaction_test))\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # ===== AGE-STRATIFIED ANALYSIS =====\n",
    "  if(all(c(\"WHITE\", \"AGE\", \"PAY1\") %in% names(NASS_SANDBOX))) {\n",
    "    cat(\"\\n--- Age-Stratified Analysis ---\\n\")\n",
    "    \n",
    "    # Create age groups\n",
    "    NASS_SANDBOX[, AGE_CATEGORY := case_when(\n",
    "      AGE < 18 ~ \"Pediatric\",\n",
    "      AGE >= 18 & AGE < 65 ~ \"Adult\",\n",
    "      AGE >= 65 ~ \"Elderly\",\n",
    "      TRUE ~ \"Unknown\"\n",
    "    )]\n",
    "    \n",
    "    # Age group statistics by payer\n",
    "    age_payer_stats <- NASS_SANDBOX[!is.na(WHITE) & !is.na(AGE_CATEGORY) & !is.na(PAY1), \n",
    "                                   .(n = .N,\n",
    "                                     white_prop = mean(WHITE, na.rm = TRUE)),\n",
    "                                   by = .(AGE_CATEGORY, PAY1)]\n",
    "    \n",
    "    cat(\"Age Group × Payer Demographics:\\n\")\n",
    "    print(age_payer_stats[order(AGE_CATEGORY, PAY1)])\n",
    "    \n",
    "    # Test for age differences in white proportion\n",
    "    if(length(unique(NASS_SANDBOX$AGE_CATEGORY)) > 2) {\n",
    "      age_test <- aov(as.numeric(WHITE) ~ AGE_CATEGORY, \n",
    "                     data = NASS_SANDBOX[!is.na(WHITE) & AGE_CATEGORY != \"Unknown\"])\n",
    "      \n",
    "      cat(\"\\nAge Group Differences in White Proportion:\\n\")\n",
    "      print(summary(age_test))\n",
    "    }\n",
    "  }\n",
    "  \n",
    "} else {\n",
    "  cat(\"❌ NASS_SANDBOX not available\\n\")\n",
    "}\n",
    "\n",
    "cat(\"\\n=== DEMOGRAPHIC STRATIFICATION COMPLETE ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321c6272",
   "metadata": {},
   "source": [
    "# 4. Advanced Visualization Techniques\n",
    "\n",
    "Create complex visualizations using ggplot2, plotly, and other advanced plotting libraries for demographic and procedural analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85349b35",
   "metadata": {},
   "source": [
    "## Advanced Visualization Package Setup\n",
    "\n",
    "Install and configure advanced plotting libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dde260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python plotting packages\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(\"✅ Python plotting packages available\")\n",
    "    PYTHON_VIZ_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Python plotting packages missing: {e}\")\n",
    "    print(\"Installing plotly and seaborn...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'plotly', 'seaborn'])\n",
    "    try:\n",
    "        import plotly.express as px\n",
    "        import plotly.graph_objects as go\n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "        PYTHON_VIZ_AVAILABLE = True\n",
    "        print(\"✅ Python plotting packages installed\")\n",
    "    except:\n",
    "        PYTHON_VIZ_AVAILABLE = False\n",
    "        print(\"❌ Python plotting installation failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28f08ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Advanced R visualization packages\n",
    "viz_packages <- c(\n",
    "  \"ggplot2\",       # Core plotting\n",
    "  \"plotly\",        # Interactive plots\n",
    "  \"gganimate\",     # Animated plots  \n",
    "  \"ggridges\",      # Ridge plots\n",
    "  \"ggalluvial\",    # Alluvial diagrams\n",
    "  \"corrplot\",      # Correlation plots\n",
    "  \"pheatmap\",      # Heatmaps\n",
    "  \"RColorBrewer\",  # Color palettes\n",
    "  \"viridis\",       # Color scales\n",
    "  \"patchwork\"      # Plot composition\n",
    ")\n",
    "\n",
    "cat(\"Setting up advanced visualization packages...\\n\")\n",
    "\n",
    "for(pkg in viz_packages) {\n",
    "  if(!require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "    cat(\"Installing\", pkg, \"...\\n\")\n",
    "    tryCatch({\n",
    "      install.packages(pkg, quiet = TRUE)\n",
    "      library(pkg, character.only = TRUE, quietly = TRUE)\n",
    "      cat(\"✅\", pkg, \"ready\\n\")\n",
    "    }, error = function(e) {\n",
    "      cat(\"⚠️\", pkg, \"installation failed\\n\")\n",
    "    })\n",
    "  } else {\n",
    "    cat(\"✅\", pkg, \"ready\\n\")\n",
    "  }\n",
    "}\n",
    "\n",
    "# Set up advanced theme\n",
    "theme_advanced <- theme_minimal() +\n",
    "  theme(\n",
    "    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n",
    "    plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray40\"),\n",
    "    axis.title = element_text(size = 12, face = \"bold\"),\n",
    "    axis.text = element_text(size = 10),\n",
    "    legend.title = element_text(size = 11, face = \"bold\"),\n",
    "    legend.text = element_text(size = 10),\n",
    "    strip.text = element_text(size = 11, face = \"bold\"),\n",
    "    panel.grid.minor = element_blank(),\n",
    "    plot.background = element_rect(fill = \"white\", color = NA),\n",
    "    panel.background = element_rect(fill = \"white\", color = NA)\n",
    "  )\n",
    "\n",
    "cat(\"\\nAdvanced visualization setup complete\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f21c72",
   "metadata": {},
   "source": [
    "## Interactive Demographics Dashboard\n",
    "\n",
    "Create interactive visualizations for demographic exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badc9932",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Interactive Demographics Dashboard\n",
    "cat(\"=== CREATING INTERACTIVE DEMOGRAPHICS DASHBOARD ===\\n\")\n",
    "\n",
    "if(exists(\"NASS_SANDBOX\") && require(\"ggplot2\", quietly = TRUE)) {\n",
    "  \n",
    "  # ===== PLOT 1: Multi-dimensional Demographics =====\n",
    "  if(all(c(\"AGE\", \"WHITE\", \"FEMALE\", \"ZIPINC_QRTL\", \"HOSP_REGION\") %in% names(NASS_SANDBOX))) {\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    plot_data <- NASS_SANDBOX[!is.na(AGE) & !is.na(WHITE) & !is.na(FEMALE) & \n",
    "                             !is.na(ZIPINC_QRTL) & !is.na(HOSP_REGION)]\n",
    "    \n",
    "    # Sample data if too large (for performance)\n",
    "    if(nrow(plot_data) > 10000) {\n",
    "      set.seed(42)\n",
    "      plot_data <- plot_data[sample(.N, 10000)]\n",
    "      cat(\"Sampled 10,000 observations for plotting\\n\")\n",
    "    }\n",
    "    \n",
    "    # Create income labels\n",
    "    plot_data[, INCOME_LABEL := factor(ZIPINC_QRTL, \n",
    "                                      levels = 1:4, \n",
    "                                      labels = c(\"Q1-Low\", \"Q2\", \"Q3\", \"Q4-High\"))]\n",
    "    \n",
    "    # Create region labels\n",
    "    plot_data[, REGION_LABEL := factor(HOSP_REGION,\n",
    "                                      levels = 1:4,\n",
    "                                      labels = c(\"Northeast\", \"Midwest\", \"South\", \"West\"))]\n",
    "    \n",
    "    p1 <- ggplot(plot_data, aes(x = AGE, y = as.numeric(WHITE), \n",
    "                               color = INCOME_LABEL, shape = factor(FEMALE))) +\n",
    "      geom_point(alpha = 0.6, size = 1.5) +\n",
    "      geom_smooth(method = \"loess\", se = FALSE, linewidth = 1) +\n",
    "      facet_wrap(~REGION_LABEL, ncol = 2) +\n",
    "      theme_advanced +\n",
    "      labs(\n",
    "        title = \"Multi-Dimensional Demographics Analysis\",\n",
    "        subtitle = \"White Proportion by Age, Income, Gender, and Region\",\n",
    "        x = \"Age (years)\",\n",
    "        y = \"White (1=Yes, 0=No)\",\n",
    "        color = \"Income Quartile\",\n",
    "        shape = \"Gender\"\n",
    "      ) +\n",
    "      scale_color_viridis_d(option = \"plasma\") +\n",
    "      scale_shape_manual(values = c(\"0\" = 16, \"1\" = 17), \n",
    "                        labels = c(\"Male\", \"Female\")) +\n",
    "      ylim(0, 1)\n",
    "    \n",
    "    print(p1)\n",
    "    cat(\"Multi-dimensional demographics plot created\\n\")\n",
    "  }\n",
    "  \n",
    "  # ===== PLOT 2: Ridge Plot for Age Distributions =====\n",
    "  if(require(\"ggridges\", quietly = TRUE) && \n",
    "     all(c(\"AGE\", \"PAY1\", \"WHITE\") %in% names(NASS_SANDBOX))) {\n",
    "    \n",
    "    # Prepare data\n",
    "    ridge_data <- NASS_SANDBOX[!is.na(AGE) & !is.na(PAY1) & PAY1 %in% 1:6]\n",
    "    \n",
    "    # Create payer labels\n",
    "    ridge_data[, PAYER_LABEL := factor(PAY1,\n",
    "                                      levels = 1:6,\n",
    "                                      labels = c(\"Medicare\", \"Medicaid\", \"Private\", \n",
    "                                               \"Self-pay\", \"No Charge\", \"Other\"))]\n",
    "    \n",
    "    p2 <- ggplot(ridge_data, aes(x = AGE, y = PAYER_LABEL, fill = factor(WHITE))) +\n",
    "      geom_density_ridges(alpha = 0.7, scale = 2) +\n",
    "      theme_advanced +\n",
    "      labs(\n",
    "        title = \"Age Distribution by Payer Type and Race\",\n",
    "        subtitle = \"Density ridges showing age patterns across payment methods\",\n",
    "        x = \"Age (years)\",\n",
    "        y = \"Primary Payer\",\n",
    "        fill = \"Race\"\n",
    "      ) +\n",
    "      scale_fill_manual(values = c(\"0\" = \"#E31A1C\", \"1\" = \"#1F78B4\"),\n",
    "                       labels = c(\"Non-White\", \"White\")) +\n",
    "      xlim(0, 100)\n",
    "    \n",
    "    print(p2)\n",
    "    cat(\"Ridge plot for age distributions created\\n\")\n",
    "  }\n",
    "  \n",
    "  # ===== PLOT 3: Correlation Heatmap =====\n",
    "  if(require(\"corrplot\", quietly = TRUE)) {\n",
    "    \n",
    "    # Select numeric variables for correlation\n",
    "    numeric_vars <- c(\"AGE\", \"WHITE\", \"FEMALE\", \"ZIPINC_QRTL\", \"PAY1\", \n",
    "                     \"HOSP_LOCATION\", \"HOSP_TEACH\", \"HOSP_REGION\")\n",
    "    available_numeric <- numeric_vars[numeric_vars %in% names(NASS_SANDBOX)]\n",
    "    \n",
    "    if(length(available_numeric) >= 3) {\n",
    "      cor_data <- NASS_SANDBOX[, ..available_numeric]\n",
    "      cor_data <- cor_data[complete.cases(cor_data)]\n",
    "      \n",
    "      # Convert logical to numeric\n",
    "      for(col in names(cor_data)) {\n",
    "        if(is.logical(cor_data[[col]])) {\n",
    "          cor_data[, (col) := as.numeric(get(col))]\n",
    "        }\n",
    "      }\n",
    "      \n",
    "      cor_matrix <- cor(cor_data, use = \"complete.obs\")\n",
    "      \n",
    "      # Create correlation plot\n",
    "      par(mfrow = c(1, 1))\n",
    "      corrplot(cor_matrix, \n",
    "               method = \"color\",\n",
    "               type = \"upper\", \n",
    "               order = \"hclust\",\n",
    "               col = colorRampPalette(c(\"#E31A1C\", \"white\", \"#1F78B4\"))(100),\n",
    "               tl.cex = 0.8,\n",
    "               tl.col = \"black\",\n",
    "               addCoef.col = \"black\",\n",
    "               number.cex = 0.7,\n",
    "               title = \"Variable Correlation Matrix\")\n",
    "      \n",
    "      cat(\"Correlation heatmap created\\n\")\n",
    "    }\n",
    "  }\n",
    "  \n",
    "} else {\n",
    "  cat(\"❌ Required data or packages not available for visualization\\n\")\n",
    "}\n",
    "\n",
    "cat(\"\\n=== INTERACTIVE DASHBOARD COMPLETE ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ce4fa6",
   "metadata": {},
   "source": [
    "## Procedure-Specific Visualizations\n",
    "\n",
    "Advanced visualizations focusing on surgical procedures and utilization patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18df301",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Procedure-Specific Advanced Visualizations\n",
    "cat(\"=== PROCEDURE-SPECIFIC VISUALIZATIONS ===\\n\")\n",
    "\n",
    "if(exists(\"NASS_SANDBOX\") && all(c(\"CPTCCS1\", \"WHITE\", \"AGE\", \"ZIPINC_QRTL\") %in% names(NASS_SANDBOX))) {\n",
    "  \n",
    "  # ===== Top Procedures Analysis =====\n",
    "  top_procedures <- NASS_SANDBOX[, .N, by = CPTCCS1][order(-N)][1:10]\n",
    "  cat(\"Analyzing top\", nrow(top_procedures), \"procedures\\n\")\n",
    "  \n",
    "  # Filter data for top procedures\n",
    "  proc_data <- NASS_SANDBOX[CPTCCS1 %in% top_procedures$CPTCCS1]\n",
    "  \n",
    "  # ===== PLOT 1: Procedure Utilization by Demographics =====\n",
    "  if(require(\"ggalluvial\", quietly = TRUE)) {\n",
    "    \n",
    "    # Prepare alluvial data\n",
    "    alluvial_data <- proc_data[!is.na(WHITE) & !is.na(ZIPINC_QRTL), \n",
    "                              .(count = .N),\n",
    "                              by = .(CPTCCS1, WHITE, ZIPINC_QRTL)]\n",
    "    \n",
    "    # Filter to top 5 procedures for readability\n",
    "    top5_procs <- top_procedures$CPTCCS1[1:5]\n",
    "    alluvial_data <- alluvial_data[CPTCCS1 %in% top5_procs]\n",
    "    \n",
    "    # Create labels\n",
    "    alluvial_data[, RACE_LABEL := ifelse(WHITE == 1, \"White\", \"Non-White\")]\n",
    "    alluvial_data[, INCOME_LABEL := paste0(\"Q\", ZIPINC_QRTL)]\n",
    "    \n",
    "    p3 <- ggplot(alluvial_data, \n",
    "                aes(axis1 = CPTCCS1, axis2 = RACE_LABEL, axis3 = INCOME_LABEL, y = count)) +\n",
    "      geom_alluvium(aes(fill = RACE_LABEL), alpha = 0.7) +\n",
    "      geom_stratum(alpha = 0.8, color = \"white\") +\n",
    "      geom_text(stat = \"stratum\", aes(label = after_stat(stratum)), size = 3) +\n",
    "      theme_advanced +\n",
    "      theme(axis.text.x = element_blank(),\n",
    "            axis.ticks.x = element_blank(),\n",
    "            panel.grid = element_blank()) +\n",
    "      labs(\n",
    "        title = \"Patient Flow: Top Procedures → Race → Income\",\n",
    "        subtitle = \"Alluvial diagram showing demographic patterns in procedure utilization\",\n",
    "        y = \"Number of Patients\",\n",
    "        fill = \"Race\"\n",
    "      ) +\n",
    "      scale_x_discrete(limits = c(\"Procedure\", \"Race\", \"Income Quartile\")) +\n",
    "      scale_fill_manual(values = c(\"Non-White\" = \"#E31A1C\", \"White\" = \"#1F78B4\"))\n",
    "    \n",
    "    print(p3)\n",
    "    cat(\"Alluvial diagram created\\n\")\n",
    "  }\n",
    "  \n",
    "  # ===== PLOT 2: Age Distribution by Procedure =====\n",
    "  p4 <- ggplot(proc_data[CPTCCS1 %in% top_procedures$CPTCCS1[1:8]], \n",
    "              aes(x = AGE, fill = factor(CPTCCS1))) +\n",
    "    geom_density(alpha = 0.6) +\n",
    "    facet_wrap(~CPTCCS1, scales = \"free_y\", ncol = 2) +\n",
    "    theme_advanced +\n",
    "    theme(legend.position = \"none\") +\n",
    "    labs(\n",
    "      title = \"Age Distribution by Surgical Procedure\",\n",
    "      subtitle = \"Density plots showing age patterns for top procedures\",\n",
    "      x = \"Age (years)\",\n",
    "      y = \"Density\"\n",
    "    ) +\n",
    "    scale_fill_viridis_d(option = \"turbo\") +\n",
    "    xlim(0, 100)\n",
    "  \n",
    "  print(p4)\n",
    "  cat(\"Age distribution by procedure created\\n\")\n",
    "  \n",
    "  # ===== PLOT 3: Procedure Complexity Heatmap =====\n",
    "  if(require(\"pheatmap\", quietly = TRUE) && \"TOTCHG\" %in% names(NASS_SANDBOX)) {\n",
    "    \n",
    "    # Calculate procedure complexity metrics\n",
    "    complexity_data <- proc_data[!is.na(TOTCHG) & TOTCHG > 0, \n",
    "                                .(mean_age = mean(AGE, na.rm = TRUE),\n",
    "                                  mean_charge = mean(TOTCHG, na.rm = TRUE),\n",
    "                                  white_prop = mean(WHITE, na.rm = TRUE),\n",
    "                                  female_prop = mean(as.numeric(FEMALE), na.rm = TRUE),\n",
    "                                  count = .N),\n",
    "                                by = CPTCCS1]\n",
    "    \n",
    "    # Filter for procedures with sufficient volume\n",
    "    complexity_data <- complexity_data[count >= 50]\n",
    "    \n",
    "    if(nrow(complexity_data) >= 3) {\n",
    "      # Prepare matrix for heatmap\n",
    "      heatmap_matrix <- as.matrix(complexity_data[, .(mean_age, mean_charge, white_prop, female_prop)])\n",
    "      rownames(heatmap_matrix) <- complexity_data$CPTCCS1\n",
    "      colnames(heatmap_matrix) <- c(\"Mean Age\", \"Mean Charge\", \"White Prop\", \"Female Prop\")\n",
    "      \n",
    "      # Scale the data\n",
    "      heatmap_matrix_scaled <- scale(heatmap_matrix)\n",
    "      \n",
    "      # Create heatmap\n",
    "      pheatmap(heatmap_matrix_scaled,\n",
    "               main = \"Procedure Complexity and Demographics Heatmap\",\n",
    "               color = colorRampPalette(c(\"#E31A1C\", \"white\", \"#1F78B4\"))(100),\n",
    "               cellwidth = 20,\n",
    "               cellheight = 15,\n",
    "               fontsize = 10)\n",
    "      \n",
    "      cat(\"Procedure complexity heatmap created\\n\")\n",
    "    }\n",
    "  }\n",
    "  \n",
    "} else {\n",
    "  cat(\"❌ Required procedure data not available\\n\")\n",
    "}\n",
    "\n",
    "cat(\"\\n=== PROCEDURE VISUALIZATIONS COMPLETE ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3072ff68",
   "metadata": {},
   "source": [
    "# 5. Machine Learning Pipeline Development\n",
    "\n",
    "Build and test machine learning models for predicting ambulatory surgery usage patterns using demographic and socioeconomic variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f04cfef",
   "metadata": {},
   "source": [
    "## Machine Learning Package Setup\n",
    "\n",
    "Install and configure machine learning libraries for both Python and R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f6bd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python ML packages\n",
    "ml_packages = ['scikit-learn', 'xgboost', 'lightgbm', 'shap']\n",
    "\n",
    "for pkg in ml_packages:\n",
    "    try:\n",
    "        __import__(pkg.replace('-', '_'))\n",
    "        print(f\"✅ {pkg} available\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {pkg}...\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg])\n",
    "        print(f\"✅ {pkg} installed\")\n",
    "\n",
    "# Import ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "print(\"✅ Python ML environment ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1facf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# R Machine Learning packages\n",
    "ml_r_packages <- c(\n",
    "  \"randomForest\",   # Random Forest\n",
    "  \"caret\",          # ML framework\n",
    "  \"xgboost\",        # Gradient boosting\n",
    "  \"pROC\",           # ROC analysis\n",
    "  \"ROCR\",           # ROC curves\n",
    "  \"e1071\",          # SVM\n",
    "  \"glmnet\"          # Regularized regression\n",
    ")\n",
    "\n",
    "cat(\"Setting up R machine learning packages...\\n\")\n",
    "\n",
    "for(pkg in ml_r_packages) {\n",
    "  if(!require(pkg, character.only = TRUE, quietly = TRUE)) {\n",
    "    cat(\"Installing\", pkg, \"...\\n\")\n",
    "    tryCatch({\n",
    "      install.packages(pkg, quiet = TRUE, dependencies = TRUE)\n",
    "      library(pkg, character.only = TRUE, quietly = TRUE)\n",
    "      cat(\"✅\", pkg, \"ready\\n\")\n",
    "    }, error = function(e) {\n",
    "      cat(\"⚠️\", pkg, \"installation failed\\n\")\n",
    "    })\n",
    "  } else {\n",
    "    cat(\"✅\", pkg, \"ready\\n\")\n",
    "  }\n",
    "}\n",
    "\n",
    "cat(\"R ML environment setup complete\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229c855d",
   "metadata": {},
   "source": [
    "## Feature Engineering and Data Preparation\n",
    "\n",
    "Prepare features for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dd1a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for ML\n",
    "if df_sandbox is not None:\n",
    "    print(\"=== FEATURE ENGINEERING FOR MACHINE LEARNING ===\")\n",
    "    \n",
    "    # Create ML dataset\n",
    "    ml_data = df_sandbox.copy()\n",
    "    \n",
    "    # Target variable: WHITE (race prediction)\n",
    "    if 'WHITE' in ml_data.columns:\n",
    "        target_col = 'WHITE'\n",
    "        print(f\"Target variable: {target_col}\")\n",
    "        print(f\"Target distribution: {ml_data[target_col].value_counts().to_dict()}\")\n",
    "    else:\n",
    "        print(\"❌ Target variable WHITE not found\")\n",
    "        target_col = None\n",
    "    \n",
    "    if target_col:\n",
    "        # Feature selection\n",
    "        feature_cols = [\n",
    "            'AGE', 'FEMALE', 'ZIPINC_QRTL', 'PAY1',\n",
    "            'HOSP_LOCATION', 'HOSP_TEACH', 'HOSP_REGION'\n",
    "        ]\n",
    "        \n",
    "        # Check which features are available\n",
    "        available_features = [col for col in feature_cols if col in ml_data.columns]\n",
    "        print(f\"Available features: {available_features}\")\n",
    "        \n",
    "        if len(available_features) >= 3:\n",
    "            # Create feature matrix\n",
    "            X = ml_data[available_features].copy()\n",
    "            y = ml_data[target_col].copy()\n",
    "            \n",
    "            # Handle missing values\n",
    "            X = X.fillna(X.median() if X.dtypes.name in ['int64', 'float64'] else X.mode().iloc[0])\n",
    "            \n",
    "            # Convert categorical variables\n",
    "            categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "            for col in categorical_cols:\n",
    "                le = LabelEncoder()\n",
    "                X[col] = le.fit_transform(X[col].astype(str))\n",
    "            \n",
    "            # Ensure all columns are numeric\n",
    "            for col in X.columns:\n",
    "                X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "            X = X.fillna(0)\n",
    "            \n",
    "            # Remove any remaining missing values in target\n",
    "            valid_idx = ~y.isna()\n",
    "            X = X[valid_idx]\n",
    "            y = y[valid_idx]\n",
    "            \n",
    "            print(f\"Final dataset shape: {X.shape}\")\n",
    "            print(f\"Features: {list(X.columns)}\")\n",
    "            \n",
    "            # Train-test split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42, stratify=y\n",
    "            )\n",
    "            \n",
    "            print(f\"Training set: {X_train.shape}\")\n",
    "            print(f\"Test set: {X_test.shape}\")\n",
    "            \n",
    "            # Store for R analysis\n",
    "            globals()['X_train'] = X_train\n",
    "            globals()['X_test'] = X_test\n",
    "            globals()['y_train'] = y_train\n",
    "            globals()['y_test'] = y_test\n",
    "            \n",
    "        else:\n",
    "            print(\"❌ Insufficient features available\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No data available for feature engineering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70da2a88",
   "metadata": {},
   "source": [
    "## Python Machine Learning Models\n",
    "\n",
    "Build and evaluate multiple ML models using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a83f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Machine Learning Pipeline\n",
    "if 'X_train' in globals():\n",
    "    print(\"=== PYTHON MACHINE LEARNING MODELS ===\")\n",
    "    \n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Scale features for logistic regression\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n--- {name} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Use scaled data for logistic regression, original for tree-based\n",
    "            if 'Logistic' in name:\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                y_pred = model.predict(X_test_scaled)\n",
    "                y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Evaluate model\n",
    "            accuracy = model.score(X_test_scaled if 'Logistic' in name else X_test, y_test)\n",
    "            auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "            print(f\"Accuracy: {accuracy:.3f}\")\n",
    "            print(f\"AUC Score: {auc_score:.3f}\")\n",
    "            \n",
    "            # Classification report\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(y_test, y_pred))\n",
    "            \n",
    "            # Feature importance (for tree-based models)\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                feature_importance = pd.DataFrame({\n",
    "                    'feature': X_train.columns,\n",
    "                    'importance': model.feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                print(\"\\nTop Feature Importances:\")\n",
    "                print(feature_importance.head())\n",
    "            \n",
    "            results[name] = {\n",
    "                'model': model,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc_score,\n",
    "                'predictions': y_pred_proba\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Model {name} failed: {e}\")\n",
    "    \n",
    "    # Compare models\n",
    "    if results:\n",
    "        print(\"\\n=== MODEL COMPARISON ===\")\n",
    "        comparison = pd.DataFrame({\n",
    "            name: {'Accuracy': res['accuracy'], 'AUC': res['auc']}\n",
    "            for name, res in results.items()\n",
    "        }).T\n",
    "        \n",
    "        print(comparison.round(3))\n",
    "        \n",
    "        # Best model\n",
    "        best_model_name = comparison['AUC'].idxmax()\n",
    "        print(f\"\\nBest model by AUC: {best_model_name}\")\n",
    "        \n",
    "        # Store best model results\n",
    "        globals()['best_model_results'] = results[best_model_name]\n",
    "        globals()['model_comparison'] = comparison\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Training data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816dc05e",
   "metadata": {},
   "source": [
    "## R Machine Learning Analysis\n",
    "\n",
    "Complement Python analysis with R-based ML approaches, including survey-weighted models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc8039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i X_train -i X_test -i y_train -i y_test\n",
    "\n",
    "# R Machine Learning Analysis\n",
    "cat(\"=== R MACHINE LEARNING ANALYSIS ===\\n\")\n",
    "\n",
    "if(exists(\"X_train\") && require(\"randomForest\", quietly = TRUE) && require(\"caret\", quietly = TRUE)) {\n",
    "  \n",
    "  # Prepare data for R analysis\n",
    "  train_data <- data.frame(X_train, target = y_train)\n",
    "  test_data <- data.frame(X_test, target = y_test)\n",
    "  \n",
    "  cat(\"R ML data prepared:\\n\")\n",
    "  cat(\"Training:\", nrow(train_data), \"x\", ncol(train_data), \"\\n\")\n",
    "  cat(\"Testing:\", nrow(test_data), \"x\", ncol(test_data), \"\\n\")\n",
    "  \n",
    "  # ===== MODEL 1: Random Forest with caret =====\n",
    "  cat(\"\\n--- Random Forest (caret framework) ---\\n\")\n",
    "  \n",
    "  tryCatch({\n",
    "    # Set up cross-validation\n",
    "    ctrl <- trainControl(method = \"cv\", number = 5, \n",
    "                        summaryFunction = twoClassSummary,\n",
    "                        classProbs = TRUE)\n",
    "    \n",
    "    # Convert target to factor with proper levels for caret\n",
    "    train_data$target <- factor(ifelse(train_data$target == 1, \"White\", \"NonWhite\"))\n",
    "    test_data$target <- factor(ifelse(test_data$target == 1, \"White\", \"NonWhite\"))\n",
    "    \n",
    "    # Train random forest\n",
    "    rf_model <- train(target ~ ., \n",
    "                     data = train_data,\n",
    "                     method = \"rf\",\n",
    "                     trControl = ctrl,\n",
    "                     metric = \"ROC\",\n",
    "                     ntree = 100)\n",
    "    \n",
    "    cat(\"Random Forest model trained successfully\\n\")\n",
    "    print(rf_model)\n",
    "    \n",
    "    # Make predictions\n",
    "    rf_pred <- predict(rf_model, test_data, type = \"prob\")\n",
    "    rf_pred_class <- predict(rf_model, test_data)\n",
    "    \n",
    "    # Calculate AUC\n",
    "    if(require(\"pROC\", quietly = TRUE)) {\n",
    "      roc_obj <- roc(test_data$target, rf_pred$White)\n",
    "      auc_score <- auc(roc_obj)\n",
    "      cat(\"AUC Score:\", round(auc_score, 3), \"\\n\")\n",
    "    }\n",
    "    \n",
    "    # Feature importance\n",
    "    importance <- varImp(rf_model)\n",
    "    cat(\"\\nFeature Importance:\\n\")\n",
    "    print(importance)\n",
    "    \n",
    "  }, error = function(e) {\n",
    "    cat(\"Random Forest failed:\", e$message, \"\\n\")\n",
    "  })\n",
    "  \n",
    "  # ===== MODEL 2: Survey-Weighted Analysis =====\n",
    "  if(\"DISCWT\" %in% names(NASS_SANDBOX) && require(\"survey\", quietly = TRUE)) {\n",
    "    cat(\"\\n--- Survey-Weighted Logistic Regression ---\\n\")\n",
    "    \n",
    "    tryCatch({\n",
    "      # Prepare survey data\n",
    "      survey_data <- NASS_SANDBOX[!is.na(WHITE) & !is.na(AGE) & !is.na(FEMALE) & \n",
    "                                 !is.na(ZIPINC_QRTL) & !is.na(DISCWT)]\n",
    "      \n",
    "      # Create survey design\n",
    "      survey_design <- svydesign(ids = ~1, weights = ~DISCWT, data = survey_data)\n",
    "      \n",
    "      # Fit survey-weighted model\n",
    "      survey_model <- svyglm(WHITE ~ AGE + FEMALE + ZIPINC_QRTL + PAY1 + \n",
    "                            HOSP_LOCATION + HOSP_TEACH + HOSP_REGION,\n",
    "                            design = survey_design,\n",
    "                            family = binomial())\n",
    "      \n",
    "      cat(\"Survey-weighted model fitted\\n\")\n",
    "      print(summary(survey_model))\n",
    "      \n",
    "      # Pseudo R-squared for survey models\n",
    "      if(require(\"survey\", quietly = TRUE)) {\n",
    "        psrsq <- psrsq(survey_model)\n",
    "        cat(\"Pseudo R-squared:\", round(psrsq, 3), \"\\n\")\n",
    "      }\n",
    "      \n",
    "    }, error = function(e) {\n",
    "      cat(\"Survey-weighted model failed:\", e$message, \"\\n\")\n",
    "    })\n",
    "  }\n",
    "  \n",
    "  # ===== MODEL 3: Regularized Regression =====\n",
    "  if(require(\"glmnet\", quietly = TRUE)) {\n",
    "    cat(\"\\n--- Regularized Logistic Regression (glmnet) ---\\n\")\n",
    "    \n",
    "    tryCatch({\n",
    "      # Prepare matrices for glmnet\n",
    "      X_matrix <- as.matrix(X_train)\n",
    "      y_vector <- as.numeric(y_train)\n",
    "      \n",
    "      # Fit elastic net with cross-validation\n",
    "      cv_fit <- cv.glmnet(X_matrix, y_vector, family = \"binomial\", \n",
    "                         alpha = 0.5, nfolds = 5)\n",
    "      \n",
    "      cat(\"Elastic net model fitted\\n\")\n",
    "      cat(\"Best lambda:\", round(cv_fit$lambda.min, 4), \"\\n\")\n",
    "      cat(\"CV AUC:\", round(max(cv_fit$cvm), 3), \"\\n\")\n",
    "      \n",
    "      # Coefficients at best lambda\n",
    "      coef_best <- coef(cv_fit, s = \"lambda.min\")\n",
    "      cat(\"\\nNon-zero coefficients:\\n\")\n",
    "      print(coef_best[coef_best[,1] != 0, , drop = FALSE])\n",
    "      \n",
    "      # Predictions on test set\n",
    "      X_test_matrix <- as.matrix(X_test)\n",
    "      pred_probs <- predict(cv_fit, X_test_matrix, s = \"lambda.min\", type = \"response\")\n",
    "      \n",
    "      # Calculate test AUC\n",
    "      if(require(\"pROC\", quietly = TRUE)) {\n",
    "        test_roc <- roc(y_test, as.numeric(pred_probs))\n",
    "        test_auc <- auc(test_roc)\n",
    "        cat(\"Test AUC:\", round(test_auc, 3), \"\\n\")\n",
    "      }\n",
    "      \n",
    "    }, error = function(e) {\n",
    "      cat(\"Regularized regression failed:\", e$message, \"\\n\")\n",
    "    })\n",
    "  }\n",
    "  \n",
    "} else {\n",
    "  cat(\"❌ Required data or packages not available for R ML analysis\\n\")\n",
    "}\n",
    "\n",
    "cat(\"\\n=== R MACHINE LEARNING COMPLETE ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77773c9",
   "metadata": {},
   "source": [
    "## Model Interpretation and Visualization\n",
    "\n",
    "Create interpretable visualizations of model performance and feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586849a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Interpretation and Visualization\n",
    "if 'best_model_results' in globals() and PYTHON_VIZ_AVAILABLE:\n",
    "    print(\"=== MODEL INTERPRETATION AND VISUALIZATION ===\")\n",
    "    \n",
    "    # ROC Curves Comparison\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Subplot 1: ROC Curves\n",
    "    plt.subplot(1, 2, 1)\n",
    "    \n",
    "    for name, result in globals().get('results', {}).items():\n",
    "        if 'predictions' in result:\n",
    "            fpr, tpr, _ = roc_curve(y_test, result['predictions'])\n",
    "            plt.plot(fpr, tpr, label=f\"{name} (AUC={result['auc']:.3f})\")\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 2: Feature Importance (if available)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    \n",
    "    best_model = best_model_results['model']\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        feature_names = X_train.columns\n",
    "        importances = best_model.feature_importances_\n",
    "        \n",
    "        # Sort features by importance\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        plt.bar(range(len(importances)), importances[indices])\n",
    "        plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45)\n",
    "        plt.title('Feature Importance')\n",
    "        plt.ylabel('Importance')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Feature importance\\nnot available', \n",
    "                ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Feature Importance')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance Summary\n",
    "    print(\"\\n=== FINAL MODEL PERFORMANCE SUMMARY ===\")\n",
    "    if 'model_comparison' in globals():\n",
    "        print(\"Model Performance Comparison:\")\n",
    "        print(globals()['model_comparison'].round(3))\n",
    "    \n",
    "    print(f\"\\nBest Model: {best_model_results}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Model results not available for interpretation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a20227e",
   "metadata": {},
   "source": [
    "# 6. Additional Census Comparisons\n",
    "\n",
    "Extend Census API integration to include additional demographic variables and perform comprehensive population-level comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191e0b83",
   "metadata": {},
   "source": [
    "## Extended Census Variables\n",
    "\n",
    "Retrieve additional demographic variables from Census API for comprehensive comparison."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
