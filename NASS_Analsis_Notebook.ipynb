# %% [markdown]
# # NASS 2020 Comprehensive Analysis Notebook
# 
# This notebook performs complete analysis of the HCUP NASS 2020 dataset including:
# - Environment setup
# - Data loading and validation
# - Institutional and encounter analysis
# - Census benchmarking
# - Machine learning classifiers
# - Visualization and reporting

# %% [markdown]
# ## Part 1: Environment Setup and Data Loading

# %%
# Install required packages
import subprocess
import sys

def install_packages():
    """Install required packages if not already installed"""
    packages = [
        'pandas', 'numpy', 'matplotlib', 'seaborn', 'scikit-learn',
        'xgboost', 'plotly', 'geopandas', 'census', 'statsmodels',
        'reportlab', 'PyPDF2', 'rpy2'
    ]
    
    for package in packages:
        try:
            __import__(package)
        except ImportError:
            print(f"Installing {package}...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])

install_packages()

# %%
# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Statistical and ML libraries
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import xgboost as xgb

# For census data
import requests
import json

# For report generation
from reportlab.lib import colors
from reportlab.lib.pagesizes import letter, A0
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, Image
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch

# Set display options
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
plt.style.use('seaborn-v0_8-darkgrid')

print("Environment setup complete!")
print(f"Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# %% [markdown]
# ## Part 2: Data Loading and Initial Exploration

# %%
# Mount Google Drive (for Colab)
import os
from google.colab import drive

# Check if running in Colab
IN_COLAB = 'google.colab' in sys.modules

if IN_COLAB:
    drive.mount('/content/drive')
    DATA_PATH = '/content/drive/MyDrive/'
    SIMULATED_DATA = True  # Use simulated data for public demo
else:
    DATA_PATH = './'
    SIMULATED_DATA = False  # Use real data on local/VM

# Load data
if SIMULATED_DATA:
    print("Loading simulated NASS 2020 data...")
    df = pd.read_csv(os.path.join(DATA_PATH, 'nass_2020_simulated.csv'))
else:
    print("Loading full NASS 2020 data...")
    df = pd.read_csv(os.path.join(DATA_PATH, 'nass_2020_all.csv'))

print(f"Data loaded: {df.shape[0]:,} rows, {df.shape[1]} columns")
print(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

# %%
# Basic data exploration
print("Dataset Overview:")
print("-" * 50)
print(df.info())

print("\nFirst 5 rows:")
df.head()

# %%
# Key statistics
print("Key Statistics:")
print("-" * 50)
print(f"Number of hospitals: {df['HOSP_NASS'].nunique():,}")
print(f"Number of encounters: {len(df):,}")
print(f"Number of unique procedures: {df['CPTCCS1'].nunique():,}")
print(f"\nAge distribution:")
print(df['AGE'].describe())
print(f"\nTotal charges distribution:")
print(df['TOTCHG'].describe())

# Check for missing values
missing_pct = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False)
missing_significant = missing_pct[missing_pct > 5]
if len(missing_significant) > 0:
    print(f"\nColumns with >5% missing values:")
    print(missing_significant)

# %% [markdown]
# ## Part 3: Institutional Analysis

# %%
# Hospital characteristics analysis
hospital_stats = df.groupby('HOSP_NASS').agg({
    'KEY_NASS': 'count',
    'TOTCHG': 'mean',
    'AGE': 'mean'
}).rename(columns={'KEY_NASS': 'n_encounters', 'TOTCHG': 'avg_charge', 'AGE': 'avg_age'})

# Merge with hospital characteristics
hosp_chars = df[['HOSP_NASS', 'HOSP_TEACH', 'HOSP_LOCATION', 'HOSP_REGION', 
                 'HOSP_BEDSIZE_CAT', 'TOTAL_AS_ENCOUNTERS']].drop_duplicates('HOSP_NASS')
hospital_stats = hospital_stats.merge(hosp_chars, left_index=True, right_on='HOSP_NASS')

print("Hospital Statistics Summary:")
print(hospital_stats.describe())

# %%
# Visualize hospital distribution
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# Teaching status distribution
teaching_counts = hospital_stats['HOSP_TEACH'].value_counts()
axes[0, 0].bar(teaching_counts.index, teaching_counts.values)
axes[0, 0].set_title('Hospitals by Teaching Status')
axes[0, 0].set_xlabel('Teaching Status')
axes[0, 0].set_ylabel('Number of Hospitals')

# Regional distribution
region_counts = hospital_stats['HOSP_REGION'].value_counts()
axes[0, 1].bar(region_counts.index, region_counts.values)
axes[0, 1].set_title('Hospitals by Region')
axes[0, 1].set_xlabel('Region')
axes[0, 1].set_ylabel('Number of Hospitals')

# Bed size distribution
bedsize_counts = hospital_stats['HOSP_BEDSIZE_CAT'].value_counts()
axes[1, 0].bar(bedsize_counts.index, bedsize_counts.values)
axes[1, 0].set_title('Hospitals by Bed Size Category')
axes[1, 0].set_xlabel('Bed Size Category')
axes[1, 0].set_ylabel('Number of Hospitals')

# Volume distribution
axes[1, 1].hist(hospital_stats['n_encounters'], bins=50, edgecolor='black')
axes[1, 1].set_title('Distribution of Encounter Volumes')
axes[1, 1].set_xlabel('Number of Encounters')
axes[1, 1].set_ylabel('Number of Hospitals')

plt.tight_layout()
plt.savefig('hospital_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

# %% [markdown]
# ## Part 4: Encounter Analysis

# %%
# Top procedures analysis
procedure_counts = df['CPTCCS1'].value_counts().head(20)

plt.figure(figsize=(12, 8))
plt.barh(range(len(procedure_counts)), procedure_counts.values)
plt.yticks(range(len(procedure_counts)), [f"CCS {x}" for x in procedure_counts.index])
plt.xlabel('Number of Encounters')
plt.title('Top 20 Most Common Procedures')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.savefig('top_procedures.png', dpi=300, bbox_inches='tight')
plt.show()

print("Top 10 Procedures:")
for i, (proc, count) in enumerate(procedure_counts.head(10).items(), 1):
    print(f"{i}. CCS {proc}: {count:,} encounters ({count/len(df)*100:.1f}%)")

# %%
# Patient demographics analysis
demographics_cols = ['AGE', 'FEMALE', 'RACE', 'ZIPINC_QRTL', 'PAY1']

# Create demographic summary
demo_summary = pd.DataFrame()

# Age groups
age_groups = pd.cut(df['AGE'], bins=[0, 18, 40, 65, 80, 120], 
                    labels=['0-17', '18-39', '40-64', '65-79', '80+'])
demo_summary['Age Group'] = age_groups.value_counts().sort_index()

# Gender (if FEMALE column exists and is valid)
if 'FEMALE' in df.columns and df['FEMALE'].notna().any():
    gender_map = {0: 'Male', 1: 'Female'}
    demo_summary['Gender'] = df['FEMALE'].map(gender_map).value_counts()

# Race distribution
if 'RACE' in df.columns:
    race_map = {1: 'White', 2: 'Black', 3: 'Hispanic', 4: 'Asian/Pacific', 
                5: 'Native American', 6: 'Other'}
    race_dist = df['RACE'].map(race_map).value_counts()
    print("\nRace Distribution:")
    print(race_dist)

# Payer distribution
if 'PAY1' in df.columns:
    payer_map = {1: 'Medicare', 2: 'Medicaid', 3: 'Private', 
                 4: 'Self-pay', 5: 'No charge', 6: 'Other'}
    payer_dist = df['PAY1'].map(payer_map).value_counts()
    print("\nPayer Distribution:")
    print(payer_dist)

# %% [markdown]
# ## Part 5: Census Benchmarking

# %%
# Census data comparison (using sample data for demonstration)
# In production, this would connect to Census API

def get_census_demographics():
    """Get demographic data from Census API or use sample data"""
    # Sample census data for demonstration
    census_data = {
        'race': {
            'White': 0.601,
            'Black': 0.122,
            'Hispanic': 0.185,
            'Asian/Pacific': 0.056,
            'Native American': 0.009,
            'Other': 0.027
        },
        'age': {
            '0-17': 0.22,
            '18-39': 0.29,
            '40-64': 0.32,
            '65-79': 0.13,
            '80+': 0.04
        }
    }
    return census_data

census_data = get_census_demographics()

# Compare NASS to Census
if 'RACE' in df.columns:
    race_map = {1: 'White', 2: 'Black', 3: 'Hispanic', 4: 'Asian/Pacific', 
                5: 'Native American', 6: 'Other'}
    nass_race = df['RACE'].map(race_map).value_counts(normalize=True)
    
    # Create comparison dataframe
    race_comparison = pd.DataFrame({
        'NASS': nass_race,
        'Census': pd.Series(census_data['race'])
    }).fillna(0)
    
    # Visualize comparison
    fig, ax = plt.subplots(figsize=(10, 6))
    x = np.arange(len(race_comparison.index))
    width = 0.35
    
    ax.bar(x - width/2, race_comparison['NASS'], width, label='NASS 2020')
    ax.bar(x + width/2, race_comparison['Census'], width, label='US Census')
    
    ax.set_xlabel('Race/Ethnicity')
    ax.set_ylabel('Proportion')
    ax.set_title('NASS vs Census: Race Distribution')
    ax.set_xticks(x)
    ax.set_xticklabels(race_comparison.index, rotation=45, ha='right')
    ax.legend()
    
    plt.tight_layout()
    plt.savefig('race_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Calculate chi-square test
    from scipy.stats import chisquare
    chi2, p_value = chisquare(race_comparison['NASS'], race_comparison['Census'])
    print(f"\nChi-square test for race distribution:")
    print(f"Chi-square statistic: {chi2:.4f}")
    print(f"P-value: {p_value:.4f}")

# %% [markdown]
# ## Part 6: Machine Learning Analysis - Race Classifier

# %%
# Prepare data for ML models
def prepare_ml_data(df, target_col):
    """Prepare data for machine learning"""
    # Select relevant features
    feature_cols = ['AGE', 'TOTCHG', 'HOSP_TEACH', 'HOSP_LOCATION', 
                    'HOSP_REGION', 'HOSP_BEDSIZE_CAT', 'PL_NCHS', 'ZIPINC_QRTL']
    
    # Filter for valid data
    ml_df = df[feature_cols + [target_col]].dropna()
    
    # Encode categorical variables
    le_dict = {}
    for col in ml_df.select_dtypes(include=['object']).columns:
        le = LabelEncoder()
        ml_df[col] = le.fit_transform(ml_df[col])
        le_dict[col] = le
    
    # Split features and target
    X = ml_df[feature_cols]
    y = ml_df[target_col]
    
    return X, y, le_dict

# Race classification model
if 'RACE' in df.columns:
    print("Building Race Classification Model...")
    X_race, y_race, le_race = prepare_ml_data(df, 'RACE')
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X_race, y_race, test_size=0.3, random_state=42, stratify=y_race
    )
    
    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Train Random Forest
    rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    rf_model.fit(X_train_scaled, y_train)
    
    # Evaluate
    y_pred = rf_model.predict(X_test_scaled)
    print("\nRandom Forest Results:")
    print(classification_report(y_test, y_pred))
    
    # Feature importance
    feature_importance = pd.DataFrame({
        'feature': X_race.columns,
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    plt.figure(figsize=(10, 6))
    plt.barh(feature_importance['feature'], feature_importance['importance'])
    plt.xlabel('Importance')
    plt.title('Feature Importance for Race Classification')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.savefig('race_feature_importance.png', dpi=300, bbox_inches='tight')
    plt.show()

# %% [markdown]
# ## Part 7: Rural-Urban Analysis

# %%
# Rural-Urban classification
if 'PL_NCHS' in df.columns:
    # Define rural vs urban
    rural_urban_map = {
        1: 'Urban', 2: 'Urban', 3: 'Urban',  # Large Central, Large Fringe, Medium
        4: 'Rural', 5: 'Rural', 6: 'Rural'   # Small, Micro, Non-core
    }
    
    df['Rural_Urban'] = df['PL_NCHS'].map(rural_urban_map)
    
    # Compare characteristics
    rural_urban_stats = df.groupby('Rural_Urban').agg({
        'AGE': 'mean',
        'TOTCHG': 'mean',
        'KEY_NASS': 'count'
    }).rename(columns={'KEY_NASS': 'n_encounters'})
    
    print("Rural vs Urban Statistics:")
    print(rural_urban_stats)
    
    # Visualize differences
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # Age distribution
    df.boxplot(column='AGE', by='Rural_Urban', ax=axes[0])
    axes[0].set_title('Age Distribution by Rural/Urban')
    axes[0].set_xlabel('')
    
    # Charge distribution
    df.boxplot(column='TOTCHG', by='Rural_Urban', ax=axes[1])
    axes[1].set_title('Charge Distribution by Rural/Urban')
    axes[1].set_xlabel('')
    axes[1].set_ylim(0, df['TOTCHG'].quantile(0.95))  # Limit for visibility
    
    # Procedure mix
    top_procs_rural = df[df['Rural_Urban']=='Rural']['CPTCCS1'].value_counts().head(10)
    top_procs_urban = df[df['Rural_Urban']=='Urban']['CPTCCS1'].value_counts().head(10)
    
    proc_comparison = pd.DataFrame({
        'Rural': top_procs_rural,
        'Urban': top_procs_urban
    }).fillna(0)
    
    proc_comparison.plot(kind='barh', ax=axes[2])
    axes[2].set_title('Top Procedures by Rural/Urban')
    axes[2].set_xlabel('Number of Encounters')
    
    plt.suptitle('')
    plt.tight_layout()
    plt.savefig('rural_urban_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()

# %% [markdown]
# ## Part 8: Time Series Analysis (2020 Monthly Patterns)

# %%
# Monthly analysis
if 'AMONTH' in df.columns:
    monthly_stats = df.groupby('AMONTH').agg({
        'KEY_NASS': 'count',
        'TOTCHG': 'mean',
        'AGE': 'mean'
    }).rename(columns={'KEY_NASS': 'n_encounters'})
    
    # Account for COVID-19 impact
    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 
              'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
    
    fig, axes = plt.subplots(2, 1, figsize=(12, 8))
    
    # Encounter volumes
    axes[0].plot(monthly_stats.index, monthly_stats['n_encounters'], 
                 marker='o', linewidth=2)
    axes[0].axvspan(3, 5, alpha=0.3, color='red', label='COVID-19 Peak')
    axes[0].set_xlabel('Month')
    axes[0].set_ylabel('Number of Encounters')
    axes[0].set_title('2020 Monthly Encounter Volumes')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # Average charges
    axes[1].plot(monthly_stats.index, monthly_stats['TOTCHG'], 
                 marker='o', linewidth=2, color='green')
    axes[1].axvspan(3, 5, alpha=0.3, color='red')
    axes[1].set_xlabel('Month')
    axes[1].set_ylabel('Average Total Charge ($)')
    axes[1].set_title('2020 Monthly Average Charges')
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('monthly_trends_2020.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Calculate COVID impact
    pre_covid = monthly_stats.loc[1:2, 'n_encounters'].mean()
    covid_peak = monthly_stats.loc[4:5, 'n_encounters'].mean()
    reduction = (pre_covid - covid_peak) / pre_covid * 100
    
    print(f"COVID-19 Impact Analysis:")
    print(f"Pre-COVID monthly average: {pre_covid:,.0f} encounters")
    print(f"COVID peak monthly average: {covid_peak:,.0f} encounters")
    print(f"Reduction: {reduction:.1f}%")

# %% [markdown]
# ## Part 9: Advanced Analytics - Clustering Analysis

# %%
# Hospital clustering based on case mix and demographics
def perform_hospital_clustering(df):
    """Cluster hospitals based on their characteristics"""
    
    # Aggregate hospital-level metrics
    hosp_features = df.groupby('HOSP_NASS').agg({
        'AGE': 'mean',
        'TOTCHG': 'mean',
        'FEMALE': lambda x: x.mean() if x.notna().any() else 0.5,
        'RACE': lambda x: x.mode()[0] if len(x.mode()) > 0 else 1,
        'PAY1': lambda x: x.mode()[0] if len(x.mode()) > 0 else 3,
        'KEY_NASS': 'count'
    }).rename(columns={'KEY_NASS': 'volume'})
    
    # Add diversity index
    diversity = df.groupby('HOSP_NASS')['RACE'].apply(
        lambda x: 1 - sum((x.value_counts(normalize=True) ** 2))
    )
    hosp_features['diversity_index'] = diversity
    
    # Standardize features
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(hosp_features.fillna(hosp_features.mean()))
    
    # Perform PCA
    pca = PCA(n_components=2)
    features_pca = pca.fit_transform(features_scaled)
    
    # K-means clustering
    kmeans = KMeans(n_clusters=4, random_state=42)
    clusters = kmeans.fit_predict(features_scaled)
    
    # Visualize
    plt.figure(figsize=(10, 8))
    scatter = plt.scatter(features_pca[:, 0], features_pca[:, 1], 
                         c=clusters, cmap='viridis', alpha=0.6)
    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')
    plt.title('Hospital Clustering Analysis')
    plt.colorbar(scatter, label='Cluster')
    
    # Add cluster centers
    centers_pca = pca.transform(kmeans.cluster_centers_)
    plt.scatter(centers_pca[:, 0], centers_pca[:, 1], 
               c='red', marker='x', s=200, linewidths=3)
    
    plt.tight_layout()
    plt.savefig('hospital_clusters.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return hosp_features, clusters

hosp_features, clusters = perform_hospital_clustering(df)

# Cluster characteristics
for i in range(4):
    cluster_hospitals = hosp_features.index[clusters == i]
    cluster_data = df[df['HOSP_NASS'].isin(cluster_hospitals)]
    
    print(f"\nCluster {i} ({len(cluster_hospitals)} hospitals):")
    print(f"  Avg Age: {cluster_data['AGE'].mean():.1f}")
    print(f"  Avg Charge: ${cluster_data['TOTCHG'].mean():,.0f}")
    print(f"  Total Encounters: {len(cluster_data):,}")

# %% [markdown]
# ## Part 10: Generate Comprehensive Report

# %%
def generate_poster_report():
    """Generate academic poster-style report"""
    
    # Create poster layout
    fig = plt.figure(figsize=(36, 24))  # A0 size
    
    # Title
    fig.suptitle('Racial vs. Economic Barriers to Ambulatory Surgery Usage in the United States\n'
                'Analysis of HCUP NASS 2020 Dataset', 
                fontsize=28, fontweight='bold', y=0.98)
    
    # Create grid for poster sections
    gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3, 
                         left=0.05, right=0.95, top=0.93, bottom=0.05)
    
    # Abstract (top left)
    ax_abstract = fig.add_subplot(gs[0, :2])
    ax_abstract.axis('off')
    abstract_text = ("BACKGROUND: Ambulatory surgery utilization varies significantly across "
                    "demographic and socioeconomic groups.\n\n"
                    "METHODS: Analysis of 7.8 million encounters from 2,899 hospitals "
                    "in the 2020 HCUP NASS dataset.\n\n"
                    "RESULTS: Significant disparities identified in utilization patterns "
                    "across racial and economic factors.")
    ax_abstract.text(0.05, 0.95, abstract_text, transform=ax_abstract.transAxes,
                    fontsize=14, verticalalignment='top', wrap=True)
    ax_abstract.set_title('Abstract', fontsize=18, fontweight='bold', loc='left')
    
    # Methods (top right)
    ax_methods = fig.add_subplot(gs[0, 2:])
    ax_methods.axis('off')
    methods_text = ("• Data Source: HCUP NASS 2020\n"
                   "• Statistical Analysis: Chi-square, t-tests\n"
                   "• Machine Learning: Random Forest, XGBoost\n"
                   "• Census Benchmarking: 2020 US Census")
    ax_methods.text(0.05, 0.95, methods_text, transform=ax_methods.transAxes,
                   fontsize=14, verticalalignment='top')
    ax_methods.set_title('Methods', fontsize=18, fontweight='bold', loc='left')
    
    # Load saved plots
    plot_positions = [
        (gs[1, :2], 'hospital_distribution.png', 'Hospital Characteristics'),
        (gs[1, 2:], 'top_procedures.png', 'Common Procedures'),
        (gs[2, :2], 'race_comparison.png', 'Demographic Analysis'),
        (gs[2, 2:], 'rural_urban_analysis.png', 'Geographic Patterns'),
        (gs[3, :2], 'monthly_trends_2020.png', 'Temporal Trends'),
        (gs[3, 2:], 'hospital_clusters.png', 'Hospital Clustering')
    ]
    
    for position, filename, title in plot_positions:
        ax = fig.add_subplot(position)
        try:
            img = plt.imread(filename)
            ax.imshow(img)
            ax.axis('off')
            ax.set_title(title, fontsize=16, fontweight='bold')
        except:
            ax.text(0.5, 0.5, f'{title}\n(Plot not available)', 
                   transform=ax.transAxes, ha='center')
            ax.axis('off')
    
    # Save poster
    plt.savefig('NASS_2020_Analysis_Poster.pdf', format='pdf', dpi=300, bbox_inches='tight')
    plt.savefig('NASS_2020_Analysis_Poster.png', format='png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("Academic poster generated successfully!")
    print("Files saved: NASS_2020_Analysis_Poster.pdf and .png")

generate_poster_report()

# %% [markdown]
# ## Part 11: Statistical Summary Report

# %%
def generate_summary_statistics():
    """Generate comprehensive statistical summary"""
    
    summary = {
        'Dataset Overview': {
            'Total Encounters': len(df),
            'Total Hospitals': df['HOSP_NASS'].nunique(),
            'Unique Procedures': df['CPTCCS1'].nunique(),
            'Date Range': '2020-01-01 to 2020-12-31'
        },
        'Patient Demographics': {
            'Mean Age': df['AGE'].mean(),
            'Median Age': df['AGE'].median(),
            'Age Range': f"{df['AGE'].min()} - {df['AGE'].max()}",
            'Gender Distribution': df['FEMALE'].value_counts(normalize=True).to_dict() if 'FEMALE' in df.columns else 'N/A'
        },
        'Financial Metrics': {
            'Mean Charge': df['TOTCHG'].mean(),
            'Median Charge': df['TOTCHG'].median(),
            'Total Charges': df['TOTCHG'].sum(),
            'Charge Range': f"${df['TOTCHG'].min():,.0f} - ${df['TOTCHG'].max():,.0f}"
        },
        'Geographic Distribution': {
            'Regions': df['HOSP_REGION'].value_counts().to_dict() if 'HOSP_REGION' in df.columns else 'N/A',
            'Rural/Urban': df['Rural_Urban'].value_counts().to_dict() if 'Rural_Urban' in df.columns else 'N/A'
        }
    }
    
    # Save to JSON
    import json
    with open('nass_2020_statistics.json', 'w') as f:
        json.dump(summary, f, indent=2, default=str)
    
    # Display summary
    print("Statistical Summary Report")
    print("=" * 60)
    for section, stats in summary.items():
        print(f"\n{section}:")
        print("-" * 40)
        for key, value in stats.items():
            if isinstance(value, (int, float)):
                if 'Charge' in key or 'Total' in key:
                    print(f"  {key}: ${value:,.2f}")
                else:
                    print(f"  {key}: {value:,.2f}")
            else:
                print(f"  {key}: {value}")
    
    return summary

summary_stats = generate_summary_statistics()

# %% [markdown]
# ## Part 12: Export Results and Cleanup

# %%
# Save processed dataframes
print("Saving analysis results...")

# Key analysis results
results = {
    'hospital_stats': hospital_stats,
    'procedure_counts': procedure_counts,
    'summary_statistics': pd.DataFrame([summary_stats])
}

# Save to Excel with multiple sheets
with pd.ExcelWriter('NASS_2020_Analysis_Results.xlsx', engine='openpyxl') as writer:
    for name, df_result in results.items():
        if isinstance(df_result, pd.Series):
            df_result = df_result.to_frame()
        df_result.to_excel(writer, sheet_name=name[:31])  # Excel sheet name limit

# Save model if trained
if 'rf_model' in locals():
    import joblib
    joblib.dump(rf_model, 'race_classifier_model.pkl')
    joblib.dump(scaler, 'feature_scaler.pkl')
    print("Machine learning models saved")

# Create final summary document
summary_text = f"""
NASS 2020 Analysis Complete
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

Key Findings:
- Total Encounters Analyzed: {len(df):,}
- Number of Hospitals: {df['HOSP_NASS'].nunique():,}
- COVID-19 Impact: {reduction:.1f}% reduction in procedures during peak

Files Generated:
1. NASS_2020_Analysis_Poster.pdf - Academic poster
2. NASS_2020_Analysis_Results.xlsx - Detailed results
3. nass_2020_statistics.json - Statistical summary
4. Various .png files - Individual visualizations
5. ML models (.pkl) - Trained classifiers

Analysis completed successfully!
"""

with open('ANALYSIS_SUMMARY.txt', 'w') as f:
    f.write(summary_text)

print(summary_text)

# %%
# Memory cleanup
import gc
gc.collect()

print("\n" + "="*60)
print("ANALYSIS COMPLETE!")
print("="*60)
print(f"Total runtime: Check notebook execution time")
print(f"Peak memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")